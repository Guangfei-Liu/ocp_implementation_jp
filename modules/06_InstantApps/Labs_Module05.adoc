:scrollbar:
:data-uri:
:icons: images/icons
:toc2:		

:numbered:

=== Connect to Lab

. If not already connected, connect to your administration host:
+
----

yourdesktop$ ssh -i ~/.ssh/id_rsa your-opentlc-login@oselab-GUID.oslab.opentlc.com

----

. SSH to the master host:
+
----

[root@oselab-GUID ~]# ssh root@master00-GUID.oslab.opentlc.com

----
+
[NOTE]
If prompted for a password use *r3dh4t1!*
+
----

root@master00-GUID.oslab.opentlc.com's password: ******** (r3dh4t1!) 

----

== Templates, Instant Apps, and "Quickstarts"

This example will involve a build of another application, but also a service
that has two pods -- a "front-end" web tier and a "back-end" database tier. This
application also makes use of auto-generated parameters and other neat features
of OpenShift. One thing of note is that this project already has the
wiring/plumbing between the front- and back-end components pre-defined as part
of its JSON and embedded in the source code. Adding resources "after the fact"
will come in a later lab.

This example is effectively a "quickstart" -- a pre-defined application that
comes in a template that you can just fire up and start using or hacking on.

=== A Project for the Quickstart

. On the master host as `joe`, create a new project:
+
----

[joe@master00-GUID ~]$ osc new-project quickstart --display-name="Quickstart" \
    --description='A demonstration of a "quickstart/template"'

----
+
This also changes you to use that project:
+
----

Now using project "quickstart" on server "https://master00-GUID.oslab.opentlc.com:8443".

----

=== A Quick Aside on Templates

From the [OpenShift
documentation](http://docs.openshift.org/latest/dev_guide/templates.html):

    A template describes a set of resources intended to be used together that
    can be customized and processed to produce a configuration. Each template
    can define a list of parameters that can be modified for consumption by
    containers.

As we mentioned previously, this template has some auto-generated parameters.
For example, take a look at the following JSON:

    "parameters": [
      {
        "name": "ADMIN_USERNAME",
        "description": "administrator username",
        "generate": "expression",
        "from": "admin[A-Z0-9]{3}"
      },

This portion of the template's JSON tells OpenShift to generate an expression
using a regex-like string that will be presented as ADMIN_USERNAME.

=== Adding the Template

. On the master host as `root` in the `/root/training/beta4` folder:
+
----

[root@master00-GUID ~]# cd /root/training/beta4;osc create -f integrated-template.json -n openshift

----
+
What did you just do? The `integrated-template.json` file defined a template. By
"creating" it, you have added it to the `openshift` project.

=== Create an Instance of the Template

. In the web console, log in as `joe`.

. Find the "Quickstart" project, and then click the "Create +" button.
+
We've seen this page before, but now it contains something new -- an "instant app(lication)". An instant app is a "special" kind of template (relaly, it just has the "instant-app" tag). The idea behind an
"instant app" is that, when creating an instance of the template, you will have
a fully functional application. in this example, our "instant" app is just a
simple key-value storage and retrieval webpage.

. Click "quickstart-keyvalue-application", and you'll see a modal pop-up that
provides more information about the template.

. Click "Select template..."
+
The next page that you will see is the template "configuration" page. This is
where you can specify certain options for how the application components will be
insantiated.
+
It will:
+
* Show you what Docker images are used

* Let you add label:value pairs that can be used for other things

* Let you set specific values for any parameters, if you so choose

. Leave all of the defaults and simply click "Create".
+
Once you hit the "Create" button, the services and pods and
replicationcontrollers etc. will be instantiated.
+
The cool thing about the template is that it has a built-in route. The not so
cool thing is that route is not configurable at the moment. But, it's there!

. Click "Browse" and then "Services" you will see that there is a route for
the *frontend* service:
+
----

`integrated.cloudapps.example.com`

----
+
The build was started for us immediately after creating an instance of the
template, so you can wait for it to finish. Feel free to check the build logs.

. Once the build is complete, you can go on to the next step.

=== Using Your App

Once the app is built, you should be able to visit the routed URL and
actually use the application!

    http://integrated.cloudapps-GUID.oslab.opentlc.com

[NOTE]
HTTPS will *not* work for this example because the form submission was
written with HTTP links. Be sure to use HTTP.

== Creating and Wiring Disparate Components

Quickstarts are great, but sometimes a developer wants to build up the various
components manually. Let's take our quickstart example and treat it like two
separate "applications" that we want to wire together.

=== Create a New Project

. On the master host become the user `alice`:
+
----

[root@master00-GUID ~]# su - alice

----

. On the master host as the user `alice` reate a project for this example:
+
----

[alice@master00-GUID ~]$ osc new-project wiring --display-name="Exploring Parameters" \
    --description='An exploration of wiring using parameters'

----

. Log into the web console as `alice`. Can you see `joe`'s projects and content?

. Before continuing, `alice` will also need the training repository run the following on the master host as `alice`:
+
----

[alice@master00-GUID ~]$ cd;git clone https://github.com/openshift/training.git
[alice@master00-GUID ~]$ cd ~/training/beta4

----

=== Stand Up the Frontend

The first step will be to stand up the frontend of our application. For
argument's sake, this could have just as easily been brand new vanilla code.
However, to make things faster, we'll start with an application that already is
looking for a DB, but won't fail spectacularly if one isn't found.

. On the master host process the frontend template and then examine it:
+
----

[alice@master00-GUID ~]$ osc process -f frontend-template.json > frontend-config.json

----
+
[NOTE]
If you are using a different domain, you will need to edit the route
before running `create`.
+
In the config, you will see that a DB password and other parameters have been
generated (remember the template and parameter info from earlier?).

. On the master host create the configuration:
+
----

[alice@master00-GUID ~]$ osc create -f frontend-config.json

----

As soon as you create this, all of the resources will be created *and* a build
will be started for you. Let's go ahead and wait until this build completes
before continuing.

=== Visit Your Application

Once the new build is finished and the frontend service's endpoint has been
updated, visit your application. The frontend configuration contained a route
for `wiring.cloudapps.example.com`. You should see a note that the database is
missing. So, let's create it!

=== Create the Database Config

Remember, `osc process` will examine a template, generate any desired
parameters, and spit out a JSON `config`uration that can be `created` with
`osc`.

Processing the template for the db will generate some values for the DB root
user and password, but they don't actually match what was previously generated
when we set up the front-end. In the "quickstart" example, we generated these
values and used them for both the frontend and the backend at the exact same
time. Since we are processing them separately now, some manual intervention is
required.

This template uses the Red Hat MySQL Docker container, which knows to take some
env-vars when it fires up (eg: the MySQL user / password). More information on
the upstream of this container can be found here:

    https://github.com/openshift/mysql

. Take a look at the frontend configuration (`frontend-config.json`) and find the
value for `MYSQL_USER`. For example, `userMXG`. Then insert these values into
the template using the `process` command and create the result:
+
----

[alice@master00-GUID ~]$ grep -A 1 MYSQL_* frontend-config.json

----
+
----
                                                "name": "MYSQL_USER",
                                                "key": "MYSQL_USER",
                                                "value": "userMXG"
    --
                                                "name": "MYSQL_PASSWORD",
                                                "key": "MYSQL_PASSWORD",
                                                "value": "slDrggRv"
    --
                                                "name": "MYSQL_DATABASE",
                                                "key": "MYSQL_DATABASE",
                                                "value": "root"
----
+
----

[alice@master00-GUID ~]$ osc process -f db-template.json \
        -v MYSQL_USER=userMXG,MYSQL_PASSWORD=slDrggRv,MYSQL_DATABASE=root \
        | osc create -f -

----
+
`osc process` can be passed values for parameters, which will override
auto-generation.
+
It may take a little while for the MySQL container to download (if you didn't
pre-fetch it). It's a good idea to verify that the database is running before
continuing.  

. If you don't happen to have a MySQL client installed you can still
verify MySQL is running with curl:
+
----

[alice@master00-GUID ~]$ curl `osc get services | grep database | awk '{print $4}'`:5434

----
+
MySQL doesn't speak HTTP so you will see garbled output like this (however,
you'll know your database is running!):
+
----

GARBLED TEXT...packets out of order

----

=== Visit Your Application Again

Visit your application again with your web browser. Why does it still say that
there is no database?

When the frontend was first built and created, there was no service called
"database", so the environment variable `DATABASE_SERVICE_HOST` did not get
populated with any values. Our database does exist now, and there is a service
for it, but OpenShift could not "inject" those values into the frontend
container.

=== Replication Controllers

The easiest way to get this going? Just nuke the existing pod. 

. Get the replication controller running for both the frontend and backend:
+
----

[alice@master00-GUID ~]$ osc get replicationcontroller

----

. The replication controller is configured to ensure that we always have the
desired number of replicas (instances) running. We can look at how many that
should be:
+
----

[alice@master00-GUID ~]$ osc describe rc frontend-1

----
+
So, if we kill the pod, the RC will detect that, and fire it back up. When it
gets fired up this time, it will then have the `DATABASE_SERVICE_HOST` value,
which means it will be able to connect to the DB, which means that we should no
longer see the database error!

. As `alice`, go ahead and find your frontend pod, and then kill it:
+
----

[alice@master00-GUID ~]$ osc delete pod `osc get pod | grep front | awk '{print $1}'`

----
+
You'll see something like:
+
----

pods/frontend-1-b6bgy

----
+
That was the generated name of the pod when the replication controller stood it
up the first time. You also see some deployment hook pods. We will talk about
deployment hooks a bit later.

. After a few moments, we can look at the list of pods again:
+
----

[alice@master00-GUID ~]$ osc get pod | grep front

----
+
We should see a different name for the pod this time:
+
----

    frontend-1-0fs20

----
+
This shows that, underneath the covers, the RC restarted our pod. Since it was
restarted, it should have a value for the `DATABASE_SERVICE_HOST` environment
variable. 

. Go to the node where the pod is running, and find the Docker container
id as `root`:
+
----

[alice@master00-GUID ~]$ docker inspect `docker ps | grep wiring | grep front | grep run | awk \
    '{print $1}'` | grep DATABASE

----
+
The output will look something like:
+
----

"MYSQL_DATABASE=root",
"DATABASE_PORT_5434_TCP_ADDR=172.30.17.106",
"DATABASE_PORT=tcp://172.30.17.106:5434",
"DATABASE_PORT_5434_TCP=tcp://172.30.17.106:5434",
"DATABASE_PORT_5434_TCP_PROTO=tcp",
"DATABASE_SERVICE_HOST=172.30.17.106",
"DATABASE_SERVICE_PORT=5434",
"DATABASE_PORT_5434_TCP_PORT=5434",

----

=== Revisit the Webpage

Go ahead and revisit `http://wiring.cloudapps-GUID.oslab.opentlc.com` in your browser, and you should see that the application is now fully
functional!

[NOTE]
There is a process to deploy instances of templates that we already
used in the "quickstart" case. For some reason, the MySQL database template
doesn't deploy successfully with the current example. Otherwise we would have
done 100% of this through the webUI.

Here's the bug for reference:

    https://github.com/openshift/origin/issues/2947

== Using Persistent Storage (Optional)

Having a database for development is nice, but what if you actually want the
data you store to stick around after the DB pod is redeployed? Pods are
ephemeral, and so is their storage by default. For shared or persistent
storage, we need a way to specify that pods should use external volumes.

We can do this a number of ways. [Kubernetes provides methods for directly
specifying the mounting of several different volume
types.](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/volumes.md)
This is perfect if you want to use known external resources. But that's
not very PaaS-y. If I'm using a PaaS, I might really just rather request a
chunk of storage and not need a side channel to provision that. OpenShift 3
provides a mechanism for doing just this.

=== Export an NFS Volume

For the purposes of this training, we will just demonstrate the master
exporting an NFS volume for use as storage by the database. **You would
almost certainly not want to do this in production.** If you happen
to have another host with an NFS export handy, feel free to substitute
that instead of the master.

. As `root` on the master host ensure that nfs-utils is installed (**on all systems**):
+
----

[root@master00-GUID ~]# yum -y install nfs-utils

----

. Create the directory we will export:
+
----

[root@master00-GUID ~]# mkdir -p /var/export/vol1
[root@master00-GUID ~]# chown nfsnobody:nfsnobody /var/export/vol1
[root@master00-GUID ~]# chmod 700 /var/export/vol1

. Add the following line to `/etc/exports`:
+
----

[root@master00-GUID ~]# echo "/var/export/vol1 *(rw,sync,all_squash)" >> /etc/exports

----


. Enable and start NFS services:
+
----

[root@master00-GUID ~]# systemctl enable rpcbind nfs-server
[root@master00-GUID ~]# systemctl start rpcbind nfs-server nfs-lock nfs-idmap

----
+
Note that the volume is owned by `nfsnobody` and access by all remote users
is "squashed" to be access by this user. This essentially disables user
permissions for clients mounting the volume. While another configuration
might be preferable, one problem you may run into is that the container
cannot modify the permissions of the actual volume directory when mounted.
In the case of MySQL below, MySQL would like to have the volume belong to
the `mysql` user, and assumes that it is, which causes problems later.
Arguably, the container should operate differently. In the long run, we
probably need to come up with best practices for use of NFS from containers.

=== NFS Firewall

**In our lab environment, the firewall is disabled on the master host, so these steps are not necessary.**

We will need to open ports on the firewall on the master to enable NFS to
communicate from the nodes. First, let's add rules for NFS to the running state
of the firewall:

    iptables -I OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 111 -j ACCEPT
    iptables -I OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2049 -j ACCEPT
    iptables -I OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 20048 -j ACCEPT
    iptables -I OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 50825 -j ACCEPT
    iptables -I OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 53248 -j ACCEPT

Next, let's add the rules to `/etc/sysconfig/iptables`. Put them at the top of
the `OS_FIREWALL_ALLOW` set:

    -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 53248 -j ACCEPT
    -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 50825 -j ACCEPT
    -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 20048 -j ACCEPT
    -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2049 -j ACCEPT
    -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 111 -j ACCEPT

Now, we have to edit NFS' configuration to use these ports. First, let's edit
`/etc/sysconfig/nfs`. Change the RPC option to the following:

    RPCMOUNTDOPTS="-p 20048"

Change the STATD option to the following:

    STATDARG="-p 50825"

Then, edit `/etc/sysctl.conf`:

    fs.nfs.nlm_tcpport=53248
    fs.nfs.nlm_udpport=53248

Then, persist the `sysctl` changes:

    sysctl -p

Lastly, restart NFS:

    systemctl restart nfs

=== Allow NFS Access in SELinux Policy

By default policy, containers are not allowed to write to NFS mounted
directories.  We want to do just that with our database, so enable that on
all nodes where the pod could land (i.e. all of them) with:
+
----

[root@master00-GUID ~]#setsebool -P virt_use_nfs=true

----

Once the ansible-based installer does this automatically, we can remove this
section from the document.

### Create a PersistentVolume
It is the PaaS administrator's responsibility to define the storage that is
available to users. Storage is represented by a PersistentVolume that
encapsulates the details of a particular volume which can be backed by any
of the [volume types available via
Kubernetes](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/volumes.md).
In this case it will be our NFS volume.

Currently PersistentVolume objects must be created "by hand". Modify the
`beta4/persistent-volume.json` file as needed if you are using a different
NFS mount:

    {
      "apiVersion": "v1",
      "kind": "PersistentVolume",
      "metadata": {
        "name": "pv0001"
      },
      "spec": {
        "capacity": {
            "storage": "5Gi"
            },
        "accessModes": [ "ReadWriteMany" ],
        "nfs": {
            "path": "/var/export/vol1",
            "server": "ose3-master.example.com"
        }
      }
    }

Create this object as the `root` (administrative) user:

    # osc create -f persistent-volume.json
    persistentvolumes/pv0001

This defines a volume for OpenShift projects to use in deployments. The
storage should correspond to how much is actually available (make each
volume a separate filesystem if you want to enforce this limit). Take a
look at it now:

    # osc describe persistentvolumes/pv0001
    Name:   pv0001
    Labels: <none>
    Status: Available
    Claim:

### Claim the PersistentVolume
Now that the administrator has provided a PersistentVolume, any project can
make a claim on that storage. We do this by creating a PersistentVolumeClaim
that specifies what kind and how much storage is desired:

    {
      "apiVersion": "v1",
      "kind": "PersistentVolumeClaim",
      "metadata": {
        "name": "claim1"
      },
      "spec": {
        "accessModes": [ "ReadWriteMany" ],
        "resources": {
          "requests": {
            "storage": "5Gi"
          }
        }
      }
    }

We can have `alice` do this in the `wiring` project:

    $ osc create -f persistent-volume-claim.json
    persistentVolumeClaim/claim1

This claim will be bound to a suitable PersistentVolume (one that is big
enough and allows the requested accessModes). The user does not have any
real visibility into PersistentVolumes, including whether the backing
storage is NFS or something else; they simply know when their claim has
been filled ("bound" to a PersistentVolume).

    $ osc get pvc
    NAME      LABELS    STATUS    VOLUME
    claim1    map[]     Bound     pv0001

If as `root` we now go back and look at our PV, we will also see that it has
been claimed:

    # osc describe pv/pv0001
    Name:   pv0001
    Labels: <none>
    Status: Bound
    Claim:  wiring/claim1

The PersistentVolume is now claimed and can't be claimed by any other project.

Although this flow assumes the administrator pre-creates volumes in
anticipation of their use later, it would be possible to create an external
process that watches the API for a PersistentVolumeClaim to be created,
dynamically provisions a corresponding volume, and creates the API object
to fulfill the claim.

### Use the Claimed Volume
Finally, we need to modify our `database` DeploymentConfig to specify that
this volume should be mounted where the database will use it. As `alice`:

    $ osc edit dc/database

The part we will need to edit is the pod template. We will need to add two
parts: 

* a definition of the volume
* where to mount it inside the container

First, directly under the `template` `spec:` line, add this YAML (indented from the `spec:` line):

          volumes:
          - name: pvol
            persistentVolumeClaim:
              claimName: claim1

Then to have the container mount this, add this YAML after the
`terminationMessagePath:` line:

            volumeMounts:
            - mountPath: /var/lib/mysql/data
              name: pvol

Remember that YAML is sensitive to indentation. The final template should
look like this:

    template:
      metadata:
        creationTimestamp: null
        labels:
          deploymentconfig: database
      spec:
        volumes:
        - name: pvol
          persistentVolumeClaim:
            claimName: claim1
        containers:
        - capabilities: {}
    [...]
          terminationMessagePath: /dev/termination-log
          volumeMounts:
          - mountPath: /var/lib/mysql/data
            name: pvol
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        serviceAccount: ""

Save and exit. This change to configuration will trigger a new deployment
of the database, and this time, it will be using the NFS volume we exported
from master.

### Restart the Frontend
Any values or data we had inserted previously just got blown away. The
`deploymentConfig` update caused a new MySQL pod to be launched. Since this is
the first time the pod was launched with persistent data, any previous data was
lost.

Additionally, the Frontend pod will perform a database initialization when it
starts up. Since we haven't restarted the frontend, our database is actually
bare. If you try to use the app now, you'll get "Internal Server Error".

Go ahead and kill the Frontend pod like we did previously to cause it to
restart:

     osc delete pod `osc get pod | grep front | awk {'print $1'}`

Once the new pod has started, go ahead and visit the web page. Add a few values
via the application. Then delete the database pod and wait for it to come back.
You should be able to retrieve the same values you entered.

Remember, to quickly delete the Database pod you can do the following:

    osc delete pod/`osc get pod | grep -e "database-[0-9]" | awk {'print $1'}`

**Note:** This doesn't seem to work right now, but we're not sure why. I think
it has to do with Ruby's persistent connection to the MySQL service not going
away gracefully, or something. Killing the frontend again will definitely work.

For further confirmation that your database pod is in fact using the NFS
volume, simply check what is stored there on `master`:

    # ls /var/export/vol1
    database-3-n1i2t.pid  ibdata1  ib_logfile0  ib_logfile1  mysql  performance_schema  root

Further information on use of PersistentVolumes is available in the
[OpenShift Origin documentation](http://docs.openshift.org/latest/dev_guide/volumes.html).
This is a very new feature, so it is very manual for now, but look for more tooling
taking advantage of PersistentVolumes to be created in the future.

## Rollback/Activate and Code Lifecycle
Not every coder is perfect, and sometimes you want to rollback to a previous
incarnation of your application. Sometimes you then want to go forward to a
newer version, too.

The next few labs require that you have a Github account. We will take Alice's
"wiring" application and modify its front-end and then rebuild. We'll roll-back
to the original version, and then go forward to our re-built version.

### Fork the Repository
Our wiring example's frontend service uses the following Github repository:

    https://github.com/openshift/ruby-hello-world

Go ahead and fork this into your own account by clicking the *Fork* Button at
the upper right.

### Update the BuildConfig
Remember that a `BuildConfig`(uration) tells OpenShift how to do a build.
Still as the `alice` user, take a look at the current `BuildConfig` for our
frontend:

    osc get buildconfig ruby-sample-build -o yaml
    apiVersion: v1beta1
    kind: BuildConfig
    metadata:
      creationTimestamp: 2015-03-10T15:40:26-04:00
      labels:
        template: application-template-stibuild
      name: ruby-sample-build
      namespace: wiring
      resourceVersion: "831"
      selfLink: /osapi/v1beta1/buildConfigs/ruby-sample-build?namespace=wiring
      uid: 4cff2e5e-c75d-11e4-806e-525400b33d1d
    parameters:
      output:
        to:
          kind: ImageStream
          name: origin-ruby-sample
      source:
        git:
          uri: git://github.com/openshift/ruby-hello-world.git
          ref: beta4
        type: Git
      strategy:
        stiStrategy:
          builderImage: openshift/ruby-20-rhel7
          image: openshift/ruby-20-rhel7
        type: STI
    triggers:
    - github:
        secret: secret101
      type: github
    - generic:
        secret: secret101
      type: generic
    - imageChange:
        from:
          name: ruby-20-rhel7
        image: openshift/ruby-20-rhel7
        imageRepositoryRef:
          name: ruby-20-rhel7
        tag: latest
      type: imageChange

As you can see, the current configuration points at the
`openshift/ruby-hello-world` repository. Since you've forked this repo, let's go
ahead and re-point our configuration. Our friend `osc edit` comes to the rescue
again:

    osc edit bc ruby-sample-build

Change the "uri" reference to match the name of your Github
repository. Assuming your github user is `alice`, you would point it
to `git://github.com/alice/ruby-hello-world.git`. Save and exit
the editor.

If you again run `osc get buildconfig ruby-sample-build -o yaml` you should see
that the `uri` has been updated.

### Change the Code
Github's web interface will let you make edits to files. Go to your forked
repository (eg: https://github.com/alice/ruby-hello-world), select the `beta3`
branch, and find the file `main.erb` in the `views` folder.

Change the following HTML:

    <div class="page-header" align=center>
      <h1> Welcome to an OpenShift v3 Demo App! </h1>
    </div>

To read (with the typo):

    <div class="page-header" align=center>
      <h1> This is my crustom demo! </h1>
    </div>

You can edit code on Github by clicking the pencil icon which is next to the
"History" button. Provide some nifty commit message like "Personalizing the
application."

If you know how to use Git/Github, you can just do this "normally".

### Start a Build with a Webhook
Webhooks are a way to integrate external systems into your OpenShift
environment so that they can fire off OpenShift builds. Generally
speaking, one would make code changes, update the code repository, and
then some process would hit OpenShift's webhook URL in order to start
a build with the new code.

Your GitHub account has the capability to configure a webhook to request
whenever a commit is pushed to a specific branch; however, it would only
be able to make a request against your OpenShift master if that master
is exposed on the Internet, so you will probably need to simulate the
request manually for now.

To find the webhook URL, you can visit the web console, click into the
project, click on *Browse* and then on *Builds*. You'll see two webhook
URLs. Copy the *Generic* one. It should look like:

    https://ose3-master.example.com:8443/osapi/v1beta1/buildConfigHooks/ruby-sample-build//github?namespace=wiring

**Note**: As of the cut of beta 4, the generic webhook URL was incorrect in the
webUI. Note the correct syntax above. This is fixed already, but did not make it
in:

    https://github.com/openshift/origin/issues/2981

If you look at the `frontend-config.json` file that you created earlier,
you'll notice the same "secret101" entries in triggers. These are
basically passwords so that just anyone on the web can't trigger the
build with knowledge of the name only. You could of course have adjusted
the passwords or had the template generate randomized ones.

This time, in order to run a build for the frontend, we'll use `curl` to hit our
webhook URL.

First, look at the list of builds:

    osc get build

You should see that the first build had completed. Then, `curl`:

    curl -i -H "Accept: application/json" \
    -H "X-HTTP-Method-Override: PUT" -X POST -k \
    https://ose3-master.example.com:8443/osapi/v1beta1/buildConfigHooks/ruby-sample-build//github?namespace=wiring

And now `get build` again:

    osc get build
    NAME                  TYPE      STATUS     POD
    ruby-sample-build-1   Source    Complete   ruby-sample-build-1
    ruby-sample-build-2   Source    Pending    ruby-sample-build-2

You can see that this could have been part of some CI/CD workflow that
automatically called our webhook once the code was tested.

You can also check the web interface (logged in as `alice`) and see
that the build is running. Once it is complete, point your web browser
at the application:

    http://wiring.cloudapps.example.com/

You should see your big fat typo.

**Note: Remember that it can take a minute for your service endpoint to get
updated. You might get a `503` error if you try to access the application before
this happens.**

Since we failed to properly test our application, and our ugly typo has made it
into production, a nastygram from corporate marketing has told us that we need
to revert to the previous version, ASAP.

If you log into the web console as `alice` and find the `Deployments` section of
the `Browse` menu, you'll see that there are two deployments of our frontend: 1
and 2.

You can also see this information from the cli by doing:

    osc get replicationcontroller

The semantics of this are that a `DeploymentConfig` ensures a
`ReplicationController` is created to manage the deployment of the built `Image`
from the `ImageStream`.

Simple, right?

### Rollback
You can rollback a deployment using the CLI. Let's go and checkout what a rollback to
`frontend-1` would look like:

    osc rollback frontend-1 --dry-run

Since it looks OK, let's go ahead and do it:

    osc rollback frontend-1

If you look at the `Browse` tab of your project, you'll see that in the `Pods`
section there is a `frontend-3...` pod now. After a few moments, revisit the
application in your web browser, and you should see the old "Welcome..." text.

### Activate
Corporate marketing called again. They think the typo makes us look hip and
cool. Let's now roll forward (activate) the typo-enabled application:

    osc rollback frontend-2

## Customized Build and Run Processes
OpenShift v3 supports customization of both the build and run processes.
Generally speaking, this involves modifying the various S2I scripts from the
builder image. When OpenShift builds your code, it checks to see if any of the
scripts in the `.sti/bin` folder of your repository override/supercede the
builder image's scripts. If so, it will execute the repository script instead.

More information on the scripts, their execution during the process, and
customization can be found here:

    http://docs.openshift.org/latest/creating_images/sti.html#sti-scripts

### Add a Script
You will find a script called `custom-assemble.sh` in the `beta3` folder. Go to
your Github repository for your application from the previous lab, find the
`beta3` branch, and find the `.sti/bin` folder.

* Click the "+" button at the top (to the right of `bin` in the
    breadcrumbs).
* Name your file `assemble`.
* Paste the contents of `custom-assemble.sh` into the text area.
* Provide a nifty commit message.
* Click the "commit" button.

**Note:** If you know how to Git(hub), you can do this via your shell.

Once the file is added, we can now do another build. The "custom" assemble
script will log some extra data.

### Kick Off a Build
Our old friend `curl` is back:

    curl -i -H "Accept: application/json" \
    -H "X-HTTP-Method-Override: PUT" -X POST -k \
    https://ose3-master.example.com:8443/osapi/v1beta1/buildConfigHooks/ruby-sample-build//github?namespace=wiring

### Watch the Build Logs
Using the skills you have learned, watch the build logs for this build. If you
miss them, remember that you can find the Docker container that ran the build
and look at its Docker logs.

Did You See It?

    2015-03-11T14:57:00.022957957Z I0311 10:57:00.022913       1 sti.go:357]
    ---> CUSTOM S2I ASSEMBLE COMPLETE

But where's the output from the custom `run` script? The `assemble` script is
run inside of your builder pod. That's what you see by using `build-logs` - the
output of the assemble script. The
`run` script actually is what is executed to "start" your application's pod. In
other words, the `run` script is what starts the Ruby process for an image that
was built based on the `ruby-20-rhel7` S2I builder. 

To look inside the builder pod, as `alice`:

    osc logs `osc get pod | grep -e "[0-9]-build" | tail -1 | awk {'print $1'}` | grep CUSTOM

You should see something similar to:

    2015-04-27T22:23:24.110630393Z ---> CUSTOM S2I ASSEMBLE COMPLETE

## Lifecycle Pre and Post Deployment Hooks
Like in OpenShift 2, we have the capability of "hooks" - performing actions both
before and after the **deployment**. In other words, once an S2I build is
complete, the resulting Docker image is pushed into the registry. Once the push
is complete, OpenShift detects an `ImageChange` and, if so configured, triggers
a **deployment**.

The *pre*-deployment hook is executed just *before* the new image is deployed.

The *post*-deployment hook is executed just *after* the new image is deployed.

How is this accomplished? OpenShift will actually spin-up an *extra* instance of
your built image, execute your hook script(s), and then shut the instance down.
Neat, huh?

Since we already have our `wiring` app pointing at our forked code repository,
let's go ahead and add a database migration file. In the `beta4` folder you will
find a file called `1_sample_table.rb`. Add this file to the `db/migrate` folder
of the `ruby-hello-world` repository that you forked. If you don't add this file
to the right folder, the rest of the steps will fail.

### Examining Deployment Hooks
Take a look at the following JSON:

    "strategy": {
        "type": "Recreate",
        "resource": {},
        "recreateParams": {
            "pre": {
                "failurePolicy": "Abort",
                "execNewPod": {
                    "command": [
                        "/bin/true"
                    ],
                    "env": [
                        {
                            "name": "CUSTOM_VAR1",
                            "value": "custom_value1"
                        }
                    ],
                    "containerName": "ruby-helloworld"
                }
            },
            "post": {
                "failurePolicy": "Ignore",
                "execNewPod": {
                    "command": [
                        "/bin/false"
                    ],
                    "env": [
                        {
                            "name": "CUSTOM_VAR2",
                            "value": "custom_value2"
                        }
                    ],
                    "containerName": "ruby-helloworld"
                }
            }
        }
    },

You can see that both a *pre* and *post* deployment hook are defined. They don't
actually do anything useful. But they are good examples.

The pre-deployment hook executes "/bin/true" whose exit code is always 0 --
success. If for some reason this failed (non-zero exit), our policy would be to
`Abort` -- consider the entire deployment a failure and stop.

The post-deployment hook executes "/bin/false" whose exit code is always 1 --
failure. The policy is to `Ignore`, or do nothing. For non-essential tasks that
might rely on an external service, this might be a good policy.

More information on these strategies, the various policies, and other
information can be found in the documentation:

    http://docs.openshift.org/latest/dev_guide/deployments.html

### Modifying the Hooks
Since we are talking about **deployments**, let's look at our
`DeploymentConfig`s. As the `alice` user in the `wiring` project:

    osc get dc

You should see something like:

    NAME       TRIGGERS       LATEST VERSION
    database   ConfigChange   1
    frontend   ImageChange    7

Since we are trying to associate a Rails database migration hook with our
application, we are ultimately talking about a deployment of the frontend. If
you edit the frontend's `DeploymentConfig` as `alice`:

    osc edit dc frontend -ojson

Yes, the default for `osc edit` is to use YAML. For this exercise, JSON will be
easier as it is indentation-insensitive. Find the section that looks like the
following before continuing:

    "spec": {
        "strategy": {
            "type": "Recreate",
            "resources": {}
        },

A Rails migration is commonly performed when we have added/modified the database
as part of our code change. In the case of a pre- or post-deployment hook, it
would make sense to:

* Attempt to migrate the database
* Abort the new deployment if the migration fails

Otherwise we could end up with our new code deployed but our database schema
would not match. This could be a *Real Bad Thing (TM)*.

In the case of the `ruby-20` builder image, we are actually using RHEL7 and the
Red Hat Software Collections (SCL) to get our Ruby 2.0 support. So, the command
we want to run looks like:

    /usr/bin/scl enable ruby200 ror40 'cd /opt/openshift/src ; bundle exec rake db:migrate'

This command:

* executes inside an SCL "shell"
* enables the Ruby 2.0.0 and Ruby On Rails 4.0 environments
* changes to the `/opt/openshift/src` directory (where our applications' code is
    located)
* executes `bundle exec rake db:migrate`

If you're not familiar with Ruby, Rails, or Bundler, that's OK. Just trust us.
Would we lie to you?

The `command` directive inside the hook's definition tells us which command to
actually execute. It is required that this is an array of individual strings.
Represented in JSON, our desired command above represented as a string array
looks like:

    "command": [
        "/usr/bin/scl",
        "enable",
        "ruby200",
        "ror40",
        "cd /opt/openshift/src ; bundle exec rake db:migrate"
    ]

This is great, but actually manipulating the database requires that we talk
**to** the database. Talking to the database requires a user and a password.
Smartly, our hook pods inherit the same environment variables as the main
deployed pods, so we'll have access to the same datbase information.

Looking at the original hook example in the previous section, and our command
reference above, in the end, you will have something that looks like:

    "strategy": {
        "type": "Recreate",
        "resources": {},
        "recreateParams": {
            "pre": {
                "failurePolicy": "Abort",
                "execNewPod": {
                    "command": [
                        "/usr/bin/scl",
                        "enable",
                        "ruby200",
                        "ror40",
                        "cd /opt/openshift/src ; bundle exec rake db:migrate"
                    ],
                    "containerName": "ruby-helloworld"
                }
            },
        }
    },

Remember, indentation isn't critical in JSON, but closing brackets and braces
are. When you are done editing the deployment config, save and quit your editor.

### Quickly Clean Up
When we did our previous builds and rollbacks and etc, we ended up with a lot of
stale pods that are not running (`Succeeded`). Currently we do not auto-delete
these pods because we have no log store -- once they are deleted, you can't view
their logs any longer.

For now, we can clean up by doing the following as `alice`:

    osc get pod |\
    grep -E "[0-9]-build" |\
    awk {'print $1'} |\
    xargs -r osc delete pod

This will get rid of all of our old build and lifecycle pods. The lifecycle pods
are the pre- and post-deployment hook pods, and the sti-build pods are the pods
in which our previous builds occurred.

### Build Again
Now that we have modified the deployment configuration and cleaned up a bit, we
need to trigger another deployment. While killing the frontend pod would trigger
another deployment, our current Docker image doesn't have the database migration
file in it. Nothing really useful would happen.

In order to get the database migration file into the Docker image, we actually
need to do another build. Remember, the S2I process starts with the builder
image, fetches the source code, executes the (customized) assemble script, and
then pushes the resulting Docker image into the registry. **Then** the
deployment happens.

As `alice`:

    osc start-build ruby-sample-build

Or go into the web console and click the "Start Build" button in the Builds
area.

### Verify the Migration
About a minute after the build completes, you should see something like the following output
of `osc get pod` as `alice`:

    POD                                IP          CONTAINER(S)               IMAGE(S)                                                                                                                HOST                                    LABELS                                                                                                                  STATUS       CREATED         MESSAGE
    database-2-rj72q                   10.1.0.15                                                                                                                                                      ose3-master.example.com/192.168.133.2   deployment=database-2,deploymentconfig=database,name=database                                                           Running      About an hour   
                                                   ruby-helloworld-database   registry.access.redhat.com/openshift3_beta/mysql-55-rhel7                                                                                                                                                                                                                               Running      About an hour   
    deployment-frontend-7-hook-4i8ch                                                                                                                                                                  ose3-node1.example.com/192.168.133.3    <none>                                                                                                                  Succeeded    41 seconds      
                                                   lifecycle                  172.30.118.110:5000/wiring/origin-ruby-sample@sha256:2984cfcae1dd42c257bd2f79284293df8992726ae24b43470e6ffd08affc3dfd                                                                                                                                                                   Terminated   36 seconds      exit code 0
    frontend-7-nnnxz                   10.1.1.24                                                                                                                                                      ose3-node1.example.com/192.168.133.3    deployment=frontend-7,deploymentconfig=frontend,name=frontend                                                           Running      29 seconds      
                                                   ruby-helloworld            172.30.118.110:5000/wiring/origin-ruby-sample@sha256:2984cfcae1dd42c257bd2f79284293df8992726ae24b43470e6ffd08affc3dfd                                                                                                                                                                   Running      26 seconds      
    ruby-sample-build-7-build                                                                                                                                                                         ose3-master.example.com/192.168.133.2   build=ruby-sample-build-7,buildconfig=ruby-sample-build,name=ruby-sample-build,template=application-template-stibuild   Succeeded    2 minutes       
                                                   sti-build                  openshift3_beta/ose-sti-builder:v0.5.2.2                                                                                                                                                                                                                                                Terminated   2 minutes       exit code 0

Yes, it's ugly, thanks for reminding us.

You'll see that there is a single `hook`/`lifecycle` pod -- this corresponds
with the pod that ran our pre-deployment hook.

Inspect this pod's logs:

    osc logs deployment-frontend-7-hook-4i8ch

The output should show something like:

    == 1 SampleTable: migrating ===================================================
    -- create_table(:sample_table)
       -> 0.1075s
    == 1 SampleTable: migrated (0.1078s) ==========================================

If you have no output, you may have forgotten to actually put the migration file
in your repo. Without that file, the migration does nothing, which produces no
output.

For giggles, you can even talk directly to the database on its service IP/port
using the `mysql` client and the environment variables (you would need the
`mysql` package installed on your master, for example).

As `alice`, find your database:

    [alice@ose3-master beta4]$ osc get service
    NAME       LABELS    SELECTOR        IP(S)            PORT(S)
    database   <none>    name=database   172.30.108.133   5434/TCP
    frontend   <none>    name=frontend   172.30.229.16    5432/TCP

Then, somewhere inside your OpenShift environment, use the `mysql` client to
connect to this service and dump the table that we created:

    mysql -u userJKL \
      -p 5678efgh \
      -h 172.30.108.133 \
      -P 5434 \
      -e 'show tables; describe sample_table;' \
      root
    +-------------------+
    | Tables_in_root    |
    +-------------------+
    | sample_table      |
    | key_pairs         |
    | schema_migrations |
    +-------------------+
    +-------+--------------+------+-----+---------+----------------+
    | Field | Type         | Null | Key | Default | Extra          |
    +-------+--------------+------+-----+---------+----------------+
    | id    | int(11)      | NO   | PRI | NULL    | auto_increment |
    | name  | varchar(255) | NO   |     | NULL    |                |
    +-------+--------------+------+-----+---------+----------------+

## Arbitrary Docker Image (Builder)
One of the first things we did with OpenShift was launch an "arbitrary" Docker
image from the Docker Hub. However, we can also build Docker images from Docker
files, too. While this is a "build" process, it's not a "source-to-image"
process -- we're not working with only a source code repo.

As an example, the CentOS community maintains a Wordpress all-in-one Docker
image:

    https://github.com/CentOS/CentOS-Dockerfiles/tree/master/wordpress/centos7

We've taken the content of this subfolder and placed it in the GitHub
`openshift/centos7-wordpress` repository. Let's run `osc new-app` and see what
happens:

    osc new-app https://github.com/openshift/centos7-wordpress.git -o yaml

This all looks good for now.

### Create a Project
As `alice`, go ahead and create a new project:

    osc new-project wordpress --display-name="Wordpress" \
    --description='Building an arbitrary Wordpress Docker image'

### Build Wordpress
Let's choose the Wordpress example:

    osc new-app -l name=wordpress https://github.com/openshift/centos7-wordpress.git

    imageStreams/centos
    imageStreams/centos7-wordpress
    buildConfigs/centos7-wordpress
    deploymentConfigs/centos7-wordpress
    services/centos7-wordpress
    A build was created - you can run `osc start-build centos7-wordpress` to start it.
    Service "centos7-wordpress" created at 172.30.135.252 with port mappings 22.

Then, start the build:

    osc start-build centos7-wordpress

**Note: This can take a *really* long time to build.**

You will need a route for this application, as `curl` won't do a whole lot for
us here. Additionally, `osc new-app` currently has a bug in the way services are
detected, so we'll have a service for SSH (thus port 22 above) but not one for
httpd. So we'll add on a service and route for web access.

    osc create -f wordpress-addition.json

### Test Your Application
You should be able to visit:

    http://wordpress.cloudapps.example.com

Check it out!

Remember - not only did we use an arbitrary Docker image, we actually built the
Docker image using OpenShift. Technically there was no "code repository". So, if
you allow it, developers can actually simply build Docker containers as their
"apps" and run them directly on OpenShift.

### Application Resource Labels

You may have wondered about the `-l name=wordpress` in the invocation above. This
applies a label to all of the resources created by `osc new-app` so that they can
be easily distinguished from any other resources in a project. For example, we
can easily delete only the things with this label:

    osc delete all -l name=wordpress

    buildConfigs/centos7-wordpress
    builds/centos7-wordpress-1
    imageStreams/centos
    imageStreams/centos7-wordpress
    deploymentConfigs/centos7-wordpress
    replicationcontrollers/centos7-wordpress-1
    services/centos7-wordpress

Notice that the things we created from wordpress-addition.json didn't
have this label, so they didn't get deleted:

    osc get services

    NAME                      LABELS    SELECTOR                             IP             PORT(S)
    wordpress-httpd-service   <none>    deploymentconfig=centos7-wordpress   172.30.17.83   80/TCP

    osc get route

    NAME              HOST/PORT                         PATH      SERVICE                   LABELS
    wordpress-route   wordpress.cloudapps.example.com             wordpress-httpd-service

Labels will be useful for many things, including identification in the web console.

## EAP Example
This example requires internet access because the Maven configuration uses
public repositories.

If you have a Java application whose Maven configuration uses local
repositories, or has no Maven requirements, you could probably substitute that
code repository for the one below.

### Create a Project
Using the skills you have learned earlier in the training, create a new project
for the EAP example. Choose a user as the administrator, and make sure to use
that user in the subsequent commands as necessary.

### Instantiate the Template
When we imported the imagestreams into the `openshift` namespace earlier, we
also brought in JBoss EAP and Tomcat S2I builder images.

Take a look at the `eap6-basic-sti.json` in the `beta4` folder.  You'll see that
there are a number of bash-style variables (`${SOMETHING}`) in use in this
template. This template is already configured to use the EAP builder image, so
we can use the web console to simply isntantiate it in the desired way.

We want to:

* set the application name to *helloworld*
* set the application hostname to *helloworld.cloudapps.example.com*
* set the Git URI to
    *https://github.com/jboss-developer/jboss-eap-quickstarts/*
* set the Git ref to *6.4.x*
* set the Git context dir to *helloworld*
* set Github and Generic trigger secrets to *secret*

Ok, we're ready:

1. Add the `eap6-basic-sti.json` template to your project using the commandline:

        osc create -f eap6-basic-sti.json

1. Create the secret for the EAP template:

        osc create -f eap-app-secret.json

1. Go into the web console.

1. Find the project you created and click on it.

1. Click the "Create..." button.

1. Click the "Browse all templates..." button.

1. Click the "eap6-basic-sti" example.

1. Click "Select template".

Now that you are on the overview page, you'll have to click "Edit Paremeters"
and fill in the values with the things we wanted above. Hit "Create" when you
are done.

In the UI you will see a bunch of things get created -- several services, some
routes, and etc.

### Update the BuildConfig
The template assumes that the imageStream exists in our current project, but
that is not the case. The EAP imageStream exists in the `openshift` namespace.
So we need to edit the resulting `buildConfig` and specify that.

    osc edit bc helloworld

You will need to edit the `strategy` section to look like the following:

    strategy:
      sourceStrategy:
        from:
          kind: ImageStreamTag
          name: jboss-eap6-openshift:6.4
          namespace: openshift

**REMEMBER** indentation is *important* in YAML.

### Watch the Build
In a few moments a build will start. You can watch the build if you choose, or
just look at the web console and wait for it to finish. If you do watch the
build, you might notice some Maven errors.  These are non-critical and will not
affect the success or failure of the build.

### Visit Your Application
We specified a route via defining the application hostname, so you should be able to
visit your app at:

    http://helloworld.cloudapps.example.com/jboss-helloworld

The reason that it is "/jboss-helloworld" and not just "/" is because the
helloworld application does not use a "ROOT.war". If you don't understand this,
it's because Java is confusing.
