
:noaudio:

ifdef::revealjs_slideshow[]

[#cover,data-background-image="image/1156524-bg_redhat.png" data-background-color="#cc0000"]

== &nbsp;
:noaudio:

[#cover-h1]
Red Hat OpenShift Enterprise Implementation

[#cover-h2]
OpenShift 3 Installation

[#cover-logo]
image::{revealjs_cover_image}[]

endif::[]

== Module Topics

* Installation Scenario
* Installation Methods
* Installation Workflow
* Prerequisites
* Host Preparation
* Docker Installation
* OpenShift Enterprise Installation
* Regions and Zones
* Registry Deployment
* Default HAProxy Router Deployment
* OpenShift Enterprise Population
* Persistent Storage Using NFS

ifdef::showscript[]


=== Transcript
Welcome to module three of the OpenShift Enterprise Implementation course.

This module reviews the installation process and shows how to configure the
 scheduler, registry, and router containers, as well as how to set up persistent
  storage.

endif::showscript[]
== Installation Scenario

[cols="1,1,4,2"]
|=======================================================================
|Host |IP |Description |Labels
|*Master1* |`192.168.0.101` a|* Hosts the web console, _API service_, and _etcd store_
* Cannot be scheduled
* No need for label a|* *Region*: `NA`
* *Zone*: `NA`
* *Schedulable*: `false`
|*InfraNode1* |`192.168.0.251` |Used only to host
_Infrastructure_ containers such as `Registry` and HAProxy `Router` a|* *Region*: `Infra`
* *Zone*: `infranode`
* *Schedulable*: `true`
|*Node1* |`192.168.0.201` a|* Part of primary region
* Cohosts all user workloads a|* *Region*: `Primary`
* *Zone*: `East`
* *Schedulable*: `true`
|*Node2* |`192.168.0.202` a|* Part of primary region
* Cohosts all user workloads a|* *Region*: `Primary`
* *Zone*: `West`
* *Schedulable*: `true`
|`oselab` |`192.168.0.3` |Simulates the corporate DNS server
and NFS backend
|=======================================================================


ifdef::showscript[]

=== Transcript
In this sample scenario, you set up four hosts to simulate an OpenShift
 Enterprise 3 environment.

You use the *master* host to host some of the management components of OpenShift
 Enterprise, such as the web console, the _API service_, and the _etcd store_.

While *Infranode* is a regular node like the others, you dedicate it to be used
 only for _Infrastructure containers_ by changing its labels. This is strictly a
  design choice, not a mandatory constraint.

The two remaining *nodes* are hosts that you use to run containers (pods) in the
 OpenShift Enterprise environment. You label the two nodes to be in the same
  region but different zones. This simulates the use case of an environment in a
   single region and possibly two cloud availability zones.

endif::showscript[]
== Installation Methods

.Quick Install
** Lets you use interactive CLI utility to install OpenShift across set of hosts
** Installer made available by installing utility package
 (`atomic-openshift-utils`) on provisioning host
** https://install.openshift.com
** Uses Ansible playbooks in background
** Does not assume familiarity with Ansible

.Advanced Install
** For complex environments requiring deeper customization of installation and
 maintenance
** Uses Ansible playbooks
** Assumes familiarity with Ansible



ifdef::showscript[]

=== Transcript

There are two ways to install OpenShift Enterprise 3.
The Quick Install method uses an interactive CLI utility to install OpenShift
 across a set of hosts. The installer is made available by installing the
  utility package `atomic-openshift-utils`, available in the OpenShift
   Repository, on the provisioning host.

The Quick Install method uses Ansible in the background of the
 interactive CLI utility.

For more complex environments where deeper customization of installation and
 maintenance is required, an Advanced Install method using Ansible playbooks is
  available. This method assumes familiarity with Ansible.

This module focuses on the Quick Install method.



endif::showscript[]
== Installation Workflow

* Prerequisites
** System requirements
** Set up DNS
** Prepare host
* OpenShift Enterprise installation
** Download and run installation utility
* Post-install tasks
** Deploy integrated Docker registry
** Deploy HAProxy router
** Populate OpenShift installation with image streams and templates
** Configure authentication and create project for users
** Set up and configure NFS server for use with _persistent volumes_

ifdef::showscript[]
=== Transcript

Use this workflow to install OpenShift Enterprise. You start by making sure that
 the system prerequisites have been met. These are the basic requirements for a
  viable OpenShift Enterprise environment. They include setting up the DNS
   requirements and preparing the hosts for OpenShift Enterprise deployment.

Next, you use the CLI installation utility to install the OpenShift Enterprise
 software.

Finally, you deploy some containerized infrastructure components such as the
 default router and the integrated Docker registry. You also configure
  authentication and set up an NFS server to serve your persistent volume
   requests.

endif::showscript[]
== Prerequisites

.System Requirements

* Masters
** Physical or virtual system
** Base OS: Red Hat Enterprise Linux 7.1 with _Minimal_ installation option
** Two vCPUs
** Minimum 8 GB RAM
** Minimum 30 GB hard disk space

* Nodes

** Physical or virtual system, or instance running on public IaaS
** Base OS: Red Hat Enterprise Linux 7.1 with _Minimal_ installation option
** Docker 1.9.1 or later
** One vCPU
** Minimum 8 GB RAM
** Minimum 15 GB hard disk space
** An additional minimum 15 GB unallocated space to be configured using
 `docker-storage-setup;`



ifdef::showscript[]
=== Transcript

These are the minimal requirements for a viable OpenShift Enterprise
 environment, for both masters and nodes.

To lower costs, the servers in the learning environment are not configured with
 the recommended settings.

endif::showscript[]
== Prerequisites

.DNS Setup

* To make environment accessible externally, create wildcard DNS entry
** Points to _node_ hosting _Default Router Container_
** Resolves to OpenShift router IP address
* In lab and examples, this is `infranode1` server
** If environment uses multiple _routers_ (HAProxy instances), use external load
 balancer or round-robin setting
* Example: Create wildcard DNS entry for `cloudapps` in DNS server
** Has low TTL
** Points to public IP address of host where the router is deployed:
+
----
*.cloudapps.example.com. 300 IN  A 85.1.3.5
----


ifdef::showscript[]

=== Transcript



To make the OpenShift Enterprise environment accessible externally, you create a
 wildcard DNS entry that points to the _node_ that is hosting the
  _Default Router Container_.

In this lab and examples, this is the `infranode1` server. If your environment
 uses multiple _routers_ (HAProxy instances), which is likely, you use an
  external load balancer or round-robin setting to use them.

The wildcard for a DNS zone must resolve ultimately to the IP address of the
 OpenShift Enterprise router.

For example, you can use the code shown here to create a wildcard DNS entry for
 `cloudapps` in your DNS Server, or something similar. The entry has a low TTL
  and points to the public IP address of the host where the router will be
   deployed.

endif::showscript[]
== Host Preparation

.Overview

* To prepare your hosts for OpenShift Enterprise 3:
** Install Red Hat Enterprise Linux 7.2
** Register hosts with `subscription-manager`
** Manage base packages:
*** `git`
*** `net-tools`
*** `bind-utils`
*** `iptables-services`
** Manage services:
*** Disable `firewalld`
*** Enable `iptables-services`
**  Install Docker 1.9.1 or later
** Make sure master does not require password for communication

ifdef::showscript[]

=== Transcript

To prepare the hosts to use with OpenShift Enterprise, consult the
 documentation. This ensures you always have the most up-to-date information.

The basic steps for preparing hosts are as follows:

* Perform a base installation of Red Hat Enterprise Linux 7.2 for master and
 node hosts.
* Use `subscription-manager` to register all the hosts to Red Hat Enterprise
 Linux 7.1 and OpenShift Enterprise 3 repositories.
* Install some utility packages, including `git`, `net-tools`, `bind-utils`, and
 `iptables-services`.
* Disable `firewalld` and enable `iptables-services`.
* Install Docker 1.9.1 or later and configure the storage back end for images.
* Make sure that the master hosts can issue remote commands on the nodes
 without requiring a password.

If you want to create a scalable environment, you should create an image
 template for the node--depending on your infrastructure provider--and then, to
  save time, spin up a new instance or VM in which these steps have already been
   performed. You should then run the installer script to add the new node to
    the OpenShift Enterprise environment/cluster.


endif::showscript[]
== Host Preparation

.Password-Less Communication

* Ensure installer has password-less access to hosts
** Ansible requires user with access to all hosts
** To run installer as non-root user, configure password-less `sudo` rights on
 each destination host
* Example: To generate SSH key on host where you invoke installation process:
+
----
# ssh-keygen
# ssh-copy-id root@node1.example.com
----
+
[NOTE]
Do not use password when generating key.

* To easily distribute SSH keys, use bash loop

ifdef::showscript[]

=== Transcript
For the installation to succeed, the installer needs to run commands on each
 member in the OpenShift Enterprise environment without requiring a password
  every time.

Ansible, which is used to run the installation process, requires a user that has
 access to all hosts. For running the installer as a non-root user, you must
  configure password-less `sudo` rights on each destination host.

To achieve this, you generate a key and copy it to the root user's ID on each
 member of the OpenShift Enterprise cluster. Do not use a password when
  generating the key.

An easy way to distribute your SSH keys is by using a bash loop.

endif::showscript[]
== Host Preparation

.Firewalls: Component Communication

* OpenShift Enterprise relies on `iptables`
* Must be running, and following ports must be open
* Allows communication between components


[cols="2,1,5"]
|=======================================================================
|Communication |Port |Description
|*Node to Node* |`4789` (UDP) |Required for SDN communication between pods on
 separate hosts.
|*Node to Master* |`53` or `8053`|Provides DNS services within the environment (not DNS
   for external access). New installations will use 8053 by default so that dnsmasq may
 be configured.
| |`4789` (UDP) |Required between nodes for SDN communication between pods on
 separate
 hosts
| |`8443` |Provides access to the API
|*Master to Node* |`10250` |Endpoint for master communication with nodes
| |`4789` (UDP) |Required between nodes for SDN communication between pods on
 separate
 hosts
|*Master to Master*  |`4789` (UDP) |Required between nodes for SDN communication
 between pods on separate hosts
| |`53` or `8053` |  Provides internal DNS services. New installations will use
 8053 by default so that dnsmasq may be configured.
| |`2379` | Used for standalone etcd (clustered) to accept changes in state.
| |`2380` | etcd requires this port be open between masters for leader election
 and peering connections when using standalone etcd (clustered).
| |`4001` | Used for embedded etcd (non-clustered) to accept changes in state.
|=======================================================================

NOTE: The OpenShift installer will open most of these ports for you.

ifdef::showscript[]

=== Transcript

OpenShift Enterprise relies heavily on `iptables` in the background. So
 `iptables` must be running, and various ports must be open to allow
  communication between OpenShift Enterprise components.

This table shows the ports you need to open. Note that port `4789` must be
 accessible on any host in the cluster, because it is required for the SDN
  overlay.

Also note that the master is running a local DNS server. Do not confuse this
 server with the DNS server that holds the wildcard DNS entry. This is a DNS
  server (`SkyDNS`) used to resolve local resources--for example, each _service_
   you define in OpenShift Enterprise has a `dns` entry that you can resolve
    locally.

endif::showscript[]
== Host Preparation

.Firewalls: External Access

* To allow external access to environment, open the following ports:

[cols="2,1,5"]
|=======================================================================
|Type |Port |Description
|*External - Master* |`8443` a|* CLI and IDE plug-ins communicate via REST to
 this port
* Web console runs on this port
|*External - Node* (or nodes) hosting `Default Router` (HAProxy) container |`80`, `443` a|* Ports opened and bound to `Default Router` container
* Proxy
communication from external world to _pods_ (containers) internally.
|=======================================================================

* Sample topology:
** Infrastructure _nodes_ running in DMZ
** Application hosting _nodes_, master, other supporting infrastructure running
 in more secure network

ifdef::showscript[]

=== Transcript
To allow users from outside your LAN to access the web console or make API calls
 to OpenShift Enterprise, you need to expose the master's `8443` port to those
  users' networks.

The `Default Router` listens on its host's ports `80` and `443` for incoming
 requests. To allow external access to your pods, you only need to expose the
  node hosting the `Default Router`.

Consider a topology where only the infrastructure _nodes_ are running in a DMZ
 and the application hosting _nodes_, master, and other supporting
  infrastructure are running in a more secure network.

endif::showscript[]
== Host Preparation

.Networking and Miscellaneous

* Install software packages:
+
----
# yum install wget git net-tools bind-utils iptables-services bridge-utils bash-completion
----

*  Update software before installation:
+
----
# yum update -y
----

[NOTE]
Red Hat highly recommends installing `bash-completion` to enable command completion
with the *Tab* key.

ifdef::showscript[]

=== Transcript

You need to install the software packages shown here on your master and run a
 `yum` update on your hosts before using the installer and installing OpenShift
  Enterprise 3.



endif::showscript[]
== Docker Installation

.Installing Docker

* Must install Docker version 1.9.1 or later from `rhel-7-server-ose-3.2-rpms`
 and have it running on master and node hosts before installing OpenShift
  Enterprise:

** Install Docker:
+
----
# yum install docker
----

** Edit `/etc/sysconfig/docker` and add `--insecure-registry 172.30.0.0/16`
to `OPTIONS` parameter
* Example:
+
----
OPTIONS=--selinux-enabled --insecure-registry 172.30.0.0/16
----
+
[NOTE]
====
** `--insecure-registry` instructs Docker daemon to trust any
Docker registry on `172.30.0.0/16` subnet
** Will deploy local registry under this subnet
====

ifdef::showscript[]

=== Transcript
Each node, including the master, requires Docker to be installed and configured.
 You must use version 1.9.1 or later of the docker software.

Use the commands shown here to install Docker and add the `--insecure-registry`
 option to the `OPTIONS` parameter.

The `--insecure-registry` option instructs the Docker daemon to trust any Docker
 registry on the `172.30.0.0/16` subnet, without requiring a certificate.

You will deploy your local registry under this subnet.


endif::showscript[]
== Docker Installation

.Docker Storage Configuration

* Docker default loopback storage mechanism:
** Not supported for production
** Appropriate for proof of concept environments
* For production environments:
** Create thin-pool logical volume
** Reconfigure Docker to use volume
* To do this use `docker-storage-setup` script after installing but before using
 Docker
** Script reads configuration options from `/etc/sysconfig/docker-storage-setup`

ifdef::showscript[]

=== Transcript

Docker’s default loopback storage mechanism is not supported for production use
 and is only appropriate for proof of concept environments. For production
  environments, you must create a thin-pool logical volume and reconfigure
   Docker to use that volume.

You can use the `docker-storage-setup` script to create a thin-pool device and
 configure Docker’s storage driver after installing Docker. Do not use Docker
  until the storage driver is configured.

The script reads configuration options from the
 `/etc/sysconfig/docker-storage-setup` file.

endif::showscript[]
== Docker Installation

.Storage Options

* When configuring `docker-storage-setup`, examine available options

* Before starting `docker-storage-setup`, reinitialize Docker:
+
----
# systemctl stop docker
# rm -rf /var/lib/docker/*
----

* Create thin-pool volume from free space in volume group where root filesystem
 resides:
** Requires no configuration
+
----
# docker-storage-setup
----

* Use existing volume group to create thin-pool:
** Example: `docker-vg`
+
----

# cat /etc/sysconfig/docker-storage-setup
DEVS=/dev/vdb
VG=docker-vg
# docker-storage-setup
----

ifdef::showscript[]

=== Transcript

You must edit the `/etc/sysconfig/docker-storage-setup` file to work as an
 answer file for `docker-storage-setup`.

When you configure the `docker-storage-setup` script for your environment, some
 options are available based on your storage configuration.

Before you start the `docker-storage-setup` script, you must reinitialize
 Docker.

You then start the script and create a thin-pool volume from the remaining free
 space in the volume group where your root filesystem resides. This requires no
  configuration.

Then you use an existing volume group, in this example `docker-vg`, to create a
 thin-pool.

endif::showscript[]
== Docker Installation

.Storage Options: Example

* Use unpartitioned block device to create new volume group and thin-pool:
** Example: Use `/dev/vdc device` to create `docker-vg`:
+
----
# cat /etc/sysconfig/docker-storage-setup
DEVS=/dev/vdb
VG=docker-vg
SETUP_LVM_THIN_POOL=yes
# docker-storage-setup
----

* Verify configuration:
** Should have `dm.thinpooldev` value in
`/etc/sysconfig/docker-storage` and `docker-pool` device
+
----
# lvs
LV                  VG        Attr       LSize  Pool Origin Data%  Meta% Move
docker-pool         docker-vg twi-a-tz-- 48.95g             0.00   0.44

# cat /etc/sysconfig/docker-storage
DOCKER_STORAGE_OPTIONS=--storage-driver devicemapper --storage-opt dm.fs=xfs --storage-opt dm.thinpooldev=/dev/mapper/docker--vg-docker--pool


----

* Restart `Docker` daemon

ifdef::showscript[]

=== Transcript

In this example, you use the `/dev/vdb` unpartitioned block device to create the
 `docker-vg` volume group that the `Docker` daemon will use.

To verify that the volume is created and configured, use the `lvs` command and
 view the `/etc/sysconfig/docker-storage` file. You should have a
 `dm.thinpooldev` value in the `/etc/sysconfig/docker-storage` file and a
  `docker-pool` device.

After you verify the setup, restart the `Docker` daemon.

endif::showscript[]
== OpenShift Enterprise Installation

* Install OpenShift `utils` package that includes installer:
+
----
# yum -y install atomic-openshift-utils
----

* Run following on host that has SSH access to intended master and nodes:
+
----
$ atomic-openshift-installer install
----

* Follow onscreen instructions to install OpenShift Enterprise
** Installer asks for hostnames or IPs of masters and nodes and configures them
 accordingly
** Configuration file with all information provided is saved in
 `~/.config/openshift/installer.cfg.yml`
*** Can use this as _answer file_

ifdef::showscript[]

=== Transcript

To install OpenShift Enterprise 3, install the OpenShift `utils` package that
 includes the installer, and run the installer CLI utility on a host that has
  password-less SSH access to your intended master and nodes.

The installer asks for the hostnames or IPs of the masters and nodes and
 configures them accordingly.

A configuration file with all the information provided is saved in
 `~/.config/openshift/installer.cfg.yml`.
You can use this as an _answer file_
  for future installations.


endif::showscript[]
== Regions and Zones

* After installation, need to label nodes
** Lets scheduler use _logic_ defined in `scheduler.json` when provisioning pods
* OpenShift Enterprise 2.0 introduced _regions_ and _zones_
** Let organizations provide topologies for application resiliency
** Apps spread throughout zones within region
** Can make different regions accessible to users
* OpenShift Enterprise 3 _topology-agnostic_
** Provides advanced controls for implementing any topologies
** Example: Use _regions_ and _zones_
*** Other options: _Prod_ and _Dev_, _Secure_ and _Insecure_, _Rack_ and _Power_
** Labels on nodes handle assignments of _regions_ and _zones_ at node level
+
----
# oc label node master1.example.com region="infra" zone="na"
# oc label node infranode1.example.com region="infra" zone="infranodes"
# oc label node node1.example.com region="primary" zone="east"
# oc label node node2.example.com region="primary" zone="west"
----

ifdef::showscript[]

=== Transcript

After you install OpenShift Enterprise, you need to label the nodes. Labeling
 the nodes lets the scheduler use _logic_ defined in the `scheduler.json` file
  when it provisions pods in your environment.

OpenShift Enterprise 2.0 introduced the specific concepts of _regions_ and
 _zones_ to let organizations provide some topologies for application
  resiliency. Applications are spread throughout the zones within a region and,
   depending on the way you configure OpenShift Enterprise, you can make
    different regions accessible to users.

OpenShift Enterprise 3 is _topology-agnostic_ and  provides advanced controls
 for implementing whatever topologies you create.

The example shown here uses the concept the _regions_ and _zones_.

Other options you can use include _Prod_ and _Dev_,
_Secure_ and _Insecure_, or _Rack and Power_.

The labels on the nodes handle the assignments of _regions_ and _zones_ at the
 node level.

endif::showscript[]
== Registry Deployment

.Registry Container

* OpenShift Enterprise:
** Builds Docker images from source code
** Deploys them
** Manages lifecycle
* To enable this, deploy Docker registry in OpenShift Enterprise environment
* OpenShift Enterprise runs registry in pod on node, just like any other workload
* Deploying registry creates _service_ and _deployment configuration_
** Both called `docker-registry`
* After deployment, pod created with name similar to `docker-registry-1-cpty9`

* To control where registry is deployed, use `--selector` flag to specify desired target

ifdef::showscript[]

=== Transcript

OpenShift Enterprise can build Docker images from your source code, deploy them,
 and manage their lifecycle. To enable this, you deploy an internal, integrated
  Docker registry in your OpenShift Enterprise environment.

OpenShift runs the registry in a pod on a node, just like any other workload.

Deploying the registry creates a _service_ and a _deployment configuration_,
 both called `docker-registry`.

After successful deployment, a pod is created with a name similar to
 `docker-registry-1-cpty9`.

To control where your registry is deployed, you use the `--selector` flag to
 specify your target by picking the labels you want to match.

You can also edit the "default" project/namespace to select the default node
 selector.
endif::showscript[]
== Registry Deployment

.Deploying Registry

* Environment includes `infra` region and dedicated
`infranode1` host
** Good practice for highly scalable environment
** Use better-performing servers for nodes or place them in DMZ for external
 access only

* To deploy registry anywhere in environment:
+
----
$ oadm registry --config=/etc/origin/master/admin.kubeconfig \
    --service-account=registry
----

* To ensure `registry` pod is hosted in `infra` region only:
+
----
$ oadm registry --config=/etc/origin/master/admin.kubeconfig \
    --service-account=registry \
    --selector='region=infra'
----


ifdef::showscript[]

=== Transcript

You can deploy the _Registry container_ anywhere in the OpenShift Environment.
 In your lab environment, you will create a region called `infra` and dedicate
  the `infranode1` host for it.

This is a good practice for a highly scalable environment. You might want to use
 better-performing servers for these nodes or place them in the DMZ so they can
 be accessed by external networks.

To deploy a registry anywhere in the environment, run the first command shown
 here.

To ensure that the `registry` pod is hosted only in the `infra` region, run the
 second command shown here with the `--selector` option added.

endif::showscript[]
== Registry Deployment

.NFS Storage for the Registry

* Registry stores Docker images, metadata
* If you deploy a pod with registry:
** Uses ephemeral volume
** Destroyed if pod exits
*** Images built or pushed into registry disappear
* For production:
** Use persistent storage
** Use `PersistentVolume` and `PersistentVolumeClaim` objects for storage for
 registry
* For non-production:
** Other options exist
** Example: `--mount-host`:
+
----
$ oadm registry --service-account=registry \
     --config=/etc/origin/master/admin.kubeconfig \
     --selector='region=infra' \
     --mount-host=<path>
----
+
*** Mounts directory from node on which registry container lives
*** If you scale up `docker-registry` deployment configuration, registry pods
 and containers might run on different nodes

ifdef::showscript[]

=== Transcript

The registry stores Docker images and metadata. If you simply deploy a pod with
 the registry, it uses an ephemeral volume that is destroyed if the pod exits.
  Any images built or pushed into that registry will disappear.

For production use, you should build persistent storage using `PersistentVolume`
 and `PersistentVolumeClaim` objects for storage for the registry.

For non-production use, other options exist to provide persistent storage, such
 as the `--mount-host` option.

The `--mount-host` option mounts a directory from the node on which the registry
 container lives. If you scale up the `docker-registry` deployment
  configuration, it is possible that your registry pods and containers will run
   on different nodes.

endif::showscript[]
== Default HAProxy Router Deployment

* `Default Router` (aka `Default HA-Proxy Router`, other names):
** Modified deployment of HAProxy
** Entry point for traffic destined for services in OpenShift Enterprise
 installation
* HAProxy-based router implementation provided as default template router
 plug-in
** Uses `openshift3/ose-haproxy-router` image to run HAProxy instance alongside
 and router plug-in
** Supports HTTP(S) traffic and TLS-enabled traffic via SNI only
** Hosted inside OpenShift Enterprise
** Essentially a proxy

* Default router’s pod listens on host network interface on port 80 and 443
** Default router's container listens on external/public ports
** Router proxies external requests for route names to IPs of actual pods
 identified by service associated with route

ifdef::showscript[]

=== Transcript

The `Default Router`, also known as the `Default HA-Proxy Router` and many other
 similar names, is a modified deployment of HAProxy. It serves as the entry
  point for all traffic destined for services in your OpenShift Enterprise
   installation.

An HAProxy-based router implementation is provided as the default template
 router plug-in. It uses the `openshift3/ose-haproxy-router` image to run an
  HAProxy instance alongside the template router plug-in. 
  The router currently supports HTTP(S) traffic and TLS-enabled traffic via
   SNI only. Like the registry and any other workload, it is hosted inside
    OpenShift Enterprise.

While it is called a _router_, it is essentially a _proxy_.

The default router’s pod listens on its host's network interface on ports 80 and
 443. Unlike most containers, which listen only on private IPs, the default
  router's container listens on external/public ports. The router proxies
   external requests for route names to the IPs of actual pods identified by the
    service associated with the route.

endif::showscript[]
== OpenShift Enterprise Population

* Can populate OpenShift Enterprise installation with Red Hat-provided
 _image streams_ and _templates_
** Make it easy to create new applications
* *Template*: Set of resources you can customize and process to produce
 configuration
** Defines list of parameters you can modify for consumption by containers
* *Image Stream*:
** Comprises of one or more Docker images identified by tags
** Presents single virtual view of related images

ifdef::showscript[]

=== Transcript
You can populate your OpenShift Enterprise installation with a useful set of
 Red Hat-provided _image streams_ and _templates_. These make it easy for
  developers to create new applications. The installer automatically adds image
   streams and common templates.

A _template_ describes a set of resources intended to be used together that you
 can customize and process to produce a configuration. Each template defines a
  list of parameters that you can modify for consumption by a container. This is
   somewhat similar to a OpenShift Enterprise 2.0 _quickstart_.

An _image stream_ comprises of one or more Docker images identified by tags. An
 image stream presents a single virtual view of related images.

endif::showscript[]
== OpenShift Enterprise Population

* Core set of image streams defines:
** Images you can use to build applications:
*** Node.js
*** Perl
*** PHP
*** Python
*** Ruby

** Images for databases:
*** MongoDB
*** MySQL
*** PostgreSQL

[NOTE]
The install utility installs these image streams and others automatically.


ifdef::showscript[]

=== Transcript

The core set of image streams defines images you can use to build Node.js,
 Perl, PHP, Python, and Ruby applications. It also defines images for MongoDB,
  MySQL, and PostgreSQL databases.

  The install utility installs these image streams and others automatically.


endif::showscript[]
== OpenShift Enterprise Population

.Image Streams

* xPaaS middleware image streams provide images for:
** Red Hat JBoss Enterprise Application Platform
** Red Hat JBoss Web Server
** Red Hat JBoss A-MQ

* Can use images to build applications for those platforms

* To create or delete core set of image streams that use Red Hat Enterprise
 Linux 7-based images:
+
----
oc create|delete -f \
    examples/image-streams/image-streams-rhel7.json \
    -n openshift
----

** To create image streams for xPaaS middleware images:
+
----
$ oc create|delete -f \
    examples/xpaas-streams/jboss-image-streams.json
    -n openshift
----

ifdef::showscript[]

=== Transcript

Red Hat provides xPaaS middleware image streams for Red Hat JBoss Enterprise
 Application Platform, Red Hat JBoss Web Server, and Red Hat JBoss A-MQ.

You can use the image streams to build application for these platforms.

Use the code shown here to create or delete the core set, xPaaS and JBoss image
 streams.



endif::showscript[]
== OpenShift Enterprise Population

.Database Service Templates

* Database service templates make it easy to run database instance
** Other components can use
* Two templates provided for each database
** To create core set of database templates:
+
----
$ oc create -f \
    examples/db-templates -n openshift
----

** Can easily instantiate templates after creating them
** Gives quick access to database deployment

NOTE: These templates and others are *installed automatically* by the
 install utility.

ifdef::showscript[]

=== Transcript

You can also deploy database templates that make it easy to run a database
 instance that other components can use.

For each database--MongoDB, MySQL, and PostgreSQL--two templates are provided.

Use the second code sample shown here to create the core set of database
 templates.

After you create the templates, you can easily instantiate them. This gives the
 templates quick access to a database deployment.

endif::showscript[]
== OpenShift Enterprise Population

.QuickStart Templates

* Define full set of objects for running application:
** *Build configurations*: Build application from source located in GitHub
 public repository
** *Deployment configurations*: Deploy application image after it is built
** *Services*: Provide internal load balancing for application pods
** *Routes*: Provide external access and load balancing to application
* To create core QuickStart templates:
+
----

$ oc create|delete -f \
    examples/quickstart-templates -n openshift

----

NOTE: These templates and others are *installed automatically* by the
 install utility.


ifdef::showscript[]

=== Transcript
The QuickStart templates define a full set of objects for a running application.
 This includes the following:

* Build configurations, to build the application from source located in a GitHub
 public repository.
* Deployment configurations, to deploy the application image after it is built.
* Services, to provide internal load balancing for the application pods.
* Routes, to provide external access and load balancing to the application.

Use the code shown here to create or delete the core QuickStart templates.

endif::showscript[]
== Persistent Storage Using NFS

.Overview

* Can provision OpenShift Enterprise cluster with persistent storage using NFS
* `Persistent Volume` framework:
** Lets administrators provision cluster with persistent storage
** Gives users access to resources without knowledge of underlying
 infrastructure
* Storage must exist in underlying infrastructure before mounting as volume in
 OpenShift Enterprise

NOTE: There are many other supported storage back ends; they will be covered
 later in the training. In our lab we use NFS for persistent storage.

ifdef::showscript[]

=== Transcript
You can provision your OpenShift Enterprise cluster with persistent storage
 using NFS. There are many other supported storage back ends; they will be
  covered later in the training. In our lab we use NFS for persistent
   storage.

A `Persistent Volume` framework lets administrators provision a cluster with
 persistent storage and gives users a way to request those resources without
  having any knowledge of the underlying infrastructure.

Storage must exist in the underlying infrastructure before you can mount it as
 a volume in OpenShift Enterprise.

endif::showscript[]
== Persistent Storage Using NFS

* To create a persistent volume that can be claimed by a pod, you must
 create a `PersistentVolume` object in pod's `Project`
* After `PersistentVolume` is created, a `PersistentVolumeClaim` must be
 created to ensure other pods and projects do not try to use `PersistentVolume`

.Persistent Volume Object Definition
[source,yaml]
----
{
  "apiVersion": "v1",
  "kind": "PersistentVolume",
  "metadata": {
    "name": "pv0001"
  },
  "spec": {
    "capacity": {
        "storage": "5Gi"
    },
    "accessModes": [ "ReadWriteOnce" ],
    "nfs": {
        "path": "/tmp",
        "server": "172.17.0.2"
    },
    "persistentVolumeReclaimPolicy": "Recycle"
  }
}
----

ifdef::showscript[]

=== Transcript

To create a persistent volume that can be claimed by a pod, you must create
 a `PersistentVolume` object in the pod's `Project`.
After a `PersistentVolume` is created, a `PersistentVolumeClaim` must also be
 created to ensure that other pods and projects do not try to use the
  `PersistentVolume`.

Here is the definition of the `Persistent Volume` object.

You must specify the storage capacity, access mode, and details of your NFS host.

endif::showscript[]
== Persistent Storage Using NFS

.Volume Security

* `PersistentVolume` objects created in context of _project_
* User request storage with `PersistentVolumeClaim` object in same project
** Claim lives only in user's namespace
** Can be referenced by pod within same namespace
* Attempt to access persistent volume across project causes pod to fail
* NFS volume must be mountable by all nodes in cluster

ifdef::showscript[]

=== Transcript

You create `PersistentVolume` objects in the context of a specific _project_.

A user can request storage with a `PersistentVolumeClaim` object in the same
 _project_. The claim lives in the user's namespace or project, and can be
  referenced only by a pod within that same namespace. Any attempt to access a
   persistent volume across a project causes the pod to fail.

Each NFS volume must be mountable by all nodes in the cluster.

endif::showscript[]
== Persistent Storage Using NFS

.SELinux and NFS Export Settings

* Default: SELinux does not allow writing from pod to remote NFS server
** NFS volume mounts correctly but is read-only
* To enable writing in SELinux on each node:
+
----
# setsebool -P virt_use_nfs 1
----

* Each exported volume on NFS server should conform to following:
** Set each export option in `/etc/exports` as follows:
+
----
/example_fs *(rw,all_squash)
----

** Each export must be owned by `nfsnobody` and have following permissions:
+
----
# chown -R nfsnobody:nfsnobody /example_fs
# chmod 777
----

ifdef::showscript[]

=== Transcript

You need to configure your NFS server exports.

By default, SELinux does not allow writing from a pod to a remote NFS server.
 The NFS volume mounts correctly, but is read-only.

To configure SElinux to let the nodes use NFS in the way required for OpenShift
 Enterprise `Persistent Volumes`, use the `setsebool` command on each node, as
  shown in the first example here.


In addition, configure each exported volume on the NFS server itself so that
 each export option is set in the `/etc/exports` file, as shown in the second
  example here, and each export is owned by `nfsnobody` and has the permissions
   shown in the third example here.

endif::showscript[]
== Persistent Storage Using NFS

.Resource Reclamation
* OpenShift Enterprise implements Kubernetes `Recyclable` plug-in interface
** Reclamation tasks based on policies set by `persistentVolumeReclaimPolicy`
 key in `PersistentVolume` object definition
* Can _reclaim_ volume after it is released from claim

* Can set `persistentVolumeReclaimPolicy` to `Retain` or `Recycle`:
** `Retain`: Volumes not deleted
*** Default setting for key
** `Recycle`: Volumes scrubbed after being released from claim
* Once recycled, can bind NFS volume to new claim

ifdef::showscript[]

=== Transcript

OpenShift Enterprise implements the Kubernetes `Recyclable` plug-in interface.
 Reclamation tasks are based on policies set by the
  `persistentVolumeReclaimPolicy` key in the `PersistentVolume` object
   definition. After a volume is released from its claim--that is, after the
    user delete the `PersistentVolumeClaim` bound to the volume--the volume
     can be _reclaimed_.

You can set the `persistentVolumeReclaimPolicy` key to `Retain` or `Recycle`.

By default, persistent volumes are set to `Retain`. In this case, volumes are
 not deleted.

NFS volumes set to `Recycle` are scrubbed--that is, `rm -rf` is run on the
 volume--after being released from their claim. After you recycle an NFS volume,
  you can bind it to a new claim.

endif::showscript[]
== Persistent Storage Using NFS

.Automation

* Can provision OpenShift Enterprise clusters with persistent storage using NFS:
** Use disk partitions to enforce storage quotas
** Enforce security by restricting volumes to namespace that has claim to them
** Configure reclamation of discarded resources for each persistent volume

* Can use scripts to automate these tasks
* See sample Ansible playbook:
https://github.com/openshift/openshift-ansible/tree/master/roles/kube_nfs_volumes


ifdef::showscript[]

=== Transcript

The preceding sections have discussed the ways you can provision OpenShift
 Enterprise clusters with persistent storage using NFS:

* Use disk partitions to enforce storage quotas
* Enforce security by restricting volumes to the namespace that has a claim to
 them
* Configure reclamation of discarded resources for each persistent volume

After you set up your OpenShift Enterprise environment and clusters to use
 persistent storage, it is easy to create scripts to automate these tasks.

To help you get started, see the example Ansible playbook at the web address
 shown here.

endif::showscript[]
== Summary

* Installation Scenario
* Installation Methods
* Installation Workflow
* Prerequisites
* Host Preparation
* Docker Installation
* OpenShift Enterprise Installation
* Regions and Zones
* Registry Deployment
* Default HAProxy Router Deployment
* OpenShift Enterprise Population
* Persistent Storage Using NFS

ifdef::showscript[]


=== Transcript

This module reviewed the OpenShift Enterprise 3 installation process and showed
 how to configure the scheduler, registry, and router containers, as well as how
  to set up persistent storage.

endif::showscript[]
