== &nbsp;
:noaudio:

ifdef::revealjs_slideshow[]
[#cover,data-background-image="image/1156524-bg_redhat.png" data-background-color="#cc0000"]

[#cover-h1]
Red Hat OpenShift Enterprise Implementation

[#cover-h2]
Resource Management

[#cover-logo]
image::{revealjs_cover_image}[]

endif::[]

== Module Topics
:noaudio:
:numbered!:

Module 05 : Resource Management

In This chapter we covered:

* OpenShift Resources
* Projects and Users
* Client tool authentication
* Resource Quota
* Service Accounts
* Persistent Volumes
* Lab: Users, Projects and Quotas
* Lab: Creating Services and Routes
* Lab: Provisioning and Configuring Persistent Volumes


ifdef::showscript[]

=== Transcript
Welcome to Module 05 of the OpenShift Enterprise Implementation course.

endif::showscript[]




== OpenShift Resources
:noaudio:

* There are a number of different resource types in OpenShift 3.
* When creating/destroying apps, scaling, building and etc, we are manipulating OpenShift and Kubernetes resources under the covers.
* Resources can have quotas enforced against them.
* OpenShift Enterprise 3.0 includes different resource types
** Resources in this context can refer to:
*** Hardware Resources: Memory, CPU, other "platform" resources
*** OpenShift Resources: Pods, services, replication controllers


ifdef::showscript[]

=== Transcript
OpenShift Enterprise 3.0 includes a number of different resource types.

Actions such as creating and destroying apps, scaling, building, and so on all result in  manipulating OpenShift Enterprise and Kubernetes resources in the background.

You can enforce quotas against resources. The quota defines limits for multiple resources--for example, in the code sample shown here, the quota called `test-quota` defines limits for several resources.

Within a project, users cannot run actions that result in exceeding these resource limits. Because the quota is enforced at the project level, it is up to the users to allocate resources--specifically, memory and CPU--to their pods and containers.

Resources in this context can refer not only to memory, CPU, and other "platform" resources, but also to pods, services, and replication controllers.

endif::showscript[]


== Projects and Users
:noaudio:

.Users and User Types

* Users : Interaction with OpenShift is associated with a user.
* An OpenShift user object represents an "actor" which may be granted permissions in the system by adding roles to them or to their groups.
* Several types of users can exist:
** *Regular users* - This is the way most interactive OpenShift users will be represented.
*** Regular users are created automatically in the system upon first login, or can be created via the API.
*** Regular users are represented with the User object.
** *System users* - Many of these are created automatically when the infrastructure is defined, mainly for the purpose of enabling the infrastructure to interact with the API securely.
*** They include a cluster administrator (with access to everything), a per-node user, users for use by routers and registries, and various others.
*** There is also an anonymous system user that is used by default for unauthenticated requests.
*** For example : _system:admin_ , _system:openshift-registry_ and _system:node:node1.example.com_

** *Service accounts* - These are special system users associated with projects;
*** some are created automatically when the project is first created,
*** while project administrators can create more for the purpose of defining access to the contents of each project.
*** Service accounts are represented with the ServiceAccount object.
*** For example : _system:serviceaccount:default:deployer_ and  _system:serviceaccount:foo:builder_

* Every user must authenticate in some way in order to access OpenShift. API requests with no authentication or invalid authentication are authenticated as requests by the anonymous system user. Once authenticated, policy determines what the user is authorized to do.

ifdef::showscript[]

=== Transcript


endif::showscript[]


== Projects and Users
:noaudio:

.Concept: "Namespace"
* A Kubernetes namespace provides a mechanism to scope resources in a cluster. In OpenShift, a project is a Kubernetes namespace with additional annotations.
* Namespaces provide a unique scope for:
** Named resources to avoid basic naming collisions.
** Delegated management authority to trusted users.
** The ability to limit community resource consumption.
* Most objects in the system are scoped by namespace, but some are excepted and have no namespace, including nodes and users.



ifdef::showscript[]

=== Transcript


endif::showscript[]



== Projects and Users
:noaudio:

.What are Projects?
* A project is a Kubernetes namespace with additional annotations, and is the central vehicle by which access to resources for regular users is managed.
* A project allows a community of users to organize and manage their content in isolation from other communities.
* Users must be given access to projects by administrators, or if allowed to create projects, automatically have access to their own projects.

* Each project scopes its own set of:
** *Objects:* Pods, services, replication controllers, etc.
** *Policies:* Rules for which users can or cannot perform actions on objects.
** *Constraints:* Quotas for each kind of object that can be limited.
** *Service accounts:* Service accounts act automatically with designated access to objects in the project.

* Cluster administrators can create projects and delegate administrative rights for the project to any member of the user community. Cluster administrators can also allow developers to create their own projects.


ifdef::showscript[]

=== Transcript


endif::showscript[]

== Client tool authentication
.Web Console Authentication
:noaudio:

* When accessing the web console from a browser at `_<master-public-addr>_:8443`,
you are automatically redirected to a login page.

* You can provide your login credentials on this page to obtain a token to make
API calls. After logging in, you can navigate your projects using the web
console.


ifdef::showscript[]

=== Transcript


endif::showscript[]


== Client tool authentication
:noaudio:

.CLI Authentication

* You can authenticate from the command line using the CLI command `oc login`.
+
----
$ oc login
----

* The command's interactive flow helps you establish a session to an OpenShift
server with the provided credentials.

* All configuration options for the `oc login` command, listed in the `oc login
--help` command output, are optional. The following example shows usage with
some common options:

* Here is a handy example, lets say we wanted to authenticate as the Openshift
Cluster Administrator (Usually root User), we could use the following:
+
----
$ oc login -u system:admin -n openshift
----
NOTE: Notice that we are setting the user name and the *project* (_namespace_)
to log in to.


ifdef::showscript[]

=== Transcript


endif::showscript[]


== Client tool authentication
:noaudio:

.CLI Authentication - Continued

* Here is a an syntax brief:
[options="nowrap"]
----
$ oc login [--username=<username>]  [--password=<password>] [--server=<server>] [--certificate-authority=</path/to/file.crt>|--insecure-skip-tls-verify]
----


* The following table describes these common options for `oc login`:

.Common CLI Configuration Options
[cols="4,8",options="header"]
|===

|Option |Description
|`-s, --server`
|Specifies the host name of the OpenShift server. If a
server is provided through this flag, the command does not ask for it
interactively. This flag can also be used if you already have a CLI
configuration file and want to log in and switch to another server.

|`-u, --username` and `-p, --password`
|Allows you to specify the credentials to log in to the OpenShift
server. If user name or password are provided through these flags, the command
does not ask for it interactively. These flags can also be used if you already
have a configuration file with a session token established and want to log in and
switch to another user name.

|`--certificate-authority`
|Correctly and securely authenticates with an OpenShift
server that uses HTTPS. The path to a certificate authority file must be
provided.

|`--insecure-skip-tls-verify`
|Allows interaction with an HTTPS server bypassing the server
certificate checks; however, note that it is not secure. If you try to `oc
login` to a HTTPS server that does not provide a valid certificate, and this or
the `--certificate-authority` flags were not provided, `oc login` will prompt
for user input to confirm (`y/N` kind of input) about connecting insecurely.
|===



ifdef::showscript[]

=== Transcript


endif::showscript[]


== Resource Quota
:noaudio:

.What is ResourceQuota
* OpenShift can limit both the number of objects created in a Project , and the
total amount of resources requested across objects in a namespace/Project.
* This facilitates sharing of a single OpenShift cluster by several teams, each
in a Project of their own, as a mechanism of preventing one team from starving
another team of cluster resources.
* A ResourceQuota object enumerates hard resource usage limits per project.
** It can limit the total number of a particular type of object that may be
created in a project, and the total amount of compute resources that may be
consumed by resources in that project.


.Usage limits
|===
|Resource Name |Description
|cpu |Total cpu usage across all containers
|memory |Total memory usage across all containers
|pods |Total number of pods
|replicationcontrollers | Total number of replication controllers
|resourcequotas | Total number of resource quotas
| services | Total number of services
| secrets | Total number of secrets
| persistentvolumeclaims |Total number of persistent volume claims
|===


ifdef::showscript[]

=== Transcript


endif::showscript[]



== Resource Quota
:noaudio:

.Quota enforcement
* After a quota is first created in a project, the project restricts the ability
to create any new resources that may violate a quota constraint until it has
calculated updated usage statistics.

* Once a quota is created and usage statistics are up-to-date, the project
accepts the creation of new content. When you create resources, your quota
usage is incremented immediately upon the request to create or modify the
resource. When you delete a resource, your quota use is decremented during the
next full recalculation of quota statistics for the project. As a result, it
 may take a moment for your quota usage statistics to be reduced to their
 current observed system value when you delete resources.

* If your modification to a project would exceed a quota usage limit, the action is denied by the server, and an appropriate error message is returned to the end-user. The error explains what quota constraint was violated, and what their currently observed usage stats are in the system.

ifdef::showscript[]

=== Transcript


endif::showscript[]



== Resource Quota
:noaudio:

.Creating and applying a quota to a project

* Sample quota definition file

+
----
{
  "apiVersion": "v1",
  "kind": "ResourceQuota",
  "metadata": {
    "name": "quota" <1>
  },
  "spec": {
    "hard": {
      "memory": "1Gi", <2>
      "cpu": "20", <3>
      "pods": "10", <4>
      "services": "5", <5>
      "replicationcontrollers":"5", <6>
      "resourcequotas":"1" <7>
    }
  }
}
----
<1>  The name of this quota document
<2>  The total amount of memory consumed across all containers may not exceed 1Gi.
<3>  The total number of cpu usage consumed across all containers may not exceed 20 Kubernetes compute units.
<4>  The total number of pods in the project
<5>  The total number of services in the project
<6>  The total number of replication controllers in the project
<7>  The total number of resource quota documents in the project

ifdef::showscript[]

=== Transcript


endif::showscript[]


== Resource Quota
:noaudio:

.Creating and applying a quota to a project

* Apply a quota to a Project
+
----

$ oc create -f create_quota_def_file.json --namespace=your_project_name

----

ifdef::showscript[]

=== Transcript


endif::showscript[]


== Service Accounts
:noaudio:

.Overview

* When a person uses the command line or web console, their API token
authenticates them to the OpenShift API.
* However, when a regular user's
credentials are not available, it is common for components to make API calls
independently. For example:

** Replication controllers make API calls to create or delete pods
** Applications inside containers can make API calls for discovery purposes
** External applications can make API calls for monitoring or integration purposes

* Service accounts provide a flexible way to control API access without sharing a regular user's credentials.


ifdef::showscript[]

=== Transcript


endif::showscript[]

== Service Accounts
:noaudio:

.Usernames and groups

* Every service account has an associated username that can be granted roles,
just like a regular user.
* The username is derived from its project and name:
*system:serviceaccount:<project>:<name>*

* For example, to add the *view* role to the *monitor-agent* service account in the *monitored-project* project:
+
----
$ oc policy add-role-to-user view system:serviceaccount:monitored-project:monitor-agent
----

ifdef::showscript[]

=== Transcript


endif::showscript[]

== Service Accounts
:noaudio:

.Usernames and groups - Continued

* Every service account is also a member of two groups:

** *system:serviceaccounts*, which includes all service accounts in the system
** *system:serviceaccounts:<project>*, which includes all service accounts in
the specified project

* For example, to allow all service accounts in all projects to view resources
in the *top-secret* project:
+
----
$ oc policy add-role-to-group view system:serviceaccounts -n top-secret
----

* To allow all service accounts in the "monitor project" to edit resources in
the *top-secret* project:
+
----
$ oc policy add-role-to-group edit system:serviceaccounts:monitor -n top-secret
----

ifdef::showscript[]

=== Transcript


endif::showscript[]

== Service Accounts
:noaudio:

.Enable service account authentication

* Service accounts authenticate to the API using tokens signed by a private RSA key.
* The authentication layer verifies the signature using a matching public RSA key.

* To enable service account token generation, update the
master configuration file `serviceAccountConfig` stanza to specify a
`privateKeyFile` (for signing), and a matching public key file in the
`publicKeyFiles` list:
+
----
serviceAccountConfig:
  ...
  masterCA: ca.crt <1>
  privateKeyFile: serviceaccounts.private.key <2>
  publicKeyFiles:
  - serviceaccounts.public.key <3>
  - ...
----

<1> CA file used to validate the API server's serving certificate
<2> Private RSA key file (for token signing)
<3> Public RSA key files (for token verification). If private key files are
provided, then the public key component is used. Multiple public key files can
be specified, and a token will be accepted if it can be validated by one of
the public keys. This allows rotation of the signing key, while still
accepting tokens generated by the previous signer.


ifdef::showscript[]

=== Transcript


endif::showscript[]

== Service Accounts
:noaudio:

.Managed service accounts

* Service accounts are required in each project to run builds, deployments, and
other pods.
* The `managedNames` setting in the master configuration file controls which
service accounts are automatically created in every project:
+
----
serviceAccountConfig:
  ...
  managedNames: <1>
  - builder <2>
  - deployer <3>
  - default <4>
  - ...
----
<1> List of service accounts to automatically create in every project
<2> A *builder* service account in each project is required by build pods, and is given the *system:image-builder* role, which allows pushing images to any image stream in the project using the internal docker registry.
<3> A *deployer* service account in each project is required by deployment pods, and is given the *system:deployer* role, which allows viewing and modifying replication controllers and pods in the project.
<4> A *default* service account is used by all other pods unless they specify a different service account.


* All service accounts in a project are given the *system:image-puller* role,
which allows pulling images from any image stream in the project using the internal docker registry.

ifdef::showscript[]

=== Transcript


endif::showscript[]

== Service Accounts
:noaudio:

.Infrastructure service accounts

* Several infrastructure controllers run using service account credentials.
* The following service accounts are created in the OpenShift infrastructure
namespace at server start, and given the following roles cluster-wide:

** The *replication-controller* service account is assigned the
*system:replication-controller* role
** The *deployment-controller* service account is assigned the
*system:deployment-controller* role
** The *build-controller* service account is assigned the
*system:build-controller* role.

NOTE: Additionally, the *build-controller* service account is included in the
privileged security context constraint in order to create privileged build pods.
 More on that later


ifdef::showscript[]

=== Transcript


endif::showscript[]


== Using Persistent Volumes
:noaudio:

.Overview
* A `PersistentVolume` object is a storage resource in an OpenShift cluster.
* Storage is provisioned by an administrator by creating `PersistentVolume`
objects from sources such as:
** NFS mounts - Supported method
** GCE Persistent Disks (Google Compute)
** EBS Volumes (Amazon Elastic Block Stores)

NOTE: Persistent volume plug-ins other than the supported NFS plug-in, such as
AWS Elastic Block Stores (EBS), GCE Persistent Disks, GlusterFS, iSCSI, and
RADOS (Ceph), are currently in Technology Preview.


ifdef::showscript[]

=== Transcript


endif::showscript[]


== Using Persistent Volumes
:noaudio:

.Requesting Storage

* Storage can be made available to you by laying claims to the resource.
* You can make a request for storage resources using a `PersistentVolumeClaim`
object;
**  the claim is paired with a volume that generally matches your request.

ifdef::showscript[]

=== Transcript


endif::showscript[]

== Using Persistent Volumes
:noaudio:

.Requesting Storage - prerequisite
* For a user to be able to *claim* a volume (`PersistentVolumeClaim`), a
Persistent Volume (`PersistentVolume`) needs to be created.
** A *cluster admin* needs to define and "created" the *pv* in the project it
belongs to.

[source,yaml]
----
{
  "apiVersion": "v1",
  "kind": "PersistentVolume",
  "metadata": {
    "name": "pv0001"
  },
  "spec": {
    "capacity": {
        "storage": "5Gi"
    },
    "accessModes": [ "ReadWriteOnce" ],
    "nfs": {
        "path": "/exports/ose_shares/share154",
        "server": "172.17.0.2"
    },
    "persistentVolumeReclaimPolicy": "Recycle"
  }
}
----

ifdef::showscript[]

=== Transcript

endif::showscript[]



== Using Persistent Volumes
:noaudio:

.Requesting Storage
* After a *PersistentVolume* has been defined in your project:
** You can request storage by creating `PersistentVolumeClaim` objects in your
*projects*:

.Persistent Volume Claim Object Definition

[source,json]
----
{
    "apiVersion": "v1",
    "kind": "PersistentVolumeClaim",
    "metadata": {
        "name": "claim1"
    },
    "spec": {
        "accessModes": [ "ReadWriteOnce" ],
        "resources": {
            "requests": {
                "storage": "5Gi"
            }
        }
    }
}
----


ifdef::showscript[]

=== Transcript


endif::showscript[]


== Using Persistent Volumes
:noaudio:

.Volume and Claim Binding
* A `PersistentVolume` is a specific resource.
* A `PersistentVolumeClaim` is a request for a resource with specific
attributes, such as storage size.
* In between the two is a process that matches a claim to an available volume
and binds them together.
** This allows the claim to be used as a volume in a pod.
** OpenShift finds the volume backing the claim and mounts it into the pod.


ifdef::showscript[]

=== Transcript


endif::showscript[]


== Using Persistent Volumes
:noaudio:

.Volume and Claim Binding

* You can tell whether a claim or volume is bound by querying using the CLI:

----
$ oc get pvc
NAME        LABELS    STATUS    VOLUME
claim1      map[]     Bound     pv0001

$ oc get pv
NAME                LABELS              CAPACITY            ACCESSMODES         STATUS    CLAIM
pv0001              map[]               5368709120          RWO                 Bound     yournamespace / claim1
----

ifdef::showscript[]

=== Transcript


endif::showscript[]


== Using Persistent Volumes
:noaudio:

.Claims as Volumes in Pods

* A `PersistentVolumeClaim` is used by a pod as a volume.
* OpenShift finds the claim with the given name in the same namespace as the
pod, then uses the claim to find the corresponding volume to mount.

* Review the example Pod Definition with a Claim:
[source,json]
----
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "name": "mypod",
        "labels": {
            "name": "frontendhttp"
        }
    },
    "spec": {
        "containers": [{
            "name": "myfrontend",
            "image": "nginx",
            "ports": [{
                "containerPort": 80,
                "name": "http-server"
            }],
            "volumeMounts": [{
                "mountPath": "/var/www/html",
                "name": "pvol"
            }]
        }],
        "volumes": [{
            "name": "pvol",
            "persistentVolumeClaim": {
                "claimName": "claim1"
            }
        }]
    }
}
----



== Summary
:noaudio:

In This chapter we covered:

* OpenShift Resources
* Projects and Users
* Client tool authentication
* Resource Quota
* Service Accounts
* Persistent Volumes

ifdef::showscript[]

=== Transcript


endif::showscript[]
