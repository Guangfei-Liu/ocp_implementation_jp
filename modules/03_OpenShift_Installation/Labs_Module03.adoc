:scrollbar:
:data-uri:
:icons: images/icons
:toc2:		

== Deploy OpenShift Enterprise
:numbered:	

In this lab we will Deploy OpenShift Enterprise on a single host (in later labs we will add more nodes).

* Configure a DNS on our *oselab* server to serve our OpenShift environment.

* Configure SSH Keys

* Configure Repositories

* Configure Network Settings

* Install Docker on our host 


* Install Ansible Installer 

* Configure and Install OpenShift Enterprise



* Test deployment.

== Lab Environment Architecture and Important Information

The lab environment consists of 4 VMs:

* `oselab-GUID.oslab.opentlc.com` (administration host)

* `master00-GUID.oslab.opentlc.com` (master host, contains Etcd and the management console)

* `infranode00-GUID.oslab.opentlc.com` (infranode host, Will run our infrastructure containers: Registry and Router)

* `node00-GUID.oslab.opentlc.com` (node host, Region: Primary, Zone: East. )

* `node01-GUID.oslab.opentlc.com` (node host, Region: Primary, Zone: West. )

[NOTE]
As a reminder you will only be allowed to SSH to the administration host from the outside of the lab environment, all other hosts have external SSH blocked.  Once on the administration host, you can SSH to the other hosts internally.  As described earlier, you will have to use your private SSH key and OPENTLC login to access the system (not root!).

Each student lab is assigned a global unique identifier (GUID) that consists of 4 characters.  This GUID is provided to you in the provisioning email that will be sent to you when you provision your lab environment.  *Anywhere you see GUID from this point on, you will replace it with your lab's GUID.*

*In each lab step take special care to make sure that you are running the step on the required host.  Each step should contain the name of the host to run the step on and the example code should contain the host name in the shell prompt.*

* Administration host example:
+
----

[root@oselab-GUID ~]# command

----

* Master host example:
+
----

[root@master00-GUID ~]# command

----


== Configure Wildcard DNS to Service the OpenShift Environment.

OpenShift requires a wildcard DNS A record.  The wildcard A record should point to the publicly available (external) IP address of the OpenShift router.  For this training, we will ensure that the router will end up on the OpenShift server that is running the master.  It is advisable to use a low TTL for this record in order for DNS client caches to expire quicker so that changes become available quicker.  The DNS server runs on the administration host.

. Connect to your administration host `oselab-GUID.oslab.opentlc.com` (your private key location may vary):
+
----

yourdesktop$ ssh -i ~/.ssh/id_rsa your-opentlc-login@oselab-GUID.oslab.opentlc.com

----

. Become the `root` user on the administration host:
+
----

-bash-4.2$ su - 

----

. Install the `bind` and `bind-utils` package on the administration host:
+
----

[root@oselab-GUID ~]# yum -y install bind bind-utils

----

. On the admistration host collect and define the environment's information:
+
----

[root@oselab-GUID ~]# masterIP=`host master00-$guid.oslab.opentlc.com ipa.opentlc.com | grep $guid | awk '{ print $4 }'`
[root@oselab-GUID ~]# domain="cloudapps-$guid.oslab.opentlc.com"

----

. On the administration host create the zone file with the wildcard DNS:
+
----

[root@oselab-GUID ~]# mkdir /var/named/zones
[root@oselab-GUID ~]# echo "\$ORIGIN  .
\$TTL 1  ;  1 seconds (for testing only)
${domain} IN SOA master.${domain}.  root.${domain}.  (
  2011112904  ;  serial
  60  ;  refresh (1 minute)
  15  ;  retry (15 seconds)
  1800  ;  expire (30 minutes)
  10  ; minimum (10 seconds)
)
  NS master.${domain}.
\$ORIGIN ${domain}.
test A ${masterIP}
* A ${masterIP}"  >  /var/named/zones/${domain}.db

----

. Configure `named.conf` on the administration host:
+
----

[root@oselab-GUID ~]# echo "// named.conf
options {
  listen-on port 53 { any; };
  directory \"/var/named\";
  dump-file \"/var/named/data/cache_dump.db\";
  statistics-file \"/var/named/data/named_stats.txt\";
  memstatistics-file \"/var/named/data/named_mem_stats.txt\";
  allow-query { any; };
  recursion yes;
  /* Path to ISC DLV key */
  bindkeys-file \"/etc/named.iscdlv.key\";
};
logging {
  channel default_debug {
    file \"data/named.run\";
    severity dynamic;
  }; 
};
zone \"${domain}\" IN {
  type master;
  file \"zones/${domain}.db\";
  allow-update { key ${domain} ; } ;
};" > /etc/named.conf

----

. On the administration host correct file permissions and start the DNS server:
+
----

[root@oselab-GUID ~]# chgrp named -R /var/named
[root@oselab-GUID ~]# chown named -R /var/named/zones
[root@oselab-GUID ~]# restorecon -R /var/named
[root@oselab-GUID ~]# chown root:named /etc/named.conf
[root@oselab-GUID ~]# restorecon /etc/named.conf

----

. Enable and start `named` on the administration host:
+
----

[root@oselab-GUID ~]# systemctl enable named
[root@oselab-GUID ~]# systemctl start named

----

. Configure FirewallD on the administation host to allow inbound DNS queries:
+
----

[root@oselab-GUID bin]# firewall-cmd --zone=public --add-service=dns --permanent
[root@oselab-GUID bin]# firewall-cmd --reload

----

=== Verify DNS configuration:

A test DNS entry was created called `test.cloud-appsGUID.oslab.opentlc.com`.  This lab will test internal and external resolution of that DNS entry.
host test.cloudapps-$guid.oslab.opentlc.com 127.0.0.1
. First try testing the DNS server running on the administration host:
+
----

[root@oselab-GUID ~]# host test.cloudapps-$guid.oslab.opentlc.com 127.0.0.1

----

. Second try testing with an external name server:
+
----

[root@oselab-GUID ~]# host test.cloudapps-$guid.oslab.opentlc.com 8.8.8.8

----
+
[NOTE]
The first time you query 8.8.8.8 you may notice lag and an error:
`;; connection timed out; trying next origin
Host test.cloudapps-GIOD.oslab.opentlc.com not found: 3(NXDOMAIN)`
This is normal.  if you do the test again, it will go faster and not error out.

. Lastly test DNS from your laptop/desktop, this might take a few minutes to be updated.  Be sure to replace GUID with the correct GUID.
+
----

yourhost$ nslookup test.cloudapps-$GUID.oslab.opentlc.com

----

== Configure SSH Keys:

The OpenShift installer uses SSH to configure hosts.  In this lab we create and install an SSH key pair on the master host and add the public key to the `authorized_hosts` file.

. On the master host, create an SSH key pair for the `root` user.
+
----

[root@master00-GUID ~]# ssh-keygen -f /root/.ssh/id_rsa -N '' 

----

. Add the public ssh key to `/root/.ssh/authorized_keys` locally to the master host:
+
----

[root@master00-GUID ~]# cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys 

----

. Configure `/etc/ssh/ssh_conf` to disable `StrictHostKeyChecking` on the master host:
+
----

[root@master00-GUID ~]# echo StrictHostKeyChecking no >> /etc/ssh/ssh_config

----
+
[NOTE]
Only do this for hosts that are used for development, testing, or demos!

. From the master host test the new SSH key by connecting to itself over the loopback interface without a keyboard prompt:
+
----

[root@master00-GUID ~]# ssh 127.0.0.1
...[output ommitted]...
[root@master00-GUID ~]# exit

----

. Copy the SSH key to the rest of the nodes in the environment
+
----

[root@master00-GUID ~]# for node in infranode00-$guid.oslab.opentlc.com node00-$guid.oslab.opentlc.com node01-$guid.oslab.opentlc.com; do ssh-copy-id root@$node ; done

----


== Configure the Repositories on the Master Host

OpenShift requires several software repositories:

* `rhel-7-server-rpms`

* `rhel-7-server-extras-rpms`

* `rhel-7-server-optional-rpms`

* `rhel-server-7-ose-beta-rpms`

Normally you would get these repositories via `subscription-manager` but we have provided a mirror that we will configure in the following lab steps.

. If not already connected, connect to your administration host `oselab-GUID.oslab.opentlc.com` using your OPENTLC login and private SSH key:
+
----

yourdesktop$ ssh -i ~/.ssh/mykey your-opentlc-login@oselab-GUID.oslab.opentlc.com

----

. From the administration host SSH to the master host as the root user:
+
----

[yourlogin@oselab-GUID ~]$ ssh root@master00-$GUID.oslab.opentlc.com

----
+
[NOTE]
When prompted for a password use *r3dh4t1!*
+
----

root@master00-GUID.oslab.opentlc.com's password: ******** (r3dh4t1!) 

----

. It is highly recommended that you use a terminal multiplexing tool such as `tmux` or `screen` in case you lose connectivity to your environment.  This will keep your session at the place it was at the time of disconnection.  You are allowed to install the `tmux` or `screen` package using `yum` on the master host.  It is not installed by default.
+
[NOTE]
For more information on using `tmux` use `man tmux` after installing the package.
For more information on using `screen` use `man screen` after installing the package.


. On the master host set up the yum repository configuration file `/etc/yum.repos.d/open.repo` with the following repositories:
+
----
[root@master00-GUID ~]# cat << EOF > /etc/yum.repos.d/open.repo
[rhel-x86_64-server-7]
name=Red Hat Enterprise Linux 7
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-7
enabled=1
gpgcheck=0

[rhel-x86_64-server-rh-common-7]
name=Red Hat Enterprise Linux 7 Common
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-rh-common-7
enabled=1
gpgcheck=0

[rhel-x86_64-server-extras-7]
name=Red Hat Enterprise Linux 7 Extras
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-extras-7
enabled=1
gpgcheck=0

[rhel-x86_64-server-optional-7]
name=Red Hat Enterprise Linux 7 Optional
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-optional-7
enabled=1
gpgcheck=0

EOF

----
+
[NOTE]
We are using a local mirror of the repositories in our lab environment, as stated earlier you would normally use `subscription-manager`.

. Add the OpenShift repository mirror to the master host:
+
----

[root@master00-GUID ~]# cat << EOF >> /etc/yum.repos.d/open.repo
[rhel-7-server-ose-3.0-rpms]
name=Red Hat Enterprise Linux 7 OSE 3
baseurl=http://www.opentlc.com/repos/rhel-7-server-ose-3.0-rpms
enabled=1
gpgcheck=0

EOF

----

. List the available repositories on the master host:
+
-----

[root@master00-GUID ~]# yum repolist 

-----
+
You should see the following:
+
----

repo id                           repo name                               status
rhel-server-7-ose-beta-rpms       Red Hat Enterprise Linux 7 OSE 3           16
rhel-x86_64-server-7              Red Hat Enterprise Linux 7              4,387
rhel-x86_64-server-extras-7       Red Hat Enterprise Linux 7 Extras          19
rhel-x86_64-server-optional-7     Red Hat Enterprise Linux 7 Optional     4,087
rhel-x86_64-server-rh-common-7    Red Hat Enterprise Linux 7 Common          19

----

. The Nodes require to be configures as well, for the sake of simplicity we will copy the repo file to all the nodes directly from the the master
+
-----
[root@master00-GUID ~]# for node in infranode00-$guid.oslab.opentlc.com node00-$guid.oslab.opentlc.com node01-$guid.oslab.opentlc.com; do scp /etc/yum.repos.d/open.repo ${node}:/etc/yum.repos.d/open.repo ; done
-----


== Verify Network Configuration:

In this lab we will verify that the master host is configured correctly for internal and external DNS name resolution.

. Verify the hostname for the master host:
+
----

[root@master00-GUID ~]# hostname -f 

----
+
.You should see the following:
----

master00-GUID.oslab.opentlc.com

----

. Take note of the master host's internal IP address:
+
----

[root@master00-GUID ~]# ip address show dev eth0|grep "inet "|awk '{print $2}'|cut -f1 -d/

----

. Make sure the master host's internal DNS entry matches the internal IP address:
+
----

[root@master00-GUID ~]# host `hostname -f` 

----

. Take note of the master host's external IP address:
+
----

[root@master00-GUID ~]# curl http://www.opentlc.com/getip

----

. Make sure the master host's external DNS entry matches the external IP address:
+
----

[root@master00-GUID ~]# host `hostname -f` 8.8.8.8

----

NOTE: It might take some time for the global DNS servers to be updated. Try again after a short while if this doesn't work on the first try.

== Install Docker

OpenShift uses Docker to store and manage container images.  In this lab we install Docker and provide it's required storage pool.

. Install the `docker` package on the master host
+ 
----

[root@master00-GUID ~]# yum -y install docker

----
+

. Install the `docker` package on *ALL* the node hosts
+ 
----

[root@node00-GUID ~]# yum -y install docker

----

[NOTE]
The default Docker storage configuration uses loopback devices and is not appropriate for production. Red Hat considers the dm.thinpooldev storage option to be the only appropriate configuration for production use.

. Remove the out of the box loopback docker storage from the master host:
+
----

[root@master00-GUID ~]# rm -rf /var/lib/docker/*

----

. Configure the *Docker* registry on the master host:
+
----

[root@master00-GUID ~]# sed -i "s/OPTIONS.*/OPTIONS='--selinux-enabled --insecure-registry 0.0.0.0\/0'/" /etc/sysconfig/docker

----

.. You can use the ssh command to do this from the master host quickly for all hosts
+
----
[root@master00-GUID ~]# for node in infranode00-$guid.oslab.opentlc.com node00-$guid.oslab.opentlc.com node01-$guid.oslab.opentlc.com; do ssh $node "yum -y install docker; rm -rf /var/lib/docker/* ; sed -i \"s/OPTIONS.*/OPTIONS='--selinux-enabled --insecure-registry 0.0.0.0\/0'/\" /etc/sysconfig/docker; reboot;" ; done
----




. In order to use `dm.thinpooldev` you must have space for an LVM thinpool available, the `docker-storage-setup` package will assist you in configuring LVM.  Run `docker-storage-setup` on the infranode host to create logical volumes for Docker:
+
----
[root@infranode-GUID ~]# pvcreate /dev/vdb
[root@infranode-GUID ~]# vgextend `vgs | grep rhel | awk '{print $1}'` /dev/vdb
[root@infranode-GUID ~]# docker-storage-setup

----
+
You should see the following:
+
----

  Rounding up size to full physical extent 32.00 MiB
  Logical volume "docker-poolmeta" created.
  Logical volume "docker-pool" created.
  WARNING: Converting logical volume rhel_host2cc260760b15/docker-pool and rhel_host2cc260760b15/docker-poolmeta to pool's data and metadata volumes.
  THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
  Converted rhel_host2cc260760b15/docker-pool to thin pool.
  Logical volume "docker-pool" changed.
  
----
+
[NOTE]
Be careful with `docker-storage-setup` as it will, by default, find any unused extents in the volume group that contains your root filesystem to create the pool.  You can also specify a specific volume group or block device.  This can be a destructive process to the specified VG or block device!  Consult the OpenShift documentation for more information.

. On the master host examine the newly created logical volume `docker-pool`:
+
----

[root@infranode-GUID ~]# lvs /dev/rhel_host2cc260760b15/docker-pool

----
+
You should see the following:
+
----

  LV          VG                    Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  docker-pool rhel_host2cc260760b15 twi-a-t--- 5.98g             0.00   0.11

----

. On the infranode host, examine the docker storage configuration:
+
----

[root@infranode-GUID ~]# cat /etc/sysconfig/docker-storage

----
+
You should see the following:
+
----

DOCKER_STORAGE_OPTIONS=-s devicemapper --storage-opt dm.fs=xfs --storage-opt dm.thinpooldev=/dev/mapper/rhel_host2cc260760b15-docker--pool

----

. Enable, start, and get status for the *Docker* service on the master host:
+
----

[root@infranode-GUID ~]# systemctl enable docker

----

. Reboot the system
+
-----

[root@infranode-GUID ~]# reboot 

-----

. Check that the *Docker* service is started
+
----
[root@infranode-GUID ~]# systemctl status docker
----
+
You should see the following:
+
----

docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled)
   Active: active (running) since Wed 2015-06-10 15:31:11 EDT; 1s ago
...OUTPUT OMMITTED...

----
+
[NOTE]
Make sure the status shows *enabled* and *active (running)*.

. In order to save time later, we will pre-fetch the docker images to the master host. This process will take about 10 minutes to complete:
+
----

[root@master00-0a0c ~]# RHN="registry.access.redhat.com";PTH="openshift3_beta"
[root@master00-0a0c ~]# \
docker pull $RHN/$PTH/ose-haproxy-router:v0.5.2.2 ; \
docker pull $RHN/$PTH/ose-deployer:v0.5.2.2 ; \
docker pull $RHN/$PTH/ose-sti-builder:v0.5.2.2 ; \
docker pull $RHN/$PTH/ose-sti-image-builder:v0.5.2.2 ; \
docker pull $RHN/$PTH/ose-docker-builder:v0.5.2.2 ; \
docker pull $RHN/$PTH/ose-pod:v0.5.2.2 ; \
docker pull $RHN/$PTH/ose-docker-registry:v0.5.2.2 ; \
docker pull $RHN/$PTH/sti-basicauthurl:latest ; \
docker pull $RHN/$PTH/ose-keepalived-ipfailover:v0.5.2.2 ; \
docker pull $RHN/$PTH/ruby-20-rhel7 ; \
docker pull $RHN/$PTH/mysql-55-rhel7 ; \
docker pull $RHN/jboss-eap-6/eap-openshift ; \
docker pull openshift/hello-openshift:v0.4.3

----

. Examine docker pool info on the master host:
+
----

[root@master00-0a0c ~]# docker info
----
+
You should something like this:
+
----

Containers: 0
Images: 63
Storage Driver: devicemapper
 Pool Name: rhel_host2cc260760b15-docker--pool
 Pool Blocksize: 524.3 kB
 Backing Filesystem: xfs
 Data file:
 Metadata file:
 Data Space Used: 2.308 GB
 Data Space Total: 6.417 GB
 Data Space Available: 4.109 GB
 Metadata Space Used: 778.2 kB
 Metadata Space Total: 33.55 MB
 Metadata Space Available: 32.78 MB
 Udev Sync Supported: true
 Library Version: 1.02.93-RHEL7 (2015-01-28)
Execution Driver: native-0.2
Kernel Version: 3.10.0-229.el7.x86_64
Operating System: Red Hat Enterprise Linux Server 7.1 (Maipo)
CPUs: 2
Total Memory: 1.797 GiB
Name: master00-GUID.oslab.opentlc.com
...

----

. On the master host examine the `docker-pool` logical volume again:
+
----

[root@master00-GUID ~]# lvs /dev/rhel_host2cc260760b15/docker-pool

----
+
You should see something similar to the following:
+
----

  LV          VG                    Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  docker-pool rhel_host2cc260760b15 twi-aot--- 5.98g             35.96  2.32
  
----

== Install OpenShift Enterprise


. Download and unpack the installation utility on a host that has SSH access to your intended master and node hosts
+
----

[root@master00-GUID ~]# curl -o oo-install-ose.tgz https://install.openshift.com/portable/oo-install-ose.tgz
[root@master00-GUID ~]# tar -zxf oo-install-ose.tgz

----
+

. Execute the installation utility to interactively configure one or more hosts
+
----

[root@master00-GUID ~]# ./oo-install-ose

----
+
[NOTE]
The steps in this section will be changing soon as there is a lot of work being done to add features to the text installer.

. Follow the instructions of the Installer
----



PLAY RECAP ******************************************************************** 
infranode00-d540.oslab.opentlc.com : ok=46   changed=20   unreachable=0    failed=0   
localhost                  : ok=5    changed=0    unreachable=0    failed=0   
master00-d540.oslab.opentlc.com : ok=102  changed=43   unreachable=0    failed=0   
node00-d540.oslab.opentlc.com : ok=46   changed=20   unreachable=0    failed=0   
node01-d540.oslab.opentlc.com : ok=46   changed=20   unreachable=0    failed=0   


The installation was successful!

----

 

. After the installer is complete, check the status of your host using the `osc get nodes` command on the master host:
+
----

root@master00-GUID ~]# oc get nodes
NAME                                 LABELS                                                                        STATUS
infranode00-d540.oslab.opentlc.com   kubernetes.io/hostname=infranode00-d540.oslab.opentlc.com                     Ready
master00-d540.oslab.opentlc.com      kubernetes.io/hostname=master00-d540.oslab.opentlc.com							    Ready,SchedulingDisabled
node00-d540.oslab.opentlc.com        kubernetes.io/hostname=node00-d540.oslab.opentlc.com                          Ready
node01-d540.oslab.opentlc.com        kubernetes.io/hostname=node01-d540.oslab.opentlc.com                          Ready



----

== Set Regions and Zones

In OpenShift 2, we introduced the specific concepts of "regions" and "zones" to enable organizations to provide some topologies for application resiliency. Apps would be spread throughout the zones in a region and, depending on the way you configured OpenShift, you could make different regions accessible to users.

For OpenShift 3, Kubernetes doesn't actually care about your topology. In other words, OpenShift is "topology agnostic". In fact, OpenShift 3 provides advanced controls for implementing whatever topologies you can dream up, leveraging filtering and affinity rules to ensure that parts of applications (pods) are either grouped together or spread apart.

For the purposes of a simple example, we'll be sticking with the "regions" and "zones" theme. But, as you go through these examples, think about what other complex topologies you could implement. Perhaps "secure" and "insecure" hosts, or other topologies.



The assignments of "regions" and "zones" at the node-level are handled by labels
on the nodes. 

. Label the Master and the nodes
+
----
oc label node master00-$guid.oslab.opentlc.com region="infra" zone="na" 
oc label node infranode00-$guid.oslab.opentlc.com region="infra" zone="infranodes" 
oc label node node00-$guid.oslab.opentlc.com region="primary" zone="east"
oc label node node01-$guid.oslab.opentlc.com region="primary" zone="west"
----


. On the master host look at how the labels were implemented with `osc get nodes`:
+
----

[root@master00-GUID ~]# oc get nodes

----
+
You should see:
+
----

NAME                                 LABELS                                                                                   STATUS
infranode00-d540.oslab.opentlc.com   kubernetes.io/hostname=infranode00-d540.oslab.opentlc.com,region=infra,zone=infranodes   Ready
master00-d540.oslab.opentlc.com      kubernetes.io/hostname=master00-d540.oslab.opentlc.com,region=infra,zone=na              Ready,SchedulingDisabled
node00-d540.oslab.opentlc.com        kubernetes.io/hostname=node00-d540.oslab.opentlc.com,region=primary,zone=east            Ready
node01-d540.oslab.opentlc.com        kubernetes.io/hostname=node01-d540.oslab.opentlc.com,region=primary,zone=west            Ready

----

At this point we have a running OpenShift environment across three hosts, with
one master and three nodes, divided up into two regions -- "infrastructure"
and "primary".

From here we will start to deploy "applications" and other resources into
OpenShift.

### Useful OpenShift Logs

RHEL 7 uses `systemd` and `journal`. As such, looking at logs is not a matter of
`/var/log/messages` any longer. You will need to use `journalctl`.

Since we are running all of the components in higher loglevels, it is suggested
that you use your terminal emulator to set up windows for each process.

On the master host you should run each of the following in its own
window:

----

[root@master00-GUID ~]# journalctl -f -u openshift-master
[root@master00-GUID ~]# journalctl -f -u openshift-node

----

[NOTE]
You will want to do this on the other nodes, but you won't need the
`openshift-master` service. You may also wish to watch the Docker logs, too.
