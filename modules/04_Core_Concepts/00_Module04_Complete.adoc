
:noaudio:

ifdef::revealjs_slideshow[]

[#cover,data-background-image="image/1156524-bg_redhat.png" data-background-color="#cc0000"]

== &nbsp;
:noaudio:

[#cover-h1]
Red Hat OpenShift Enterprise Implementation

[#cover-h2]
OpenShift 3 Core Concepts

[#cover-logo]
image::{revealjs_cover_image}[]

endif::[]
== Module Topics

* Overview
* Containers and Images
* Pods and Services
* Scheduler
* Builds and Image Streams
* Replication Controllers
* Routers

ifdef::showscript[]

=== Transcript

Welcome to module four of the OpenShift Enterprise Implementation course.

This module presents some of the core concepts in OpenShift Enterprise 3.
It discusses containers and images, pods and the services that represent them,
and projects and the users who use them.

It also reviews builds, image streams, deployments, routes, and templates.

endif::showscript[]
== Overview

[cols="2,3"]
|===================================================================
|Concept/Object |Description
|*Containers and images* |Building blocks for deploying applications
|*Pods and services* |Let containers communicate with each other and with proxy
 connections
|*Projects and users* |Provide means for communities to organize and manage
 content together
|*Builds and image streams* |Let you build working images and react to new images
|*Deployments* |Add support for software development and deployment lifecycle
|*Routes* |Announce service to the world
|*Templates* |Let you create many objects at once based on parameters
|===================================================================

ifdef::showscript[]

=== Transcript

This module provides high-level architectural information on core concepts and
 objects you will encounter when using OpenShift Enterprise.

Many of these objects come from the Kubernetes Project. OpenShift Enterprise
 expands Kubernetes to provide a more feature-rich development lifecycle
  platform.

This module discusses the following:

* Containers and images, which are the building blocks for deploying your
 applications

* Pods and services, which let containers communicate with each other and
 provide a permanent IP to represent the Pods

* Projects and users, which provide the space and means for communities to
 organize and manage their content together

* Builds and image streams, which let you build working images and react to
 new images

* Deployments, which add expanded support for the software development and
 deployment lifecycle

* Routes, which announce your service to the world

* Templates, which let you create many objects at once based on customized
 parameters

endif::showscript[]
== Containers and Images

.Containers

* Basic units of OpenShift Enterprise applications
* Lightweight mechanisms for isolating running processes
** Limits processes to interacting with designated resources only
* Can run many application instances in containers on single host
** No visibility into other processes, files, network, etc.
* Container typically provides single service (often called a _microservice_)
** Web server or database
* Can use containers for arbitrary workloads
* More info on Linux container technologies:
 https://access.redhat.com/articles/1353593

ifdef::showscript[]

=== Transcript

Containers are the basic units of OpenShift Enterprise applications. Containers
 are lightweight mechanisms for isolating running processes so that these
  processes interact only with their designated resources.

You can run many application instances in containers on a single host without
 visibility into each others' processes, files, network, and so on.

Typically, each container provides a single service (often called a
   _microservice_), such as a web server or a database. However, you can also
    use containers for arbitrary workloads.

endif::showscript[]
== Containers and Images

.Containers: Docker

* In the past few years, Linux kernel advanced capabilities for container
 technologies
* More recently, Docker project developed management interface for Linux
containers
** Learn more: https://www.docker.com/whatisdocker/
* OpenShift Enterprise and Kubernetes let you orchestrate Docker containers
 across multi-host installations


ifdef::showscript[]

=== Transcript

In the past few years, Linux kernel advanced capabilities for container
 technologies

Recently, the Docker project has developed a convenient management interface for
 Linux containers on a host.

OpenShift Enterprise and Kubernetes add the ability to orchestrate Docker
 containers across multi-host installations.

Note that although you do not directly interact with Docker tools when using
 OpenShift Enterprise, you should know about Docker's capabilities and
  terminology to understand its role in OpenShift Enterprise and how your
   applications function inside containers.

endif::showscript[]
== Containers and Images

.Docker Images

* Docker containers based on Docker images
* *Docker image*: Binary that includes:
** All requirements for running single Docker container
** Metadata describing needs and capabilities
* Docker can be considered a packaging technology for containers.
* Docker containers can access only resources defined in it's image
** Unless you give container additional access when creating it
* Can deploy image in multiple containers across multiple hosts and
 load-balance among them
** Lets OpenShift Enterprise provide redundancy and horizontal scaling for
 service packaged into image

ifdef::showscript[]

=== Transcript

A running instance of a Docker image is referred to as a container.

A Docker image is a binary that includes all of the requirements for running a
 single Docker container, as well as metadata describing its needs and
  capabilities. You can think of it as a packaging technology.

Docker containers have access only to resources defined in the image, unless
 you give the container additional access when you create it.

By deploying the same image in multiple containers across multiple hosts and
 load-balancing among them, OpenShift Enterprise can provide redundancy and
  horizontal scaling for a service packaged into an image.

endif::showscript[]
== Containers and Images

.Building and Naming Images

* You can use Docker directly to build images, but OpenShift also supplies
builders that assist with creating an image by adding your code or
configuration to existing images.

* Single image name can refer to different versions of same image
* Every image version identified by unique long hexadecimal number, or _hash_
** Example: `fd44297e2ddb050ec4f...`
** Usually shortened to 12 digits
*** Example: `fd44297e2ddb`
* Can apply tags instead of version numbers
** Examples: `v1`, `v2.1`, `GA`, `latest` (default)
** May see same image referred to as `rhel` (implies `latest` tag), `rhel:rhel7`, `fd44297e2ddb`

ifdef::showscript[]

=== Transcript

You can use Docker to build images directly, but OpenShift Enterprise also
 supplies builders that assist with creating an image by adding your code or
  configuration to existing images.

Because applications develop over time, a single image name can actually refer
 to many different versions of the same image. Every version of every image is
  identified by a unique hash, a long hexademical number that is usually
   shortened to 12 digits.

Rather than version numbers, Docker lets you apply tags to the image name to
 more precisely specify the image desired. So, for example, you might see the
  same image identified by the `rhel` tag (which implies the `latest` tag), the
   `rhel:rhel7` tag, or the specific hash in 12 digit shortened format.


endif::showscript[]
== Containers and Images

.Docker Registry

* Service for storing and retrieving Docker images
* Contains one or more Docker image repositories
** Repository contains one or more tagged images
* Docker provides own registry, Docker Hub
** Learn more:
https://registry.hub.docker.com/
** Can also use private or third-party registries
* Red Hat provides Docker registry at `registry.access.redhat.com` for subscribers
* OpenShift Enterprise can supply internal registry for managing Docker images

ifdef::showscript[]

=== Transcript

A Docker registry is a service for storing and retrieving Docker images. A
 registry contains a collection of one or more Docker image repositories. Each
  image repository in turn contains one or more tagged images. A user can
   _pull_ and _push_ images from and to the registry.

Docker provides its own registry, the Docker hub, but you can also use private
 or third-party registries.

Red Hat provides a Docker registry with certified images at
 `registry.access.redhat.com` for subscribers.

In OpenShift Enterprise 3, you usually create your own registry, referred to
 as the `Integrated Registry`, and use it to push your S2I-created images.


endif::showscript[]
== Pods and Services

.Pods Overview

* OpenShift Enterprise leverages Kubernetes concept of _pod_
* *Pod*: One or more containers deployed together on host
** Smallest compute unit you can define, deploy, manage

* Pods are the rough equivalent of OpenShift Enterprise 2 _gears_

* Each pod allocated own internal IP address, owns entire port range
* Containers within pods can share local storage and networking

ifdef::showscript[]

=== Transcript

OpenShift Enterprise leverages the Kubernetes concept of a _pod_, which is one
 or more containers deployed together on one host. A pod is the smallest compute
  unit that you can define, deploy, and manage.

Pods are the rough equivalent of  _gears_ in OpenShift Enterprise 2

Each pod is allocated its own internal IP address, thus owning its entire port
 range. Containers within pods can share their local storage and networking.


endif::showscript[]
== Pods and Services

.Pod Changes and Management

* OpenShift Enterprise treats pods as _static_ objects
** Cannot change pod definition while running

* To implement changes, OpenShift Enterprise:
** Terminates existing pod
** Recreates it with modified configuration, base image(s), or both

* Pods are expendable, do not maintain state when recreated
* Should use higher-level _controllers_ to manage pods
** Pods should usually be managed by higher-level controllers rather than
 directly by users.

ifdef::showscript[]

=== Transcript

OpenShift Enterprise treats pods as _static_ objects. You cannot change a pod
 definition while it is running. When you want to change a pod, you _recreate_
  rather than _modify_ it.

OpenShift Enterprise implements changes by terminating an existing pod and
 recreating it with a modified configuration, base image(s), or both.

OpenShift also treats pods as expendable. Pods do not maintain state when
 recreated.

Because of this, you use higher-level _controllers_, such as
 _deployment configurations_ and _replication controllers_, to manage pods,
  rather than allow users to manage pods directly.


endif::showscript[]
== Pods and Services

.Pods Lifecycle

* Lifecycle:
** Pod is _defined_
** _Assigned_ to run on node
** _Runs_ until containers exit or pods are removed

ifdef::showscript[]

=== Transcript

Pods have the following lifecycle: They are _defined_, then they are _assigned_
 by the scheduler to run on a specific node. They then _run_ until their
  container(s) exit or they are removed for some other reason.

endif::showscript[]
== Pods and Services

.Pods Definition File/Manifest

* Example definition of pod:
** Part of OpenShift Enterprise infrastructure: _Private Docker integrated registry_
** Demonstrates many pod features
+
[source,yaml]
----
 apiVersion: v1
 kind: Pod
 metadata:
   annotations: { ... }
   labels:                               <1>
     deployment: example-name-1
     deploymentconfig: example-name
     example-name: default
   generateName: example-name-1-       <2>

----
<1> Labels used to select and manage groups of pods in single operation
<2> Base name, `generateName`, used to generate unique pod name



ifdef::showscript[]

=== Transcript

Here is an example definition of a pod that provides a long-running service.
 This is actually a part of the OpenShift Enterprise infrastructure, the
  _private Docker integrated registry_.

This example demonstrates many features of pods. The next few slides examine the
 file in smaller chunks to make it easier to follow.

Note the following:

. You can _tag_ pods with one or more _labels_. You can then use the labels to
 select and manage groups of pods in a single operation.

. Pods must have a unique name within their _namespace_. In the pod definition,
 you can specify a base name and use the `generateName` attribute to
  automatically add random characters at the end of the base name,
   thus generating a unique name.


endif::showscript[]
== Pods and Services

.Pods Definition File/Manifest: `containers` and `env`

[source,yaml]
----
 spec:
   containers:                            <1>
   - env:                                 <2>
     - name: OPENSHIFT_CA_DATA
       value: ...
     - name: OPENSHIFT_CERT_DATA
       value: ...
     - name: OPENSHIFT_INSECURE
       value: "false"
     - name: OPENSHIFT_KEY_DATA
       value: ...
     - name: OPENSHIFT_MASTER
       value: https://master.example.com:8443
----

<1> `containers` specifies an array of container definitions--one in this case
<2> `env` uses variables to pass necessary values to each container, such as credentials and database connection details




ifdef::showscript[]

=== Transcript

This example illustrates the `containers` and `env` components:

. `containers` specifies an array of container definitions--in this case
 (as with most), just one.
. You can specify variables (`env`) to pass necessary values to each container.
 For example, these can be credentials and database connection details.

endif::showscript[]
== Pods and Services

.Pods Definition File/Manifest: Container
[source,yaml]
----
     image: openshift3/example-image:v1.1.0.6 <1>
     imagePullPolicy: IfNotPresent
     name: registry
     ports:                              <2>
     - containerPort: 5000
       protocol: TCP
     resources: {}
     securityContext: { ... }            <3>
     volumeMounts:                       <4>
     - mountPath: /registry
       name: registry-storage
     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
       name: default-token-br6yz
       readOnly: true
----
<1> Each container in pod is instantiated from its own Docker image
<2> Container can bind to ports, which are available on pod's IP
<3> Container's _security context_ specifies whether it can run as privileged container
<4> External storage volumes should be mounted within container


ifdef::showscript[]

=== Transcript

Here you see a container definition:

. Each container in the pod is instantiated from its own Docker image.
. The container can bind to ports, which are available on the pod's IP.
. OpenShift Enterprise defines a _security context_ for containers that
 specifies whether they can run as privileged containers, run as a user of their
  choice, and more. The default context is highly restrictive, but
   administrators can modify this as needed.
. The container specifies where external storage volumes should be mounted
 within the container. In this case, one volume stores the registry's data,
  another provides access to credentials that the registry needs to make requests
   against the OpenShift Enterprise API.

endif::showscript[]
== Pods and Services

.Pods Definition File/Manifest: Requests and Storage Volumes

[source,yaml]
----
   dnsPolicy: ClusterFirst
   imagePullSecrets:
   - name: default-dockercfg-at06w
   restartPolicy: Always
   serviceAccount: default               <1>
   volumes:                              <2>
   - emptyDir: {}
     name: registry-storage
   - name: default-token-br6yz
     secret:
       secretName: default-token-br6yz
----
<1> Service account name
<2> Volumes that pod can use


ifdef::showscript[]

=== Transcript

In this example, you see how pods make requests and define storage volumes:

. Pods commonly make requests against the OpenShift Enterprise API and need
 authentication to do so. The `serviceAccount` field specifies the
  _service account_ user that the pod should use to authenticate when making
   requests. This enables fine-grained access control for custom infrastructure
    components.
. The pod defines storage volumes that its container(s) can use. In this case,
 it provides an ephemeral volume for the registry storage and a `secret` volume
  containing the service account credentials.

You can learn more about using _secrets_ in the developer guide documentation.

endif::showscript[]
== Pods and Services

.Services

* Kubernetes service serves as internal load balancer
** Identifies set of replicated pods
** Proxies connections it receives to identified pods
* Can add or remove backing pods to or from service while service remains
 consistently available
** Lets anything depending on service refer to it at consistent internal address

* Assign services IP address and port pair
** Proxy to appropriate backing pod when accessed
* Service uses label selector to find running containers that provide certain
 network service on certain port
* Can access server by IP address and DNS name
** Name created and resolved by local DNS server on master
+
[NOTE]
When you use the `Default Router` (HAProxy), you bypass service load balancing. You use the service only to find out which pods the service represents. `Default Router` does the load balancing.

ifdef::showscript[]

=== Transcript

A Kubernetes _service_ serves as an internal load balancer. It identifies a set
 of replicated _pods_ and then proxies the connections it receives to those pods.

You can add or remove backing pods to or from a service arbitrarily while the
 service remains consistently available. This lets anything that depends on the
  service refer to it at a consistent interal IP address.

You assign services an IP address and port pair that, when accessed, proxy to
 an appropriate backing pod.

A service uses a label selector to find all the running containers that provide
 a certain network service on a certain port.

You can access the server by IP address and DNS name. The name is created and resolved by the local DNS server on the master.

Note that when you use the `Default Router` (HAProxy), you bypass the service
 load balancing. You use the service only to find out which pods the service
  represents. The `Default Router` does the load balancing.

endif::showscript[]
== Pods and Services

.Service Definition File/Manifest

* Like pods, services are REST objects
* Example: Definition of service for pod defined above:
+
[source,yaml]
----
 apiVersion: v1
 kind: Service
 metadata:
   name: example-name
 spec:
   selector:                  <1>
     example-label: example-value
   portalIP: 172.30.136.123   <2>
   ports:
   - nodePort: 0
     port: 5000               <3>
     protocol: TCP
     targetPort: 5000         <4>
----
<1> `selector` identifies all pods with specified label: `example-label: example-value`
<2> `portalIP` specifies assigned service IP
<3> `Port` specifies port on which service listens
<4> `targetPort` specifies port used to forward connections to backing pods



ifdef::showscript[]

=== Transcript

Like pods, services are REST objects. The example shown here provides the
 definition of a service for the pod defined above.

Note the following:

. The label selector identifies all pods with the `example-label: example-value`
 label as the `Pods` represented by the `service`
. When the service is created, it automatically receives a virtual IP,
 `PortalIP`, chosen from a pool of internal IPs. When you define a new service,
  you leave this blank to be assigned a random IP.
. The `Port` line specifies the port on which the service listens.
. The service uses the `targetPort` to forward connections to the backing pods.
 Those pods should listen on that port.

endif::showscript[]
== Pods and Services

.Labels

* Use labels to organize, group, choose API objects
**  Example: Tag _pods_ with labels so services can use label selectors to
 identify pods to which they proxy
** Lets services reference groups of pods
*** Can treat pods with different Docker containers as related entities

* Most objects can include labels in metadata
* Can use labels to group arbitrarily related objects
** Example: Can group application's _pods_, _services_,
 _replication controllers_, and _deployment configurations_


ifdef::showscript[]

=== Transcript

You use labels to organize, group, or choose API objects.

For example, if you tag _pods_ with labels, _services_ can use label selectors
 to identify the pods that they represent.

This lets services reference groups of pods and lets you treat pods with different Docker containers as related entities.


Most objects can include labels in their metadata, so you can use labels to
 group arbitrarily related objects. For example, you can group all of a
  particular application's _pods_, _services_, _replication controllers_, and
   _deployment configurations_.

endif::showscript[]
== Pods and Services

.Labels: Examples

* Labels = Simple key/value pairs:
+
[source,yaml]
----
 labels:
   key1: value1
   key2: value2
----

* Scenario:
** Pod consisting of `nginx` Docker container, with `role=webserver` label
** Pod consisting of `Apache httpd` Docker container, also with `role=webserver`
 label
** Service or replication controller defined to use pods with `role=webserver`
 label treats both pods as part of same group

* Example: To remove all components with the label `app=mytest`:
+
----
# oc delete all -l app=mytest
----

ifdef::showscript[]

=== Transcript

Labels are simple key/value pairs that you can use in almost every type of
 object in OpenShift Enterprise.

Labels provide an easy way to manage resources as groups, rather than
 individually.

For example, say you have two separate pods. One is a `nginx` Docker container
 and the other is an `Apache httpd` Docker container. If both are tagged with a
  `role=webserver` label, the service or replication controller defined to use
   pods with the `role=webserver` label treats both pods as part of the same
    group.

The second example here shows how labeling a group of components with the
 `app=mytest` label lets you delete all of them in one command rather than
  having to locate and delete each component manually.

endif::showscript[]
== Scheduler

.Overview

* The scheduler:
** Determines placement of new pods onto nodes within OpenShift Enterprise
 cluster
** Reads pod data and tries to find node that is good fit
** Is independent, standalone, pluggable solution
** Does not modify pod, merely creates binding that ties pod to node

ifdef::showscript[]

=== Transcript

The scheduler is responsible for determining placement of new
 pods onto nodes within the OpenShift Enterprise cluster. It reads data from
  the pod definition and tries to find a node that is a good fit based on
   configured policies.

The scheduler is completely independent and exists as a standalone, pluggable
 solution.

It does not modify the pod. It simply creates a binding that ties the pod to
 the selected node.

endif::showscript[]
== Scheduler

.Scheduler Extensibility

* Scheduler built using plug-in model
* Current implementation is plug-in
* Two ways to extend scheduler functionality:

** Enhancements
*** Add predicates and priority functions
*** Can be contributed upstream or maintained separately
*** Need to be registered with scheduler factory and specified in scheduler
 policy file

** Replacement
*** Replace scheduler with alternate implementation
** Scheduler code watches pods as they are created and identifies most suitable
 node to host them
*** Uses master API to create pod-to-node bindings for pods



ifdef::showscript[]

=== Transcript

As is the case with almost everything in OpenShift Enterprise, the
 scheduler is built using a plug-in model, and the current implementation itself
  is a plug-in.

You can extend the scheduler functionality in two ways: enhancements and
 replacement. You can enhance the scheduler functionality by adding new
  predicates and priority functions. You can either contribute these upstream
   or maintain them separately. You need to register new predicates and priority
    functions with the scheduler factory and then specify them in the scheduler
     policy file.


Alternatively, because the scheduler is a plug-in, you can replace it with a
 different implementation.

The scheduler code has a clean separation that watches new pods as they get
 created and identifies the most suitable node to host them. It then creates
  pod-to-node bindings for the pods using the master API.

endif::showscript[]
== Scheduler

.Generic Scheduler

* OpenShift Enterprise provides generic scheduler
** Default scheduling _engine_
** Selects node to host pod in three-step operation:

. Filter nodes based on specified constraints/requirements

- Runs nodes through list of filter functions called _predicates_

. Prioritize qualifying nodes

- Pass each node through series of _priority_ functions
- Assign node score between 0 - 10
- 0 indicates bad fit, 10 indicates good fit

. Select the best fit node

- Sort nodes based on scores
- Select node with highest score to host pod
- If multiple nodes have same high score, select one at random
- Priority functions equally weighted by default; more important priorities can receive higher weight

ifdef::showscript[]

=== Transcript

OpenShift Enterprise provides a default generic scheduler. It is a scheduling
 _engine_ that selects a node to host the pod in a three-step operation:

. The scheduler filters all of the available nodes based on specified
 constraints and requirements by running them through a list of filter functions
  called _predicates_, and disqualifies the nodes that do not meet the criteria.

. It then prioritizes the qualifying nodes that remain by passing them through
 a series of _priority_ functions that assign each node a score between 0 and
  10, where 0 indicates the worst possible fit and 10 the best possible fit to
   host the pod.

. The scheduler sorts the nodes by scores and selects the node with the highest
 score to host the pod. If multiple nodes have the same high score, the
  scheduler selects one of them at random.

By default, the scheduler considers every priority function to be equally
 important and gives each one a weight, or positive numeric value, of 1.
  Administrators can reconfigure the scheduler to give some priority functions
   more importance by increasing their weight.

endif::showscript[]
== Scheduler

.Scheduler Policy
* Selection of predicates and priority functions defines scheduler _policy_
* Administrators can provide JSON file that specifies predicates and priority
 functions to configure scheduler
** Overrides default scheduler policy
** If default predicates or priority functions required, must specify them in file
** Can specify path to scheduler policy file in master configuration file
* Default configuration applied if no scheduler policy file exists

ifdef::showscript[]

=== Transcript

The selection of the predicate and priority functions defines the policy for
 the scheduler. Administrators can provide a JSON file that specifies the
  predicates and priority functions to configure the scheduler.

The predicates and priority functions defined in the scheduler configuration
 file completely override the default scheduler policy. If you need any of the
  default predicates and priority functions, you must explicitly specify them

Administrators can specify the path to the scheduler policy file in the master
 configuration file.

In the absence of the scheduler policy file, the default configuration is
 applied.

endif::showscript[]
== Scheduler

.Default Scheduler Policy

* Includes following predicates:

** `PodFitsPorts`
** `PodFitsResources`
** `NoDiskConflict`
** `MatchNodeSelector`
** `HostName`

* Includes following priority functions:

** `LeastRequestedPriority`
** `BalancedResourceAllocation`
** `ServiceSpreadingPriority`
*** Each has weight of *1* applied


ifdef::showscript[]

=== Transcript

The default scheduler policy includes the _predicates_ and _priority functions_
 shown here. After the predicates disqualify (or _opt-out_) nodes, the priority
  functions and their _weight_ define the best fit for the new pod.

endif::showscript[]
== Scheduler

.Available Predicates

* OpenShift Enterprise 3 provides predicates out of the box
* Can customize by by providing parameters
* Can combine to provide additional node filtering

* Two kinds of predicates: _static_  and _configurable_

ifdef::showscript[]

=== Transcript

OpenShift Enterprise 3 provides several predicates out of the box.

You can customize some of these predicates by providing certain parameters.
 You also can combine multiple predicates to provide additional filtering of
  nodes.

There are two kinds of predicates, _static_ and _configurable_. The next few
 slides discuss them.

endif::showscript[]
== Scheduler

.Static Predicates

* Fixed names and configuration parameters that users cannot change
* Kubernetes provides following out of box:

[cols="1,4"]
|===================================================================
|Static Predicate |Description
|`PodFitsPorts` |Deems node fit for hosting pod based on absence of port conflicts
|`PodFitsResources` a|* Determines fit based on resource availability
* Nodes declare resource capacities, pods specify what resources they require
* Fit based on requested, rather than used, resources
|`NoDiskConflict` a|* Determines fit based on nonconflicting disk volumes
* Evaluates if pod can fit based on volumes requested and those already mounted
|`MatchNodeSelector` |Determines fit based on node selector query defined in pod
|`HostName` |Determines fit based on presence of host parameter and string match with host name
|===================================================================

ifdef::showscript[]

=== Transcript

Static predicates have fixed names and configuration parameters that users
 cannot change.

Kubernetes provides the static predicates shown here out of the box.

endif::showscript[]
== Scheduler

.Configurable Predicates

* User can configure to tweak function
** Can give them user-defined names
** Identified by arguments they take
* Can:
** Configure predicates of same type with different parameters
** Combine them by applying different user-defined names


ifdef::showscript[]

=== Transcript

A user can configure configurable predicates to tweak their function.

A user can give a configurable predicate any user-defined name. The predicate
 type is identified by the argument that it takes.

A user working with configurable predicates can configure multiple predicates
 of the same type but with different parameters as long as the predicates have
  different user-defined names.

endif::showscript[]
== Scheduler

.Configurable Predicates: `ServiceAffinity` and `LabelsPresence`

* `ServiceAffinity`: Filters out nodes that do not belong to topological level
 defined by provided labels
** Takes in list of labels
** Ensures affinity within nodes with same label values for pods belonging to
 same service
*** If pod specifies label value in `NodeSelector`:
*** Pod scheduled on nodes matching labels only
+
----
{"name" : "Zone", "argument" : {"serviceAffinity" : {"labels" : ["zone"]}}}
----
* `LabelsPresence`: Checks whether node has certain label defined, regardless of
 value
+
----
{"name" : "ZoneRequired", "argument" : {"labels" : ["retiring"], "presence" : false}}
----

ifdef::showscript[]

=== Transcript

`ServiceAffinity` filters out nodes that do not belong to the specified
 topological level defined by the provided labels. This predicate takes in a
  list of labels and ensures affinity within the nodes that have the same label
   values for pods belonging to the same service.

If the pod specifies a value for the labels in its `NodeSelector`, then the
 scheduler can schedule pods on only the nodes matching those labels.

`LabelsPresence` checks whether a particular node has a certain label defined,
 regardless of its value.

endif::showscript[]
== Scheduler

.Available Priority Functions

* Can specify custom set of priority functions to configure scheduler
** OpenShift Enterprise provides several priority functions out of the box
* Can customize some priority functions by providing parameters
* Can combine priority functions and give different weights to influence
 prioritization results
** Weight required, must be greater than 0

ifdef::showscript[]

=== Transcript

You can configure the scheduler by specifying a custom set of priority functions.

OpenShift Enterprise provides several priority functions out of the box. You can
 customize some of them by providing certain parameters.

You can also combine multiple priority functions and give them different weights
 to influence the results of the prioritization process. You must specify a
  weight, which must be a number greater than 0.

endif::showscript[]
== Scheduler

.Static Priority Functions

* Do not take configuration parameters or inputs from user
* Specified in scheduler configuration using predefined names and weight
 calculations

[cols="1,4"]
|===================================================================
|Static Predicate |Description
|`LeastRequestedPriority` a|* Favors nodes with fewer requested resources
* Calculates percentage of memory and CPU requested by pods scheduled on node
* Prioritizes nodes with highest available or remaining capacity
|`BalancedResourceAllocation` a|* Favors nodes with balanced resource usage rate
* Calculates difference between consumed CPU and memory as fraction of capacity
* Prioritizes nodes with smallest difference
* Should always use with `LeastRequestedPriority`
|`ServiceSpreadingPriority` |Spreads pods by minimizing number of pods belonging
 to same service onto same machine
|`EqualPriority` a|* Gives equal weight of *1* to all nodes
* Not required/recommended outside of testing.
|===================================================================


ifdef::showscript[]

=== Transcript

Static priority functions do not take any configuration parameters or inputs
 from the user. The scheduler configuration file specifies these priority
  functions using their predefined names and weight calculations.

The available static priority functions are shown here.

endif::showscript[]
== Scheduler

.Configurable Priority Functions

* User can configure by providing certain parameters.
** Can give them user-defined name
** Identified by the argument they take

* `ServiceAntiAffinity`: Takes label
** Ensures spread of pods belonging to same service across group of nodes based
 on label values
** Gives same score to all nodes with same value for specified label
** Gives higher score to nodes within group with least concentration of pods

* `LabelsPreference`: Prefers either nodes that have particular label defined
 or those that do not, regardless of value



ifdef::showscript[]

=== Transcript

A user can configure configurable priority functions by providing certain
 parameters.

A user can give a configurable priority function any user-defined name. The
 function type is identified by the argument that it takes.

`ServiceAntiAffinity` takes a label and ensures a spread of pods belonging to
 the same service across a group of nodes based on label values. It gives the
  same score to all nodes with the same value for a specified label, and gives
   a higher score to nodes within the group with the least concentration of
    pods.

`LabelsPreference` prefers either nodes that have a particular label defined or
 those that do not have a particular label defined, regardless of value.


endif::showscript[]
== Scheduler

.Use Cases

* Important use case for scheduling within OpenShift Enterprise: Support
 affinity and anti-affinity policies

* OpenShift Enterprise can implement multiple infrastructure topological levels
* Administrators can define multiple topological levels for infrastructure
 (nodes)
** To do this, specify labels on nodes
*** Example: `region` = `r1`, `zone` = `z1`, `rack` = `s1`
** Label names have no particular meaning
** Administrators can name infrastructure levels anything
*** Examples: City, building, room
** Administrators can define any number of levels for infrastructure topology
*** Three levels usually adequate
*** Example: `regions` -> `zones` -> `racks`
** Administrators can specify combination of affinity/anti-affinity rules at
 each level

ifdef::showscript[]

=== Transcript

One important use case for scheduling within OpenShift Enterprise is to support
 flexible affinity and anti-affinity policies. OpenShift Enterprise can
  implement multiple infrastructure topological levels. By specifying labels on
   the nodes, administrators can define multiple topological levels for their
    infrastructure.

Label names have no particular meaning, and administrators can name
 infrastructure levels anything. Administrators can also define any number of
  levels for their infrastructure topology, although three levels are usually
   adequate. Finally, administrators can specify any combination of affinity
    and anti-affinity rules at each level.

The examples shown here use `region`, `zone`, and `rack`. However, you can use
 any kind of topology that makes sense in your environment.


endif::showscript[]
== Scheduler

.Affinity

* Administrators can configure scheduler to specify affinity at any topological
 level or multiple levels
* Affinity indicates all pods belonging to same service are scheduled onto nodes
 belonging to same level
* Handles application latency requirements by letting administrators ensure peer
 pods do not end up being too geographically separated
* If no node available within same affinity group to host pod, pod not scheduled

ifdef::showscript[]

=== Transcript

Use the affinity function when you want all components of a service--that is,
 all the pods--to be located in the same _zone_, _region_, or _node_.

Administrators can configure the scheduler to specify affinity at any
 topological level, or even at multiple levels.

Affinity at a particular level indicates that all pods that belong to the same
 service are scheduled onto nodes that belong to the same level.

This handles any latency requirements of applications by letting administrators
 ensure that peer pods do not end up being too geographically separated. If no
  node is available within the same affinity group to host the pod, the pod is
   not scheduled.

endif::showscript[]
== Scheduler

.Anti-Affinity

* Administrators can configure scheduler to specify anti-affinity at any
 topological level or multiple levels
* Anti-affinity (or _spread_) indicates that all pods belonging to same service
 are spread across nodes belonging to that level
* Ensures that application is well spread for high availability
* Scheduler tries to balance service pods evenly across applicable nodes

ifdef::showscript[]

=== Transcript

Administrators can configure the scheduler to specify anti-affinity at any
 topological level, or even at multiple levels.

Anti-affinity, or _spread_, at a particular level indicates that all pods that
 belong to the same service are spread across nodes that belong to that level.

This is useful when trying to create a highly available service that is spread
 between availability zones or racks. It ensures that the application is well
  spread for high availability purposes.

The scheduler tries to balance the service pods across all applicable nodes as
 evenly as possible.

endif::showscript[]
== Scheduler

.Sample Policy Configuration

----
{
	"kind" : "Policy",
	"version" : "v1",
	"predicates" : [
		{"name" : "PodFitsPorts"},
		{"name" : "PodFitsResources"},
		{"name" : "NoDiskConflict"},
		{"name" : "MatchNodeSelector"},
		{"name" : "HostName"}
	],
	"priorities" : [
		{"name" : "LeastRequestedPriority", "weight" : 1},
		{"name" : "BalancedResourceAllocation", "weight" : 1},
		{"name" : "ServiceSpreadingPriority", "weight" : 1}
	]
}
----

ifdef::showscript[]

=== Transcript

The configuration shown here specifies the default scheduler configuration, as
 it might be specified via the scheduler policy file.

endif::showscript[]
== Scheduler

.Topology Example 1


* Example: Three topological levels
** Levels: `region` (affinity) -> `zone` (affinity) -> `rack` (anti-affinity)
+
[source,json]
----
{
	"kind" : "Policy",
	"version" : "v1",
	"predicates" : [
		...
		{"name" : "RegionZoneAffinity", "argument" : {"serviceAffinity" : {"labels" : ["region", "zone"]}}}
	],
	"priorities" : [
		...
    {"name" : "RackSpread", "weight" : 1, "argument" : {"serviceAntiAffinity" : {"label" : "rack"}}}
	]
}
----


ifdef::showscript[]

=== Transcript

You can use as many or as few topological levels as you like in the scheduler.

The example shown here defines three topological levels: `region`, `zone`, and
 `rack`.

This policy creates a scheduling process that puts pods in the same `region` and
 `zone` but spreads the pods among the `racks` within each zone.

Note that in all of the sample configurations provided here, the list of
 predicates and priority functions are truncated to include only the ones that
  pertain to the specified use case. In practice, a complete and meaningful
	 scheduler policy should include most, if not all, of the default predicates
	  and priority functions described earlier in this module.

endif::showscript[]
== Scheduler

.Topology Example 2
* Example: Three topological levels
** Levels: `city` (affinity) -> `building` (anti-affinity) -> `room` (anti-affinity)
+
[source,json]
----
{
	"kind" : "Policy",
	"version" : "v1",
	"predicates" : [
		...
		{"name" : "CityAffinity", "argument" : {"serviceAffinity" : {"labels" : ["city"]}}}
	],
	"priorities" : [
		...
		{"name" : "BuildingSpread", "weight" : 1, "argument" : {"serviceAntiAffinity" : {"label" : "building"}}},
		{"name" : "RoomSpread", "weight" : 1, "argument" : {"serviceAntiAffinity" : {"label" : "room"}}}
	]
}
----


ifdef::showscript[]

=== Transcript

This three-level topology example keeps the pods in the same `city` and spreads
 them between the `buildings` and the `rooms` in each building.

endif::showscript[]
== Scheduler

.Topology Example 3

* Only use nodes with `region` label defined
* Prefer nodes with `zone` label defined
+
[source,json]
----
{
	"kind" : "Policy",
	"version" : "v1",
	"predicates" : [
		...
		{"name" : "RequireRegion", "argument" : {"labelsPresence" : {"labels" : ["region"], "presence" : true}}}

	],
	"priorities" : [
		...
		{"name" : "ZonePreferred", "weight" : 1, "argument" : {"labelPreference" : {"label" : "zone", "presence" : true}}}
	]
}
----

ifdef::showscript[]

=== Transcript

In this example, the policy means that a _node_ must have a _region_ label, and
 that you prefer to use a _node_ that also has a _zone_ label defined.

endif::showscript[]
== Builds and Image Streams

.Builds Overview

* *Build*: Process of transforming input parameters into resulting object
** Most often used to transform source code into runnable image
* `BuildConfig` object: Definition of entire build process

* OpenShift Enterprise build system provides extensible support for
 _build strategies_
** Based on selectable types specified in build API
* Three build strategies available:
** Docker build
** S2I build
** Custom build

* Docker and S2I builds supported by default

ifdef::showscript[]

=== Transcript

A _build_ is the process of transforming input parameters into a resulting
 object. Most often, you use the process to transform source code into a
  runnable image. A _BuildConfig_ object is the definition of the entire build
   process.

The OpenShift Enterprise build system provides extensible support for
 _build strategies_ that are based on selectable types specified in the build
  API. Three build strategies are available:

* *Docker build*: A build based on a _Dockerfile_.
*  *S2I build*: OpenShift Enterprise's built-in builder. It builds an image
 from a base image and source code provided as a Git repository.
*  *Custom build*: Can be any process a user can define. One example is a
 Jenkins server that builds a Docker image outside of the OpenShift Enterprise
  environment.

OpenShift Enterprise supports Docker and S2I builds by default.

endif::showscript[]
== Builds and Image Streams

.Builds Overview: Resulting Objects

* Resulting object of build depends on type of builder used
** *Docker and S2I builds*: Resulting objects are runnable images
** *Custom builds*: Resulting objects are whatever author of builder image
 specifies

* For list of build commands, see Developer's Guide: https://docs.openshift.com/enterprise/latest/architecture/core_concepts/builds_and_image_streams.html

ifdef::showscript[]

=== Transcript

The resulting object of a build depends on the type of builder used to create
 it.

For Docker and S2I builds, the resulting objects are runnable images.

For custom builds, the resulting objects are whatever the author of the builder
 image specifies.

For a list of build commands, see the Developer's Guide at the web address shown
 here.


endif::showscript[]
== Builds and Image Streams

.Docker Build

* Docker build strategy invokes plain `docker build` command
* Expects repository with `Dockerfile` and required artifacts to produce
 runnable image


.S2I Build
* *S2I*: Tool for building reproducible Docker images
* Produces ready-to-run images by injecting user source into base Docker image
 (_source_) and assembling new Docker image
** Ready to use with `docker run`
* S2I supports incremental builds
** Reuse previously downloaded dependencies, previously built artifacts, etc.

ifdef::showscript[]

=== Transcript

The Docker build strategy invokes the plain _docker build_ command. It expects
 a repository with a `Dockerfile` and all required artifacts in it to produce a
  runnable image.

S2I is a tool for building reproducible Docker images. It produces ready-to-run
 images by injecting a user source into a base Docker image (the _builder_) and
  assembling a new Docker image that is ready to use within the OpenShift
   Enterprise environment or with the `docker run` command.

S2I supports incremental builds, which reuse previously downloaded dependencies,
 previously built artifacts, and so on.

endif::showscript[]
== Builds and Image Streams

.S2I Advantages

[horizontal]
Image flexibility::

* Can write S2I scripts to layer application code onto almost any existing
 Docker image
* Takes advantage of existing ecosystem
* Currently, S2I relies on `tar` to inject application source, so image must be
 able to process tarred content

Speed::

* S2I assembly process can perform large number of complex operations without
 creating new layer at each step
* Results in faster process
* Can write S2I scripts to reuse artifacts stored in previous version of
 application image
* Eliminates need to download or build image each time build is run

Patchability::

* S2I lets you rebuild application consistently if underlying image needs patch
 because of a security issue

ifdef::showscript[]

=== Transcript

Among the advantages S2I provides are image flexibility, speed, and
 patchability.

With regard to image flexibility, you can write S2I scripts to layer application
 code onto almost any existing Docker image, taking advantage of the existing
  ecosystem. This means that you can, for example, switch your builder image
   from Centos to Red Hat Enterprise Linux or from  Red Hat Enterprise Linux 7.1
    to 7.2 without any issues. You simply rebuild the image and start using it.

With regard to speed, S2I's assembly process can perform a large number of
 complex operations without creating a new layer at each step, resulting in a
  faster process. You can write S2I scripts to reuse artifacts stored in a
   previous version of the application image, which eliminates the need to
    download or build the image each time a build is run.

With regard to patchability, S2I lets you rebuild an image quickly if the base
 image requires a patch, for example, if there is a new security patch.

endif::showscript[]
== Builds and Image Streams

[horizontal]
Operational efficiency::

* PaaS operator restricts build operations instead of allowing arbitrary
 actions, such as in `Dockerfile`
* Can avoid accidental or intentional abuses of build system

Operational security::

* Building arbitrary `Dockerfile` exposes host system to root privilege
 escalation
* Malicious user can exploit this because Docker build process is run as user
 with Docker privileges
* S2I restricts operations performed as root user and can run scripts as
 non-root user

User efficiency::

* S2I prevents developers from performing arbitrary `yum install` type
 operations during application build
* Results in slow development iteration

Ecosystem efficiency::

* S2I encourages shared ecosystem of images
* Can leverage best practices for applications

ifdef::showscript[]

=== Transcript

A few more advantages of the S2I process are the operational efficiencies it
 provides and its operational security advantages.

With regard to operational efficiencies, the PaaS operator can use S2I to
 restrict build operations and not allow arbitrary actions, such as in a
  `Dockerfile`, and thus avoid accidental or intentional abuses of the build
   system.

With regard to operational security, building an arbitrary `Dockerfile` exposes
 the host system to root privilege escalation. A malicious user can exploit
  this because the entire Docker build process is run as a user with Docker
   privileges. S2I automatically restricts the operations performed as a root
    user, and can run the scripts as a non-root user.

With regard to user efficiency, S2I prevents developers from performing
 arbitrary `yum install`-type operations during their application build.
  Performing these types of operations results in slow development iteration.

With regard to ecosystem efficiency, S2I encourages a shared ecosystem of images
 where you can leverage best practices for your applications.


endif::showscript[]
== Builds and Image Streams

.Custom Build

* Custom build strategy lets you define builder image
** Responsible for entire build process
* Using own builder image lets you customize build process

* Builder can call out to external system
** Example: Jenkins or other automation agent
** Creates image and pushes it into registry

ifdef::showscript[]

=== Transcript

The custom build strategy lets you define a specific builder image that is
 responsible for the entire build process. Using your own builder image lets
  you customize your build process.

The builder can call out to an external system, such as Jenkins or any other
 automation agent, to create the image and push it into the registry.

endif::showscript[]
== Builds and Image Streams

.Image Streams

* _Image stream_ similar to Docker image repository
** Contains _Docker images_ identified by tags
** Presents single virtual view of related images
** May contain images from:
*** Own image in OpenShift's integrated Docker Registry
*** Other image streams
*** Docker image repositories from external registries

* OpenShift Enterprise stores complete metadata about each image
** Examples: command, entrypoint, environment variables, etc.

* OpenShift Enterprise images immutable

* OpenShift Enterprise components can:
** Watch for updates in an image stream
** Receive notifications when new images added
** React by performing build or deployment

ifdef::showscript[]

=== Transcript

An _image stream_ is similar to a Docker image repository in that it contains
 one or more _Docker images_ identified by tags. An image stream presents a
  single virtual view of related images. The stream may contain images from any
   of the following:

* Its own image repository in OpenShift Enterprise's integrated Docker Registry
* Other image streams
* Docker image repositories from external registries

OpenShift Enterprise stores complete metadata about each image--for example,
 command, entrypoint, environment variables, and so on. Images in OpenShift
  Enterprise are immutable.

OpenShift Enterprise components such as builds and deployments can watch an
 image stream, receive notifications when new images are added, and react by
  performing a build or a deployment, among other functions.

endif::showscript[]
== Builds and Image Streams

.Image Pull Policy

* Each container in pod has Docker image
** Can refer to image in pod after creating it and pushing it to registry

* When OpenShift Enterprise creates containers, uses `imagePullPolicy` to
 determine whether to pull image prior to starting container

* Three values for `imagePullPolicy`:

** `Always`: Always pull image
** `IfNotPresent`: Pull image only if it does not already exist on node
** `Never`: Never pull image

* If not specified, OpenShift Enterprise sets container's `imagePullPolicy`
 parameter based on image's tag
** If tag is `latest`, OpenShift Enterprise defaults `imagePullPolicy` to
 `Always`

ifdef::showscript[]

=== Transcript

Each container in a pod has a Docker image. After you create an image and push
 it to a registry, you can then refer to it in the pod.

When OpenShift Enterprise creates containers, it uses the container's
 `imagePullPolicy` to determine whether to pull the image prior to starting the
  container.

There are three possible values for `imagePullPolicy`:

* `Always`: Always pull the image
* `IfNotPresent`: Pull the image only if it does not already exist on the node
* `Never`: Never pull the image

If a container's `imagePullPolicy` parameter is not specified, OpenShift
 Enterprise sets it based on the image's tag. If the tag is `latest`, OpenShift
  Enterprise defaults `imagePullPolicy` to `Always`.


endif::showscript[]
== Replication Controllers

.Replication Controllers Overview

* Replication controller ensures specified number of pod replicas running at all
 times
* If pods exit or are deleted, replication controller instantiates more
* If more pods running than desired, replication controller deletes as many as
 necessary


ifdef::showscript[]

=== Transcript

The job of a replication controller is to ensure that a specified number of
 replicas of a pod are running at all times.

If pods exit or are deleted, the replication controller acts to instantiate more
 pods up to the desired number.

If there are more pods running than desired, the replication
 controller deletes as many pods as necessary to match the specified number.


endif::showscript[]
== Replication Controllers

.Replication Controllers Definition

* Replication controller definition includes:
** Number of replicas desired (can adjust at runtime)
** Pod definition for creating replicated pod
** Selector for identifying managed pods

* *Selector*: Set of labels all pods managed by replication controller should
 have
** Included in pod definition that replication controller instantiates
** Used by replication controller to determine how many pod instances are
 running, to adjust as needed

* Not replication controller's job to perform auto-scaling based on load or
 traffic
** Does not track either




ifdef::showscript[]

=== Transcript

The definition of a replication controller consists mainly of the following:

* The number of replicas desired, which you can adjust at runtime
* A pod definition for creating a replicated pod
* A selector for identifying managed pods

The selector is just a set of labels that all of the pods managed by the
 replication controller should have. The set of labels is included in the pod
  definition that the replication controller instantiates.

The replication controller uses this selector to determine how many instances
 of the pod are already running, to adjust as needed.

It is _not_ the replication controller's job to perform auto-scaling based on
 load or traffic, as it does not track either.

endif::showscript[]
== Replication Controllers

.Replication Controllers Definition File/Manifest

[source,yaml]
----
apiVersion: v1
kind: ReplicationController
metadata:
  name: frontend-1
spec:
  replicas: 1  <1>
  selector:    <2>
    name: frontend
  template:    <3>
    metadata:
      labels:  <4>
        name: frontend
    spec:
      containers:
      - image: openshift/hello-openshift
        name: helloworld
        ports:
        - containerPort: 8080
          protocol: TCP
      restartPolicy: Always
----
<1> Number of copies of pod to run
<2> Label selector of pod to run
<3> Template for pod that controller creates
<4> Labels on pod should include label from label selector


ifdef::showscript[]

=== Transcript

Replication controllers are a core Kubernetes object, `ReplicationController`.

Here is a sample `ReplicationController` definition with some omissions and
 callouts. Note the following:

. This is the number of copies of the pod to run.
. This is the label selector of the pod to run.
. This is a template for the pod that the controller creates.
. Labels on the pod should include those from the label selector.

endif::showscript[]
== Routers

.Routers: Overview

* Administrators can deploy _routers_ (like HAProxy `Default Router`) in
 OpenShift Enterprise cluster
* Let external clients use `route` resources created by developers

* Routers provide external hostname mapping and load balancing to applications
 over protocols that pass distinguishing information directly to router

* Currently, OpenShift Enterprise routers support these protocols:
** HTTP
** HTTPS (with SNI)
** WebSockets
** TLS with SNI

ifdef::showscript[]

=== Transcript

An OpenShift Enterprise administrator can deploy _routers_ (like the HAProxy
   `Default Router`) in an OpenShift Enterprise cluster. These enable external
    clients to use `route` resources created by developers.

OpenShift Enterprise routers provide external hostname mapping and
 load balancing to applications over protocols that pass distinguishing
  information directly to the router.

Currently, OpenShift Enterprise routers support the following protocols:

* HTTP
* HTTPS (with SNI)
* WebSockets
* TLS with SNI



endif::showscript[]
== Routers

.HAProxy Default Router

* HAProxy default router implementation: Reference implementation for template
 router plug-in
* Uses `openshift3/OpenShift Enterprise-haproxy-router` image to run HAProxy
 instance alongside template router plug-in
* Supports unsecured, edge terminated, re-encryption terminated,
  and passthrough terminated routes matching on HTTP vhost and request path

.F5 Router
* Integrates with existing F5 BIG-IP system in your
 environment
* Supports unsecured, edge terminated, re-encryption terminated,
  and passthrough terminated routes matching on HTTP vhost and request path
* Has feature parity with the HAProxy template route and some additional
 features



ifdef::showscript[]

=== Transcript

The HAProxy default router implementation is the reference implementation for a
 template router plug-in. It uses the
  `openshift3/OpenShift Enterprise-haproxy-router` image to run an HAProxy
   instance alongside the template router plug-in.

It supports unsecured, edge terminated, re-encryption terminated,
  and passthrough terminated routes matching on HTTP vhost and request path.


The F5 router plug-in integrates with an existing F5 BIG-IP system in your
 environment. F5 BIG-IP version 11.4 or newer is required to have the
  F5 iControl REST API. The F5 router supports unsecured, edge terminated,
   re-encryption terminated, and passthrough terminated routes matching on HTTP
    vhost and request path.

   The F5 router has feature parity with the HAProxy template router, along with some additional features.


endif::showscript[]
== Routers

.Routers and Routes

* `route` object describes `service` to expose and host FQDN
** Example: `route` could specify hostname `myapp.mysubdomain.company.com` and
 `service` `MyappFrontend`
** _Not_ `router`
* Creating `route` object for application lets external web client access
 application on OpenShift Enterprise using DNS name

* Router uses service selector to find service and endpoints backing service
** Bypasses service-provided load balancing and replaces with router's load
 balancing
* Routers watch cluster API and update own configuration based on changes in API
 objects
* Routers may be containerized or virtual
** Can deploy custom routers to communicate API object modifications to another
 system, such as `F5`

ifdef::showscript[]

=== Transcript

A `route` object is an object that describes a `service` to expose and a host
 FQDN. For example, a `route` could specify a hostname of
  `myapp.mysubdomain.company.com` and the `service` `MyappFrontend`.

To allow an external web client to access an application--the pod or pods--on
 OpenShift Enterprise using a DNS name, create a `route` object for your
  application.

A router uses the service selector to find the service and the endpoints backing
 the `service` defined in the `route`. This bypasses the service-provided load
  balancing and replaces it with the router's own load balancing.

Routers communicate with  OpenShift's API and automatically update their own
 configuration according to any relevant changes in the API objects. Routers may
  be containerized or virtual.

You can deploy custom routers to communicate modifications of API objects to
 another system, such as an `F5`.

endif::showscript[]
== Routers

.Routers and Routes: Requests

* To reach a router, requests for hostnames must resolve via DNS to a router or
 set of routers
* Recommended router setup:
** Define a sub-domain with a wildcard DNS entry pointing to a virtual IP
** Back virtual IP with multiple router instances on designated nodes
* Other approaches possible

ifdef::showscript[]

=== Transcript

To reach a router in the first place, requests for hostnames must resolve via
 DNS to a router or set of routers.

We recommend defining a cloud domain with a wildcard DNS entry pointing to a
 virtual IP backed by multiple router instances on designated nodes.

In this approach, you need to configure the DNS for each address outside the
 cloud domain individually.

endif::showscript[]
== Routers

.Sticky Sessions

* Underlying router configuration determines sticky session implementation
* Default HAProxy template implements sticky sessions using _balance source_
 directive
** Balances based on source IP
* Template router plug-in provides service name and namespace to underlying
 implementation
** Can use for advanced configuration such as implementing stick-tables that
 synchronize between set of peers

* Specific configuration for router implementation stored in
 `haproxy-config.template`, located in `/var/lib/haproxy/conf` directory of
  router container

ifdef::showscript[]

=== Transcript

The underlying router configuration determines implementation of sticky sessions.

The default HAProxy template implements sticky sessions using the
 _balance source_ directive, which balances based on the source IP.

In addition, the template router plug-in provides the service name and namespace
 to the underlying implementation.

You can use this for more advanced configuration such as implementing
 stick-tables that synchronize between a set of peers.

The specific configuration for this router implementation is stored in the
 `haproxy-config.template` file, located in the `/var/lib/haproxy/conf`
  directory of the router container.

endif::showscript[]
== Summary

* Overview
* Containers and Images
* Pods and Services
* Scheduler
* Builds and Image Streams
* Replication Controllers
* Routers


ifdef::showscript[]

=== Transcript

This module presented some of the core concepts in OpenShift Enterprise 3. It
 discussed containers and images, pods and the services that represent them.
It also reviewed builds, image streams, deployments, routes, and templates.

endif::showscript[]
