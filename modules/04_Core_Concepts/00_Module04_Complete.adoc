== &nbsp;
:noaudio:

ifdef::revealjs_slideshow[]

[#cover,data-background-image="image/1156524-bg_redhat.png" data-background-color="#cc0000"]


[#cover-h1]
Red Hat OpenShift Enterprise Implementation

[#cover-h2]
OpenShift 3.0 Core Concepts

[#cover-logo]
image::{revealjs_cover_image}[]

endif::[]

== Module Topics
:noaudio:

* Overview
* Containers and Images
* Pods and Services
* Projects and Users
* Builds and ImageStreams
* Replication Controllers
* Routers

ifdef::showscript[]

=== Transcript
Welcome to Module 04 of the OpenShift Enterprise Implementation course.
In this module we will learn about some of the core concepts in OSE3.0, We will
discuss Containers and Images, Pods and Services who represent them, Projects
and the users who use them and review Builds, ImageStreams, deployments, routes
and Templates.

We will dive deeper into most of these topics later in the training as well.

endif::showscript[]
== Overview
:noaudio:

The following module provide high-level, architectural information on core
concepts and objects you will encounter when using OpenShift.

Many of these objects come from Kubernetes, which is extended by OpenShift to provide
a more feature-rich development lifecycle platform.

We will learn about:

* *Containers and images* that are the building blocks
for deploying your applications.
* *Pods and services* that allow for containers to
communicate with each other and proxy connections.
* *Projects and users* that provide the space and means
for communities to organize and manage their content together.
* *Builds and image streams* that allow you to
build working images and react to new images.
* *Deployments* add expanded support for the software
development and deployment lifecycle.
* *Routes* announce your service to the world.
* *Templates* allow for many objects to be created at once
based on customized parameters.

ifdef::showscript[]

=== Transcript

The following module provide high-level, architectural information on core
concepts and objects you will encounter when using OpenShift.

Many of these objects come from Kubernetes, which is extended by OpenShift to provide
a more feature-rich development lifecycle platform.

We will learn about:
Containers and images, that are the building blocks
for deploying your applications. About, Pods and services that allow for
containers to communicate with each other and proxy connections.

We will also cover Projects and users that provide the space and means
for communities to organize and manage their content together.

Builds and image streams that allow you to build working images and react to new
 images. Deployments add expanded support for the software development and
 deployment lifecycle.

endif::showscript[]

== Containers and Images
:noaudio:

.Containers

* The basic units of OpenShift applications are called containers.
* link:https://access.redhat.com/articles/1353593[Linux container technologies]
are lightweight mechanisms for isolating running processes so that they are
limited to interacting with only their designated resources.
* Many application instances can be running in containers on a single host
without visibility into each others' processes, files, network, and so on.
* Typically, each container provides a single service (often called a
  "micro-service"), such as a web server or a database, though containers can
  be used for arbitrary workloads.

ifdef::showscript[]

=== Transcript
The basic units of OpenShift applications are called containers, Containers are
lightweight mechanisms for isolating running processes so that they are limited
to interacting with only their designated resources.

With the use of containers, many application instances can be running in
containers on a single host without visibility into each others' processes,
files, network, and so on.

Typically, each container provides a single service (often called a
  "micro-service"), such as a web server or a database, though containers can
  be used for arbitrary workloads.

endif::showscript[]

== Containers and Images
:noaudio:

.Containers Continued

* The Linux kernel has been incorporating capabilities for container technologies
for years.
* More recently the link:https://www.docker.com/whatisdocker/[Docker project]
has developed a convenient management interface for Linux containers on a host.
* OpenShift and Kubernetes add the ability to orchestrate Docker containers across
multi-host installations.

NOTE: Though you do not directly interact with Docker tools when using
OpenShift, understanding Docker's capabilities and terminology is
important for understanding its role in OpenShift and how your
applications function inside of containers.

ifdef::showscript[]

=== Transcript
 he Linux kernel has been incorporating capabilities for container technologies
 for years. Recently the Docker project has developed a convenient management
 interface for Linux containers on a host.

 OpenShift and Kubernetes add the ability to orchestrate Docker containers
 across multi-host installations.

endif::showscript[]

== Containers and Images
:noaudio:

.Docker Images

* Docker containers are based on Docker images.
** A Docker image is a binary that includes all of the requirements for running a single Docker
container, as well as metadata describing its needs and capabilities.
* You can think of it as a packaging technology.
* Docker containers only have access to resources defined in the image, unless
you give the container additional access when creating it.
* By deploying the same image in multiple containers across multiple hosts and
load balancing between them, OpenShift can provide redundancy and horizontal scaling
for a service packaged into an image.

ifdef::showscript[]

=== Transcript
A "running" instance of a Docker Image is referred to as a "Container", A Docker
 image is a binary that includes all of the requirements for running a single
 Docker container, as well as metadata describing its needs and capabilities.

By deploying the same image in multiple containers across multiple hosts and
load balancing between them, OpenShift can provide redundancy and horizontal
scaling for a service packaged into an image.

endif::showscript[]

== Containers and Images
:noaudio:

.Docker Images

* You can use Docker directly to build images, but OpenShift also supplies
builders that assist with creating an image by adding your code or
configuration to existing images.

* Since applications develop over time, a single image name can actually
refer to many different versions of the "same" image.
** Each different image is referred to uniquely by its hash (a long hexadecimal
  number e.g. `fd44297e2ddb050ec4f...`) which is usually shortened to 12
characters (e.g. `fd44297e2ddb`).
** Rather than version numbers, Docker allows applying tags (such as `v1`, `v2.1`
  , `GA`, or the default `latest`)
** in addition to the image name to further specify the image desired, so
you may see the same image referred to as `rhel` (implying the `latest`
tag), `rhel:rhel7`, or `fd44297e2ddb`.


ifdef::showscript[]

=== Transcript

 You can use Docker directly to build images, but OpenShift also supplies
 builders that assist with creating an image by adding your code or
 configuration to existing images.

 Since applications develop over time, a single image name can actually
 refer to many different versions of the "same" image, Each different image is
 referred to uniquely by its hash which is usually shortened to 12
characters.

Rather than version numbers, Docker allows applying tags (such as `v1`, `v2.1`
  , `GA`, or the default `latest`)



endif::showscript[]

== Containers and Images
:noaudio:

.Docker Registries

* A Docker registry is a service for storing and retrieving Docker images.
* A registry contains a collection of one or more Docker image repositories.
* Each image repository contains one or more tagged images.
* Docker provides its own registry, the
link:https://registry.hub.docker.com/[Docker Hub], but you may
also use private or third-party registries.
* Red Hat provides a Docker registry at `registry.access.redhat.com` for
subscribers.
* OpenShift can also supply its own internal registry for managing custom Docker
images.

ifdef::showscript[]
=== Transcript
A Docker registry is a service for storing and retrieving Docker images, a user
can "pull" and "push" images from and to the registry.

Red Hat provides a Docker registry, with certified images  at
`registry.access.redhat.com` for subscribers.

In OSE3.0 we will usually create our own registry, referred to as the
"Integrated Registry" and use it to push our S2I created images.


endif::showscript[]


== Pods and Services
:noaudio:

.Pods Overview

* OpenShift leverages the Kubernetes concept of a `pod`, which is one or more
containers deployed together on one host, and the smallest compute unit that can
be defined, deployed, and managed.

* Pods are the rough equivalent of OpenShift v2 gears, with containers
the rough equivalent of v2 cartridge instances.
** Each pod is allocated its own internal IP address, therefore owning its
entire port space, and containers within pods can share their local storage and
networking.

ifdef::showscript[]
=== Transcript
OpenShift leverages the Kubernetes concept of a `pod`, which is one or more
containers deployed together on one host, and the smallest compute unit that can
be defined, deployed, and managed.

Each pod is allocated its own internal IP address, therefore owning its
entire port space, and containers within pods can share their local storage and
networking.


endif::showscript[]

== Pods and Services
:noaudio:

.Pods Overview - Continued

* OpenShift treats pods as largely "static"; changes cannot be made to
a pod definition while it is running.

* OpenShift implements changes by terminating an existing pod and recreating it
with modified configuration, base image(s), or both.
* Pods are also treated as expendable, and *do not maintain state* when recreated.
* Therefore pods should usually be managed by higher-level _controllers_ rather
than directly by users.

ifdef::showscript[]
=== Transcript
OpenShift treats pods as largely "static"; changes cannot be made to
a pod definition while it is running, when we want to change a pod we will
"Recreate" it rather than "modify" it.

Therefore pods should usually be managed by higher-level _controllers_, such as
"Deployment Configurations" and "Replication controllers" rather
than directly by users.

endif::showscript[]


== Pods and Services
:noaudio:

.Pods Lifecycle

* Pods have a lifecycle they are; *defined*, then they are *assigned* to run on
a node, then they *run* until their container(s) exit or they are removed
for some other reason.
* Pods, depending on policy and exit code, may be removed after exiting, or may
be retained in order to enable access to the logs of their containers.

ifdef::showscript[]
=== Transcript

Pods have a lifecycle they are; *defined*, then they are *assigned*, by the
"Scheduler" to run on a specific node, then they *run* until their container(s)
exit or they are removed for some other reason.

endif::showscript[]

== Pods and Services
:noaudio:

.Pods Definition file/Manifest

* Below is an example definition of a pod that provides a long-running
service, which is actually a part of the OpenShift infrastructure: the
*private Docker integrated registry*.
* It demonstrates many features of pods, most of which are discussed in other
topics and thus only briefly mentioned here.
* We'll break the file into a few slides to make it easier to follow:
+
[source,yaml]
----
 apiVersion: v1
 kind: Pod
 metadata:
   annotations: { ... }
   labels:                                <1>
     deployment: docker-registry-1
     deploymentconfig: docker-registry
     docker-registry: default
   generateName: docker-registry-1-       <2>

----
<1> Pods can be "tagged" with one or more *labels*, which can then
 be used to select and manage groups of pods in a single operation. The labels
 are stored in key/value format in the *metadata* hash. One label in this
 example is *docker-registry=default*.
<2> Pods must have a unique name within their _namespace_. A pod definition may
specify the basis of a name with the *generateName* attribute, and random
characters will be added automatically to generate a unique name.

ifdef::showscript[]
=== Transcript
Below is an example definition of a pod that provides a long-running
service, which is actually a part of the OpenShift infrastructure.

Pods can be "tagged" with one or more *labels*, which can then
 be used to select and manage groups of pods in a single operation.

 Pods must have a unique name within their _namespace_. A pod definition may
 specify the basis of a name with the *generateName* attribute, and random
 characters will be added automatically to generate a unique name.


endif::showscript[]

== Pods and Services
:noaudio:

.Pods Definition file/Manifest - Continued

[source,yaml]
----
 spec:
   containers:                            <1>
   - env:                                 <2>
     - name: OPENSHIFT_CA_DATA
       value: ...
     - name: OPENSHIFT_CERT_DATA
       value: ...
     - name: OPENSHIFT_INSECURE
       value: "false"
     - name: OPENSHIFT_KEY_DATA
       value: ...
     - name: OPENSHIFT_MASTER
       value: https://master.example.com:8443
----
<1> *containers* specifies an array of container definitions; in this case (as
 with most), just one.
<2> Environment variables can be specified to pass necessary values to each
 container. (For example, these can be credentials and database connection details)

ifdef::showscript[]
=== Transcript
*containers* specifies an array of container definitions; in this case (as
with most), just one.

Environment variables can be specified to pass necessary values to each
 container. (For example, these can be credentials and database connection details)


endif::showscript[]

== Pods and Services
:noaudio:

.Pods Definition file/Manifest - Continued
[source,yaml]
----
     image: openshift3/docker-registry:v0.6.2 <1>
     imagePullPolicy: IfNotPresent
     name: registry
     ports:                              <2>
     - containerPort: 5000
       protocol: TCP
     resources: {}
     securityContext: { ... }            <3>
     volumeMounts:                       <4>
     - mountPath: /registry
       name: registry-storage
     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
       name: default-token-br6yz
       readOnly: true
----

<1> Each container in the pod is instantiated from its own _Docker image_.
<2> The container can bind to ports which will be made available on the pod's IP.
<3> OpenShift defines a _security context_ for containers which specifies
whether they are allowed to run as privileged containers, run as a user of their
choice, and more. The default context is very restrictive but administrators can
 modify this as needed.
<4> The container specifies where external storage volumes should be mounted
within the container. In this case, there is a volume for storing the registry's
 data, and one for access to credentials the registry needs for making requests
 against the OpenShift API.

ifdef::showscript[]
=== Transcript
Each container in the pod is instantiated from its own _Docker image_, The
container can bind to ports which will be made available on the pod's IP.

OpenShift defines a _security context_ for containers which specifies
whether they are allowed to run as privileged containers, run as a user of their
choice, and more. The default context is very restrictive but administrators can
 modify this as needed.

 The container specifies where external storage volumes should be mounted
 within the container. The second volume in this example is used to provide a
 "secret" value for the Pod to use.

endif::showscript[]

== Pods and Services
:noaudio:

.Pods Definition file/Manifest - Continued

[source,yaml]
----
   dnsPolicy: ClusterFirst
   imagePullSecrets:
   - name: default-dockercfg-at06w
   restartPolicy: Always
   serviceAccount: default               <1>
   volumes:                              <2>
   - emptyDir: {}
     name: registry-storage
   - name: default-token-br6yz
     secret:
       secretName: default-token-br6yz
----

<1> Pods making requests against the OpenShift API is a common enough pattern
 that there is a `serviceAccount` field for specifying which _service account_
  user the pod should authenticate as when making the requests. This enables
  fine-grained access control for custom infrastructure components.
<2> The pod defines storage volumes that are available to its container(s) to
 use. In this case, it provides an ephemeral volume for the registry storage and
 a *secret* volume containing the service account credentials.


ifdef::showscript[]
=== Transcript

Pods making requests against the OpenShift API is a common enough pattern
  that there is a `serviceAccount` field for specifying which _service account_
   user the pod should authenticate as when making the requests.

The pod defines storage volumes that are available to its container(s) to use.
In this case, it provides an ephemeral volume for the registry storage and a
*secret* volume containing the service account credentials.

You can learn more about using "secrets" in the developer guide documentation.

endif::showscript[]

== Pods and Services

.Services

* A Kubernetes _service_ serves as an *internal* load balancer.
 It identifies a set of replicated _pods_ in order to proxy the connections it
 receives to them.
* Backing pods can be added to or removed from a service arbitrarily while the
 _service_ remains consistently available, enabling anything that depends on the
 _service_ to refer to it at a consistent internal address.

* Services are assigned an IP address and port pair that, when accessed,
 proxy to an appropriate backing pod.
* A service uses a label selector to find  all the containers running that
 provide a certain network service on a certain port.
* A server can be accessed by its IP address and a local DNS name created and
resolved by the local DNS server on the master.

NOTE: When we use the "_Default Router_" (HA-Proxy) we bypass the _Service_ load
balancing. We only use the _Service_ to find out which _Pods_ are represented by
 The Service and load balancing use done by the "_Router_"

ifdef::showscript[]
=== Transcript

A Kubernetes _service_ serves as an *internal* load balancer.
It identifies a set of replicated _pods_ in order to proxy the connections it
receives to them. A service uses a label selector to find  all the containers
running that provide a certain network service on a certain port.

Services are assigned an IP address and port pair that, when accessed,
proxy to an appropriate backing pod. Backing pods can be added to or removed
from a service arbitrarily while the _service_ remains consistently available,
enabling anything that depends on the _service_ to refer to it at a consistent
internal address.

When we use the "_Default Router_" (HA-Proxy) we bypass the _Service_ load
balancing. We only use the _Service_ to find out which _Pods_ are represented by
 The Service and load balancing use done by the "_Router_"

endif::showscript[]

== Pods and Services
:noaudio:

.Service Definition file/Manifest

* Like pods, services are REST objects. The following
example shows the definition of a service for the pod defined above:
+
[source,yaml]
----
 apiVersion: v1
 kind: Service
 metadata:
   name: docker-registry      <1>
 spec:
   selector:                  <2>
     docker-registry: default
   portalIP: 172.30.136.123   <3>
   ports:
   - nodePort: 0
     port: 5000               <4>
     protocol: TCP
     targetPort: 5000 <5>
----

<1> The service name *docker-registry* is also used to construct an
 environment variable with the service IP that is inserted into other
 pods in the same namespace.
<2> The label selector identifies all pods with the
 *docker-registry=default* label attached as its backing pods.
<3> Virtual IP of the service, allocated automatically at creation from a pool
 of internal IPs.
<4> Port the service listens on.
<5> Port on the backing pods to which the service forwards connections.


ifdef::showscript[]
=== Transcript

Like pods, services are REST objects, you will see this method being used a lot
in this training.
The following example shows the definition of a service for the pod defined
above.

The service name *docker-registry* is also used to construct an
environment variable with the service IP that is inserted into other
pods in the same namespace (or Project).

The label selector identifies all pods with the *docker-registry=default* label
attached as its backing pods.

The _PortalIP_ represents the IP the service will be listening on, when we
define a new _service_ we will leave this blank to get assigned a random IP.
endif::showscript[]

== Pods and Services
:noaudio:

.Labels

* Labels are used to organize, group, or select API objects.
**  For example, _pods_ are "tagged" with labels, and then
 _services_ use label selectors to identify the pods they
 proxy to.
** This makes it possible for services to reference groups of pods, even
treating pods with potentially different docker containers as related entities.

* Most objects can include labels in their metadata. So labels can  be used to
group arbitrarily-related objects; for example, all of the _pods_, _services_,
 _replication controllers_, and _deployment configurations_ of a particular
 application can be grouped.


ifdef::showscript[]
=== Transcript

Labels are used to organize, group, or select API objects.
For example, _pods_ are "tagged" with labels, and then _services_ use label
selectors to identify the pods in their scope.

Most objects can include labels in their metadata. So labels can  be used to
group arbitrarily-related objects; for example, all of the _pods_, _services_,
 _replication controllers_, and _deployment configurations_ of a particular
 application can be grouped.

endif::showscript[]

== Pods and Services
:noaudio:

.Labels - Continued

* Labels are simple key/value pairs, as in the following example:
+
[source,yaml]
----
 labels:
   key1: value1
   key2: value2
----

* Consider:
 ** A pod consisting of an *nginx* docker container, with the label
 *role=webserver*.
 ** A pod consisting of an *Apache httpd* docker container, with the same label
 *role=webserver*.
 ** A service or replication controller that is defined to use pods with the
 *role=webserver* label treats both of these pods as part of the same group.

* Here is an example how you would remove all the components with the same label
*app=mytest*.
+
----
# oc delete all -l app=mytest
----

ifdef::showscript[]
=== Transcript

Labels are simple key/value pairs that can be used in almost every type of
object in OSE.

Labels are handy to manage groups of resources easily, without them you will be
required to delete/manage each item independently.

endif::showscript[]


== Scheduler
:noaudio:

.Overview

* The Kubernetes pod scheduler is responsible for determining placement of new
pods onto nodes within the OSE cluster.
* It reads data from the pod and tries to find a node that is a good fit based
on configured policies.
* It is completely independent and exists as a standalone/pluggable solution.
* It does not modify the pod and just creates a binding for the pod that ties
the pod to the particular node.

ifdef::showscript[]
=== Transcript

The Kubernetes pod scheduler is responsible for determining placement of new
pods onto nodes within the OSE cluster. It reads data from the pod and tries to
find a node that is a good fit based on configured policies.

The _Scheduler_ is completely independent and exists as a standalone/pluggable
solution.

It does not modify the pod and just creates a binding for the pod that ties
the pod to the particular node.

endif::showscript[]

== Scheduler
:noaudio:

.Scheduler Extensibility
As is the case with almost everything else in Kubernetes/OpenShift, the
scheduler is built using a plugin model and the current implementation itself is
a plugin.  There are two ways to extend the scheduler functionality:

* Enhancements
** The scheduler functionality can be enhanced by adding new predicates and
priority functions.  They can either be contributed upstream or maintained
separately.
** These predicates and priority functions would need to be registered with the
scheduler factory and then specified in the scheduler policy file.

* Replacement
** Since the scheduler is a plugin, it can be replaced in favor of an alternate
implementation.
** The scheduler code has a clean separation that watches new pods as they get
created and identifies the most suitable node to host them.
** It then creates bindings (pod to node bindings) for the pods using the master
API.



ifdef::showscript[]
=== Transcript

As is the case with almost everything else in Kubernetes/OpenShift, the
scheduler is built using a plugin model and the current implementation itself is
a plugin.

 There are two ways to extend the scheduler functionality, enhancements and
 replacement, you can either enhance the scheduler functionality by adding new
 predicates and priority functions (These would be covered in the next few
   slides) or you can completely replace the scheduler plugin with your own
   implementation.

endif::showscript[]



== Scheduler
:noaudio:

.Generic Scheduler

* The existing generic scheduler is the default OSE-provided scheduler
"engine" that selects a node to host the pod in a 3-step operation:

- Step 1: Filter the nodes
The available nodes are filtered based on the constraints or requirements
specified. This is done by running each of the nodes through the list of filter
functions called 'predicates'.

- Step 2: Prioritize the filtered list of nodes
This is achieved by passing each node through a series of 'priority' functions
that assign it a score between 0 - 10, with 0 indicating a bad fit and 10
indicating a good fit to host the pod.

- Step 3: Select the best fit node
The nodes are sorted based on their scores and the node with the highest score
is selected to host the pod. If multiple nodes have the same high score, then
one of them is selected at random.

NOTE:  The node score provided by each priority
function is multiplied by the "weight" and then combined by just adding the
scores for each node provided by all the priority functions.

ifdef::showscript[]
=== Transcript

The existing generic scheduler is the default OSE provided scheduler
"engine" that selects a node to host the pod in a 3-step operation:
- Step 1: Filter the nodes, in this step we disqualify any nodes that do not fit
 our requirements.

- Step 2: Prioritize the filtered list of nodes, between the nodes that were NOT
 disqualified in the last step, the _Scheduler_ prioritizes the nodes using the
 _Priority_ functions (more on this in the next slides)

The scheduler configuration can also take in a simple "weight" (positive numeric
  value) for each priority function. The node score provided by each priority
  function is multiplied by the "weight" (default weight is 1) and then combined
   by just adding the scores for each node provided by all the priority
   functions.

This weight attribute can be used by administrators to give higher importance to
 some priority functions.

endif::showscript[]

== Scheduler
:noaudio:

.Scheduler Policy
* The selection of the predicate and priority functions defines the policy for
the scheduler.
* Administrators can provide a JSON file that specifies the predicates and
priority functions to configure the scheduler.
** The path to the  scheduler policy file can be specified in the master
configuration file.
* In the absence of the scheduler policy file, the default configuration gets
applied.

NOTE: That the predicates and priority functions defined in the scheduler
configuration file will completely override the default scheduler policy.
If any of the default predicates and priority functions are required,
they have to be explicitly specified in the scheduler configuration file.


ifdef::showscript[]
=== Transcript

The selection of the predicate and priority functions defines the policy for
the scheduler.
Administrators can provide a JSON file that specifies the predicates and
priority functions to configure the scheduler, the path to the scheduler policy
file can be specified in the master configuration file.

endif::showscript[]


== Scheduler
:noaudio:

.Default Scheduler Policy
* The default scheduler policy includes the following predicates.

1. PodFitsPorts
1. PodFitsResources
1. NoDiskConflict
1. MatchNodeSelector
1. HostName

* The default scheduler policy includes the following priority functions.
** Each of the priority function has a weight of '1' applied to it.

1. LeastRequestedPriority
1. BalancedResourceAllocation
1. ServiceSpreadingPriority


ifdef::showscript[]
=== Transcript

The default scheduler policy includes the following "predicates" and "priority
functions", After nodes gets disqualified (or "opted-out") by the "predicates"
the "Priority functions" and their _weight_ are used to define the best fit for
our new pod.

endif::showscript[]



== Scheduler
:noaudio:

.Available Predicates
* There are several predicates provided out of the box in Kubernetes.
Some of these predicates can be customized by providing certain parameters.
* Multiple predicates can be combined to provide additional filtering of nodes.

* In the next few slides we will discuss *Static Predicates* and
*Configurable Predicates*.

ifdef::showscript[]
=== Transcript

There are several predicates provided out of the box in Kubernetes.
Some of these predicates can be customized by providing certain parameters.

In the next few slides we will discuss *Static Predicates* and
*Configurable Predicates*.


endif::showscript[]


== Scheduler
:noaudio:

.Static Predicates
These predicates do not take any configuration parameters or inputs from the
user. These are specified in the scheduler configuration using their exact name.

* *_PodFitsPorts_* deems a node to be fit for hosting a pod based on the absence
of port conflicts.

* *_PodFitsResources_* determines a fit based on resource availability.
The nodes can declare their resource capacities and then pods can specify what
resources they require.  Fit is based on requested, rather than used resources.

* *_NoDiskConflict_* determines fit based on non-conflicting disk volumes.
It evaluates if a pod can fit due to the volumes it requests, and those that
are already mounted.
* *_MatchNodeSelector_* determines fit based on node selector query that is
defined in the pod.

* *_HostName_* determines fit based on the presence of the Host parameter and a
string match with the name of the host.

ifdef::showscript[]
=== Transcript

Static Predicates do not take any configuration parameters or inputs from the
user. These are specified in the scheduler configuration using their exact name.

endif::showscript[]


== Scheduler
:noaudio:

.Configurable Predicates
* These predicates can be configured by the user to tweak their function.
** They can be given any user-defined name.
** The type of the predicate is identified by the argument that they take.
* Since these are configurable, multiple predicates of the same type (but
  different configuration parameters) can be combined as long as their
  user-defined names are different.


ifdef::showscript[]
=== Transcript

Configurable Predicates can be configured by the user to tweak their function.
They can be given any user-defined name, the type of the predicate is identified
 by the argument that they take.

Since these are configurable, multiple predicates of the same type (but
  different configuration parameters) can be combined as long as their
  user-defined names are different.

endif::showscript[]



== Scheduler
:noaudio:

.Configurable Predicates - Continued
* *_ServiceAffinity_* filters out nodes that do not belong to the specified
topological level defined by the provided labels.
** This predicate takes in a list of labels and ensures affinity within the nodes
(that have the same label values) for pods belonging to the same service.
** If the pod specifies a value for the labels in its NodeSelector, then the
nodes matching those labels are the ones where the pod is scheduled.

----
{"name" : "Zone", "argument" : {"serviceAffinity" : {"labels" : ["zone"]}}}
----
* *_LabelsPresence_* checks whether a particular node has a certain label defined
or not, regardless of its value.
----
{"name" : "ZoneRequired", "argument" : {"labels" : ["retiring"], "presence" : false}}
----

ifdef::showscript[]
=== Transcript

*_ServiceAffinity_* filters out nodes that do not belong to the specified
topological level defined by the provided labels, This predicate takes in a list
 of labels and ensures affinity within the nodes (that have the same label
   values) for pods belonging to the same service.

If the pod specifies a value for the labels in its NodeSelector, then ONLY
nodes matching those labels are the ones where the pod can be scheduled.

endif::showscript[]


== Scheduler
:noaudio:

.Available Priority Functions
* A custom set of priority functions can be specified to configure the scheduler.
** There are several priority functions provided out-of-the-box in OSE.
* Some of these priority functions can be customized by providing certain parameters.
* Multiple priority functions can be combined and different weights can be given
to each in order to impact the prioritization.
** A weight is required to be specified and cannot be 0 or negative.

ifdef::showscript[]

=== Transcript

A custom set of priority functions can be specified to configure the scheduler.
There are several priority functions provided out-of-the-box in OSE, some of
these priority functions can be customized by providing certain parameters.

Multiple priority functions can be combined and different weights can be given
to each in order to impact the prioritization, A weight is required to be
specified and cannot be 0 or negative.
endif::showscript[]



== Scheduler
:noaudio:

.Static Priority Functions
These priority functions do not take any configuration parameters or inputs from
the user.  These are specified in the scheduler configuration using their exact
name as well as the weight.

* *_LeastRequestedPriority_* favors nodes with fewer requested resources.
It calculates the percentage of memory and CPU requested by pods scheduled on
the node, and prioritizes nodes that have the highest available/remaining capacity.

* *_BalancedResourceAllocation_* favors nodes with balanced resource usage rate.
It calculates the difference between the consumed CPU and memory as a fraction
of capacity, and prioritizes the nodes based on how close the two metrics are
to each other.  This should always be used together with _LeastRequestedPriority_.

* *_ServiceSpreadingPriority_* spreads pods by minimizing the number of pods
belonging to the same service onto the same machine

* *_EqualPriority_* gives an equal weight of one to all nodes and is not
required/recommended outside of testing.


ifdef::showscript[]
=== Transcript

Static Priority Functions do not take any configuration parameters or inputs
from the user.  These are specified in the scheduler configuration using their
exact name as well as the weight.

endif::showscript[]


== Scheduler
:noaudio:

.Configurable Priority Functions
* These priority functions can be configured by the user by providing certain
parameters.
** They can be given any user-defined name.
* The type of the priority function is identified by the argument that they take.

* *_ServiceAntiAffinity_* takes a label and ensures a good spread of the pods
belonging to the same service across the group of nodes based on the label
values.  It gives the same score to all nodes that have the same value for the
specified label.  It gives a highter score to nodes within a group with the
least concentration of pods.

* *_LabelsPreference_* prefers nodes that have a particular label defined or
not, regardless of its value.



ifdef::showscript[]
=== Transcript

Configurable Priority Functions can be configured by the user by providing
certain parameters.
endif::showscript[]



== Scheduler
:noaudio:

.Use Cases

* One of the important use cases for scheduling within OpenShift is to support
flexible affinity and anti-affinity policies.

* OpenShift can implement multiple Infrastructure Topological Levels
** Administrators can define multiple topological levels for their infrastructure
(nodes).
** This is done by specifying labels on nodes
(eg: region = r1, zone = z1, rack = s1).
** These label names have no particular meaning and administrators are free to
name their infrastructure levels anything (eg, city/building/room).
** Administrators can define any number  of levels for their infrastructure
topology, with three levels usually being
adequate (eg. regions --> zones --> racks).
Administrators can specify affinity and anti-affinity rules at each of these
levels in any combination.

ifdef::showscript[]
=== Transcript

One of the important use cases for scheduling within OpenShift is to support
flexible affinity and anti-affinity policies, By specifiyng labels on the nodes
administrators can define multiple topological levels for their infrastructure.

Look at a few of the examples here, although we are using the "Region" and
"Zones" example in this training, you can use any kind of topology that makes
sense in your environment.

endif::showscript[]


== Scheduler
:noaudio:

.Affinity and Anti-Affinity

* Affinity
** Administrators should be able to configure the scheduler to specify affinity
at any topological level, or even at multiple levels.
** Affinity at a particular level indicates that all pods that belong to the
same service will be scheduled onto nodes that belong to the same level.
** This handles any latency requirements of applications by allowing
administrators to ensure that peer pods do not end up being too geographically
separated.  If no node is available within the same affinity group to host the
pod, then the pod will not get scheduled.

ifdef::showscript[]
=== Transcript

Functions based on Affinity and Anti-Affinity are a good place to start when
looking into the possibilities of use for the Scheduler.

Affinity function makes sense when you want all components of a _service_ (By
  This we mean all the pods) to be located in the same "Zone"/"Region"/"Node".
endif::showscript[]


== Scheduler
:noaudio:

.Affinity and Anti-Affinity

* Anti Affinity
** Administrators should be able to configure the scheduler to specify
anti-affinity at any topological level, or even at multiple levels.
** Anti-Affinity (or 'spread') at a particular level indicates that all pods
that belong to the same service will be spread across nodes that belong to that
level.
** This ensures that the application is well spread for high availability
purposes.
** The scheduler will try to balance the service pods across all applicable nodes
as evenly as possible.


ifdef::showscript[]
=== Transcript

Anti-Affinity (or 'spread') at a particular level indicates that all pods
that belong to the same service will be spread across nodes that belong to that
level.

This is useful when trying to create an highly available _service_ that is
spread between availability zones or Racks.
endif::showscript[]


== Scheduler
:noaudio:

.Sample Policy Configurations
The configuration below specifies the default scheduler configuration, if it
were to be specified via the scheduler policy file.
----
{
	"kind" : "Policy",
	"version" : "v1",
	"predicates" : [
		{"name" : "PodFitsPorts"},
		{"name" : "PodFitsResources"},
		{"name" : "NoDiskConflict"},
		{"name" : "MatchNodeSelector"},
		{"name" : "HostName"}
	],
	"priorities" : [
		{"name" : "LeastRequestedPriority", "weight" : 1},
		{"name" : "BalancedResourceAllocation", "weight" : 1},
		{"name" : "ServiceSpreadingPriority", "weight" : 1}
	]
}
----

ifdef::showscript[]
=== Transcript

The configuration below specifies the default scheduler configuration, if it
were to be specified via the scheduler policy file.

endif::showscript[]


== Scheduler
:noaudio:

.Topology Examples


* Example: Three topological levels defined as region, zone and rack
** Levels: region (affinity) -> zone (affinity) -> rack (anti-affinity)
+
[source,json]
----
{
	"kind" : "Policy",
	"version" : "v1",
	"predicates" : [
		...
		{"name" : "RegionZoneAffinity", "argument" : {"serviceAffinity" : {"labels" : ["region", "zone"]}}}
	],
	"priorities" : [
		...
    {"name" : "RackSpread", "weight" : 1, "argument" : {"serviceAntiAffinity" : {"label" : "rack"}}}
	]
}
----

NOTE: In all of the sample configurations provided, the list of predicates and
priority functions is truncated to include only the ones that pertain to the use
case specified. In practice, a complete/meaningful scheduler policy should
include most, if not all, of the default predicates and priority functions
listed above.


ifdef::showscript[]
=== Transcript

We can use as little or as many topological levels as we like in the
_Scheduler_.
Here is an example of three topological levels defined as region, zone and rack

This policy will create a scheduling process that will put pods in the same
Region and Zone but spread the pods between the Racks within the zone.

endif::showscript[]


== Scheduler
:noaudio:

.Topology Examples
* Example: Three topological levels defined as city, building and room
** Levels: city (affinity) -> building (anti-affinity) -> room (anti-affinity)

[source,json]
----
{
	"kind" : "Policy",
	"version" : "v1",
	"predicates" : [
		...
		{"name" : "CityAffinity", "argument" : {"serviceAffinity" : {"labels" : ["city"]}}}
	],
	"priorities" : [
		...
		{"name" : "BuildingSpread", "weight" : 1, "argument" : {"serviceAntiAffinity" : {"label" : "building"}}},
		{"name" : "RoomSpread", "weight" : 1, "argument" : {"serviceAntiAffinity" : {"label" : "room"}}}
	]
}
----


ifdef::showscript[]
=== Transcript

Here is another 3 level topology example, this time we try to keep the Pods in
the same *City* and spread them between the *buildings* and between the *rooms*
in each building.

endif::showscript[]


== Scheduler
:noaudio:

.Topology Examples

* Only use nodes with the 'region' label defined and prefer nodes with the
'zone' label defined

[source,json]
----
{
	"kind" : "Policy",
	"version" : "v1",
	"predicates" : [
		...
		{"name" : "RequireRegion", "argument" : {"labelsPresence" : {"labels" : ["region"], "presence" : true}}}

	],
	"priorities" : [
		...
		{"name" : "ZonePreferred", "weight" : 1, "argument" : {"labelPreference" : {"label" : "zone", "presence" : true}}}
	]
}
----

ifdef::showscript[]
=== Transcript

One last simple example, This policy will mean: a *node* MUST have a *region*
label, and we will prefer to use a *node* that also has a *zone* label defined.

endif::showscript[]




== Builds and Image Streams
:noaudio:

.Builds Overview

* A _build_ is the process of transforming input parameters into a resulting object.
- Most often, the process is used to transform source code into a runnable image.
- A _BuildConfig_ object is the definition of the entire build process.

* The OpenShift build system provides extensible support for _build strategies_
that are based on selectable types specified in the build API. There are three
build strategies available:
- Docker build
- Source-to-Image (S2I) build
- Custom build

By default, Docker builds and S2I builds are supported.

ifdef::showscript[]
=== Transcript

A _build_ is the process of transforming input parameters into a resulting object.
A _BuildConfig_ object is the definition of the entire build process.

The OpenShift build system provides extensible support for _build strategies_
that are based on selectable types specified in the build API. There are three
build strategies available:

- Docker build - A build based on a _Dockerfile_
- Source-to-Image (S2I) build - OpenShift's built-in builder, builds an image
based on a base image and source code provided as a Git repository
- Custom build - This can be any process a user can define, an easy example
would be a Jenkins server that builds a Docker Image outside of the OSE environment.

endif::showscript[]


== Builds and Image Streams
:noaudio:

.Builds Overview - Continued

* The resulting object of a build depends on the builder used to create it.
* For Docker and S2I builds, the resulting objects are runnable images.
* For Custom builds, the resulting objects are whatever the builder image author
 has specified.

* For a list of build commands, see the :
link:https://docs.openshift.org/latest/dev_guide/builds.html[Developer's Guide].

ifdef::showscript[]
=== Transcript

The resulting object of a build depends on the builder used to create it, for
Docker and S2I builds, the resulting objects are runnable images.

endif::showscript[]


== Builds and Image Streams
:noaudio:

.Docker Build

* The Docker build strategy invokes the plain _docker build_ command,
and therefore expects a repository with a *_Dockerfile_* and all required
artifacts in it to produce a runnable image.


.Source-to-Image (S2I) Build
* Source-to-Image (S2I)] is a tool for building reproducible Docker images.
* It produces ready-to-run images by injecting a user source into a docker image
 and assembling a new docker image.
* The new image incorporates the base image (the builder) and built source, and
is ready to use with the `docker run` command.
* S2I supports incremental builds, which re-use previously downloaded
dependencies, previously built artifacts, etc.

ifdef::showscript[]
=== Transcript

The Docker build strategy invokes the plain _docker build_ command,
and therefore expects a repository with a *_Dockerfile_* and all required
artifacts in it to produce a runnable image.

The Source-to-Image (S2I) is a tool for building reproducible Docker images, it
produces ready-to-run images by injecting a user source into a docker image
 and assembling a new docker image.

The new image incorporates the base image (the builder) and built source, and
is ready to use within the OSE environment or with `docker run`

S2I supports incremental builds, which re-use previously downloaded
dependencies, previously built artifacts, etc.

endif::showscript[]


== Builds and Image Streams
:noaudio:

.S2I Advantages

[horizontal]
Image flexibility:: S2I scripts can be written to layer application code onto
almost any existing Docker image, taking advantage of the existing ecosystem.
Note that, currently, S2I relies on `tar` to inject application
source, so the image needs to be able to process tarred content.

Speed:: With S2I, the assemble process can perform a large number of complex
operations without creating a new layer at each step, resulting in a fast
process. In addition, S2I scripts can be written to re-use artifacts stored in a
previous version of the application image, rather than having to download or
build them, each time the build is run.

Patchability:: S2I allows you to rebuild the application consistently if an
underlying image needs a patch due to a security issue.

ifdef::showscript[]
=== Transcript
Image flexibility, Speed and Patchability are only some of the advantages that
Using S2I provides, S2I scripts can be written to layer application code onto
almost any existing Docker image, this means you can switch your builder image
from Centos to RHEL of from RHEL7.1 to RHEL7.2 without issues, simply rebuild
 the image and start using it.

S2I's assemble process can perform a large number of complex
operations without creating a new layer at each step, resulting in a fast
process.

An image could be rebuilt quickly in case the base image requires a patch, for
example if there is a new security patch.

endif::showscript[]


== Builds and Image Streams
:noaudio:

.S2I Advantages - Continued

[horizontal]
Operational efficiency:: By restricting build operations instead of allowing
arbitrary actions, such as in a *_Dockerfile_*, the PaaS operator can avoid
accidental or intentional abuses of the build system.

Operational security:: Building an arbitrary *_Dockerfile_* exposes the host
system to root privilege escalation. This can be exploited by a malicious user
because the entire docker build process is run as a user with docker privileges.
S2I restricts the operations performed as a root user, and can run the scripts
as a non-root user.

User efficiency:: S2I prevents developers from performing arbitrary `yum
install` type operations during their application build, which results in slow
development iteration.

Ecosystem:: S2I encourages a shared ecosystem of images where you can leverage
best practices for your applications.

ifdef::showscript[]
=== Transcript
A few more advantages of the S2I process are the efficiencies it provides and
the operational security advantages.

By restricting build operations instead of allowing
arbitrary actions, such as in a *_Dockerfile_*, the PaaS operator can avoid
accidental or intentional abuses of the build system. S2I prevents developers
from performing arbitrary `yum install` type operations during their application
 build, which results in slow development iteration.


endif::showscript[]


== Builds and Image Streams
:noaudio:

.Custom Build

* The Custom build strategy allows developers to define a specific builder image,
responsible for the entire build process. Using your own builder image allows
you to customize your build process.

* The builder could call out to an external system, such as Jenkins or any other
 automation agent to create the image and push it into our registry.

ifdef::showscript[]

=== Transcript
The Custom build strategy allows developers to define a specific builder image,
responsible for the entire build process. Using your own builder image allows
you to customize your build process.

The builder could call out to an external system, such as Jenkins or any other
 automation agent to create the image and push it into our registry.

endif::showscript[]


== Builds and Image Streams
:noaudio:

.Image Streams

* An _image stream_ is similar to a Docker image repository in that it contains
one or more _Docker images_ identified by tags.
** An image stream presents a single virtual view of related images, it may
contain images from:

. Its own image repository in OpenShift's integrated Docker Registry
. Other image streams
. Docker image repositories from external registries

* OpenShift stores complete metadata about each image (e.g., command, entrypoint,
environment variables, etc.). Images in OpenShift are immutable.

* OpenShift components such as builds and deployments can watch an image stream
and receive notifications when new images are added, reacting by performing a
build or a deployment, for example.

ifdef::showscript[]
=== Transcript
An image stream presents a single virtual view of related images, an
_image stream_ is similar to a Docker image repository in that it contains one
or more _Docker images_ identified by tags.

OpenShift components such as builds and deployments can watch an image stream
and receive notifications when new images are added, reacting by performing a
build or a deployment, for example.



endif::showscript[]


== Builds and Image Streams
:noaudio:

.Image Pull Policy

* Each container in a pod has a Docker image. Once you have created an image and
pushed it to a registry, you can then refer to it in the pod.

* When OpenShift creates containers, it uses the container's `imagePullPolicy`
to determine if the image should be pulled prior to starting the container.

* There are three possible values for `imagePullPolicy`:

- `Always` - always pull the image.
- `IfNotPresent` - only pull the image if it does not already exist on the node.
- `Never` - never pull the image.

NOTE: If a container's `imagePullPolicy` parameter is not specified, OpenShift
sets it based on the image's tag, if the tag is: *latest*, OpenShift defaults
`imagePullPolicy` to *Always*.

ifdef::showscript[]
=== Transcript

Each container in a pod has a Docker image. Once you have created an image and
pushed it to a registry, you can then refer to it in the pod.

When OpenShift creates containers, it uses the container's `imagePullPolicy`
to determine if the image should be pulled prior to starting the container.

There are three possible values for `imagePullPolicy`:
- `Always` - always pull the image.
- `IfNotPresent` - only pull the image if it does not already exist on the node.
- `Never` - never pull the image.


endif::showscript[]

== Replication Controllers
:noaudio:

.Replication Controllers Overview

* A replication controller ensures that a specified number of replicas of a pod
are running at all times.
* If pods exit or are deleted, the replica controller acts to instantiate more
up to the desired number.
* Likewise, if there are more running than desired, it deletes as many as necessary to match the number.


ifdef::showscript[]
=== Transcript

A replication controller job is to ensure that a specified number of replicas of
 a pod are running at all times.

If pods exit or are deleted, the replica controller acts to instantiate more up
to the desired number.

Likewise, if there are more running than desired, it deletes as many as
necessary to match the number.


endif::showscript[]

== Replication Controllers
:noaudio:

.Replication Controllers Overview - Continued

* The definition of a replication controller consists mainly of:
- The number of replicas desired (which can be adjusted at runtime).
- A pod definition for creating a replicated pod.
- A selector for identifying managed pods.

* The selector is just a set of labels that all of the pods managed by the
replication controller should have. So that set of labels is included
in the pod definition that the replication controller instantiates.

* This selector is used by the replication controller to determine how many
instances of the pod are already running in order to adjust as needed.

NOTE: It is *not the job of the replication controller* to perform `auto-scaling`
based on load or traffic, as it does not track either; rather, this
would require its replica count to be adjusted by an external auto-scaler.



ifdef::showscript[]
=== Transcript
The definition of a replication controller consists mainly of:
- The number of replicas desired (which can be adjusted at runtime).
- A pod definition for creating a replicated pod.
- A selector for identifying managed pods.

The selector, that is essentially a set of labels, is used by the replication
controller to determine how many instances of the pod are already running in
order to adjust as needed.

It is *not the job of the replication controller* to perform `auto-scaling`
based on load or traffic, as it does not track either; rather, this
would require its replica count to be adjusted by an external auto-scaler.

endif::showscript[]

== Replication Controllers
:noaudio:

.Replication Controllers  Definition file/Manifest

* Replication controllers are a core Kubernetes object, _ReplicationController_.
* Here is an example _ReplicationController_ definition with some omissions
and callouts:
+
[source,yaml]
----
apiVersion: v1
kind: ReplicationController
metadata:
  name: frontend-1
spec:
  replicas: 1  <1>
  selector:    <2>
    name: frontend
  template:    <3>
    metadata:
      labels:  <4>
        name: frontend
    spec:
      containers:
      - image: openshift/hello-openshift
        name: helloworld
        ports:
        - containerPort: 8080
          protocol: TCP
      restartPolicy: Always
----

<1> The number of copies of the pod to run.
<2> The label selector of the pod to run.
<3> A template for the pod the controller creates.
<4> Labels on the pod should include those from the label selector.

ifdef::showscript[]
=== Transcript

Replication controllers are a core Kubernetes object, _ReplicationController_.

Here is an example _ReplicationController_ definition with some omissions
and callouts.

endif::showscript[]

== Routers
:noaudio:

.Routers - Overview

* An OpenShift administrator can deploy *routers* (like the HA-Proxy, "Default
Router") in an OpenShift cluster.
* These enable `route` resources created by developers to be used by external
clients.

* OpenShift *routers* provide external hostname mapping and load balancing
to applications over protocols that pass distinguishing information directly to
the router.

* Currently (OSEv3.0.0.0) OpenShift's *Default Router* only support the
following protocols:
- HTTP
- HTTPS

ifdef::showscript[]
=== Transcript

An OpenShift administrator can deploy *routers* (like the HA-Proxy, "Default
Router") in an OpenShift cluster. These enable routes created by developers to
be used by external clients.

OpenShift routers provide external hostname mapping and load balancing to
applications over protocols that pass distinguishing information directly to
the router.

endif::showscript[]

== Routers
:noaudio:

.HAProxy default Router

* The HAProxy default router implementation is the reference implementation for
a template router plug-in. It uses the *openshift3/ose-haproxy-router*
image to run an HAProxy instance alongside the template router plug-in.


ifdef::showscript[]
=== Transcript

The HAProxy default router implementation is the reference implementation for
a template router plug-in. It uses the *openshift3/ose-haproxy-router*
image to run an HAProxy instance alongside the template router plug-in.

endif::showscript[]


== Routers
:noaudio:
.Routers and Routes

* A `route` object (NOT *router*) is an object that describes a `service` to
expose and a host FQDN, for example, A `route` could specify a hostname of
"myapp.mysubdomain.company.com" and the `service` "MyappFrontend".
* To allow an external web client to access an application (Pod/Pods) on OSE
using a DNS name a developer would create a `route` object for his application.

* A router uses the service selector to find the service and the endpoints
backing the service.
* *Service-provided load balancing is bypassed* and replaced with the router's own
 load balancing.
* Routers watch the cluster API and automatically update their own configuration
 according to any relevant changes in the API objects.
 * Routers may be containerized or virtual.
 ** Custom routers can be deployed to communicate modifications of API objects
 to another system, such as an *F5*.

ifdef::showscript[]
=== Transcript
A `route` object (NOT *router*) is an object that describes a `service` to
expose and a host FQDN, for example, A `route` could specify a hostname of
"myapp.mysubdomain.company.com" and the `service` "MyappFrontend".

To allow an external web client to access an application (Pod/Pods) on OSE
using a DNS name a developer would create a `route` object for his application.

A router uses the service selector to find the service and the endpoints
backing the `service` defined in the `route`, The *Service-provided load
balancing is bypassed* and replaced with the router's own
 load balancing.

Custom routers can be deployed to communicate modifications of API objects to
another system, such as an *F5*.

endif::showscript[]

== Routers
:noaudio:

.Routers and Routes - Continued

* In order to reach a router in the first place, requests for hostnames
must resolve via DNS to a router or set of routers.
* The suggested method is to define a cloud domain with a wildcard DNS entry
pointing to a virtual IP backed by multiple router instances on designated nodes.
* DNS for addresses outside the cloud domain would need to be configured
individually. Other approaches may be feasible.

ifdef::showscript[]
=== Transcript
In order to reach a router in the first place, requests for hostnames
must resolve via DNS to a router or set of routers.

The suggested method is to define a cloud domain with a wildcard DNS entry
pointing to a virtual IP backed by multiple router instances on designated nodes.

DNS for addresses outside the cloud domain would need to be configured
individually. Other approaches may be feasible.

endif::showscript[]

== Routers
:noaudio:

.Sticky Sessions

* Implementing sticky sessions is up to the underlying router configuration.
* The default HAProxy template implements sticky sessions using the
*balance source* directive which balances based on the source IP.
* In addition, the template router plug-in provides the service name and
namespace to the underlying implementation.
** This can be used for more advanced configuration such as implementing
stick-tables that synchronize between a set of peers.

NOTE: Specific configuration for this router implementation is stored in the
*_haproxy-config.template_* file located in the *_/var/lib/haproxy/conf_*
directory of the router container.

ifdef::showscript[]
=== Transcript
Implementing sticky sessions is up to the underlying router configuration.

The default HAProxy template implements sticky sessions using the
*balance source* directive which balances based on the source IP.

In addition, the template router plug-in provides the service name and
namespace to the underlying implementation.

This can be used for more advanced configuration such as implementing
stick-tables that synchronize between a set of peers.

endif::showscript[]




== Summary
:noaudio:
* Overview
* Containers and Images
* Pods and Services
* Projects and Users
* Builds and ImageStreams
* Replication Controllers
* Routers


ifdef::showscript[]
=== Transcript
In this modules we discussed OSE's Core concepts,
* Containers and Images
* Pods and Services
* Projects and Users
* Builds and ImageStreams
* Replication Controllers
* Routers


endif::showscript[]
