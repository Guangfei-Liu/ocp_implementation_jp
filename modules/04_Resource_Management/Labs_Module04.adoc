:icons: images/icons
:toc2:		

:numbered:

== Resource Management

In this module we have the following labs: 

* Lab: Configure Authentication
* Lab: Users, Projects and Quotas
* Lab: Creating Services and Routes
* Lab: Deployment Configuration and Replication Controllers


=== Lab Environment Architecture and Important Information

The lab environment consists of 4 VMs:

* `oselab-GUID.oslab.opentlc.com` (administration host)

* `master00-GUID.oslab.opentlc.com` (master host, contains Etcd and the management console)

* `infranode00-GUID.oslab.opentlc.com` (infranode host, Will run our infrastructure containers: Registry and Router)

* `node00-GUID.oslab.opentlc.com` (node host, Region: Primary, Zone: East. )

* `node01-GUID.oslab.opentlc.com` (node host, Region: Primary, Zone: West. )

[NOTE]
As a reminder you will only be allowed to SSH to the administration host from the outside of the lab environment, all other hosts have external SSH blocked.  Once on the administration host, you can SSH to the other hosts internally.  As described earlier, you will have to use your private SSH key and OPENTLC login to access the system (not root!).

Each student lab is assigned a global unique identifier (GUID) that consists of 4 characters.  This GUID is provided to you in the provisioning email that will be sent to you when you provision your lab environment.  *Anywhere you see GUID from this point on, you will replace it with your lab's GUID.*

*In each lab step take special care to make sure that you are running the step on the required host.  Each step should contain the name of the host to run the step on and the example code should contain the host name in the shell prompt.*

* Administration host example:
+
----

[root@oselab-GUID ~]# command

----

* Master host example:
+
----

[root@master00-GUID ~]# command

----

== Connect to the Environment

. If not already connected, connect to your administration host `oselab-GUID.oslab.opentlc.com` using your OPENTLC login and private SSH key:
+
----

yourdesktop$ ssh -i ~/.ssh/id_rsa your-opentlc-login@oselab-GUID.oslab.opentlc.com

----

. SSH to the master host as the `root` user:
+
----

[yourlogin@oselab-GUID ~]$ ssh root@master00-GUID.oslab.opentlc.com

----
+
[NOTE]
If prompted for a password use *r3dh4t1!*
+
----

root@master00-GUID.oslab.opentlc.com's password: ******** (r3dh4t1!) 

----

== Lab: Configure Authentication

. Create a copy of your master's config file
+
----
[root@master00-GUID ~]# cp /etc/openshift/master/master-config.yaml /etc/openshift/master/master-config.yaml.original
----
. Edit your `/etc/openshift/master/master-config.yaml` so that the oauthConfig section look like this:
+
----
oauthConfig:
  assetPublicURL: https://master00-GUID.oslab.opentlc.com:8443/console/
  grantConfig:
    method: auto
  identityProviders:
  - name: htpasswd_auth
    challenge: true
    login: true
    provider:
      apiVersion: v1
      kind: HTPasswdPasswordIdentityProvider
      file: /etc/openshift/openshift-passwd
  masterPublicURL: https://master00-GUID.oslab.opentlc.com:8443
  masterURL: https://master00-GUID.oslab.opentlc.com:8443
  sessionConfig:
    sessionMaxAgeSeconds: 3600
    sessionName: ssn
    sessionSecretsFile:
  tokenConfig:
    accessTokenMaxAgeSeconds: 86400
    authorizeTokenMaxAgeSeconds: 500

----

=== Add Development Users

In the "real world" your developers would likely be using the OpenShift tools on
their own machines (`oc` and the web console). For this course, we
will create user accounts for two non-privileged users of OpenShift, *andrew* and
*marina*, on the master. This is done for convenience and because we'll be using
`htpasswd` for authentication.

. On the master host add two Linux accounts:
+
----

[root@master00-GUID ~]# useradd andrew
[root@master00-GUID ~]# useradd marina

----

=== Configuring htpasswd Authentication

OpenShift v3 supports a number of mechanisms for authentication. The simplest
use case for our testing purposes is `htpasswd`-based authentication.

To start, we will need the `htpasswd` binary available in the `httpd-tools` package.

. Install `httpd-tools` on the master host:
+
----

[root@master00-GUID ~]# yum -y install httpd-tools

----

. Create a password for our users, Joe and Alice on the master host:
+
----

[root@master00-GUID ~]# touch /etc/openshift/openshift-passwd
[root@master00-GUID ~]# htpasswd -b /etc/openshift/openshift-passwd andrew r3dh4t1!
[root@master00-GUID ~]# htpasswd -b /etc/openshift/openshift-passwd marina r3dh4t1!
								
----

. Restart `openshift-master` for changes to take effect
+
----
[root@master00-GUID ~]# systemctl restart openshift-master
----
          
== Lab: Users, Projects and Quotas
=== Create Projects

. On the master host use the `oadm` command to create a project, and assign an administrative user to it:
+
----

[root@master00-GUID ~]# oadm new-project resourcemanageme --display-name="Resources Management" \
    --description="This is the project we use to learn about resource management" \
    --admin=andrew

----

=== A look at resources in the Web Console 

Now that you have a project created, it's time to look at the web console, which
has been completely redesigned for V3.

. Open your desktop/laptop web browser and visit the following URL:
+
----

https://master00-GUID.oslab.opentlc.com:8443

----
+
[NOTE]
Be aware that it may take up to 90 seconds for the web console to be available
any time you restart the master.

. On your first visit your browser will need to accept the self-signed SSL
certificate.

. You will be asked for a username and a password. Remembering
that we created a user previously, `andrew`, go ahead and enter that and use
the password (`r3dh4t1!`) you set earlier.

. Once you are in, click the *"Resources Management"* project. There really isn't
anything of interest at the moment, because we haven't put anything into our
project.


=== Applying Quota to Projects

At this point we have created our "Resource Management" project, so let's apply the quota above
to it. 

. Create a Quota definition file 
+
----

[root@master00-GUID ~]# cat << EOF > quota.json
{
  "apiVersion": "v1",
  "kind": "ResourceQuota",
  "metadata": {
    "name": "test-quota"
  },
  "spec": {
    "hard": {
      "memory": "1Gi",
      "cpu": "20",
      "pods": "3",
      "services": "5",
      "replicationcontrollers":"5",
      "resourcequotas":"1"
    }
  }
}
EOF

----

. On the master host apply the file you just created with the `oc create` command:
+
----

[root@master00-GUID ~]# oc create -f quota.json --namespace=resourcemanageme

----

. On the master host make sure it was created:
+
----

[root@master00-GUID ~]# oc get -n resourcemanageme quota

----
+
----

NAME
test-quota

----

. On the master host verify limits and examine usage:
+
----

[root@master00-GUID ~]# oc describe quota test-quota -n resourcemanageme

----
+
----

Name:                   test-quota
Resource                Used    Hard
--------                ----    ----
cpu                     0       20
memory                  0       1Gi
pods                    0       3
replicationcontrollers  0       5
resourcequotas          1       1
services                0       5

----

. Go back into the web console and click into the "Resource Management"
project.

. Click on the *Settings* tab and you'll see that the quota information
is displayed.

[NOTE]
Once creating the quota, it can take a few moments for it to be fully
processed. If you get blank output from the `get` or `describe` commands, wait a
few moments and try again.

== Applying Limit Ranges to Projects

In order for quotas to be effective you need to also create Limit Ranges
which set the maximum, minimum, and default allocations of memory and cpu at
both a pod and container level. Without default values for containers projects
with quotas will fail because the deployer and other infrastructure pods are
unbounded and therefore forbidden.

. Create the Limits file 
+
----
[root@master00-GUID ~]# cat << EOF > limits.json
{
    "kind": "LimitRange",
    "apiVersion": "v1",
    "metadata": {
        "name": "limits",
        "creationTimestamp": null
    },
    "spec": {
        "limits": [
            {
                "type": "Pod",
                "max": {
                    "cpu": "500m",
                    "memory": "750Mi"
                },
                "min": {
                    "cpu": "10m",
                    "memory": "5Mi"
                }
            },
            {
                "type": "Container",
                "max": {
                    "cpu": "500m",
                    "memory": "750Mi"
                },
                "min": {
                    "cpu": "10m",
                    "memory": "5Mi"
                },
                "default": {
                    "cpu": "100m",
                    "memory": "100Mi"
                }
            }
        ]
    }
}
EOF


----

. On the master host run `oc create` against the `limits.json` file and the "resourcemanageme" project
+
----

[root@master00-GUID ~]# oc create -f limits.json --namespace=resourcemanageme

----

. Review your limit ranges on the master host:
+
----

[root@master00-GUID ~]# oc describe limitranges limits -n resourcemanageme

----
+
----

Name:           limits
Type            Resource        Min     Max     Default
----            --------        ---     ---     ---
Pod             memory          5Mi     750Mi   -
Pod             cpu             10m     500m    -
Container       cpu             10m     500m    100m
Container       memory          5Mi     750Mi   100Mi

----

=== Test your Quotas

.Authenticate to OpenShift Enterprise and Choose Your Project

. Connect to the OpenShift Enterprise master by following the same steps you used previously.
. Authenticate user `andrew` to Openshift Enterprise and create a token in the `.config/openshift/.config` file:
+
----

[root@master00 ~]# su - andrew
[andrew@master00 ~]$ guid=`hostname|cut -f2 -d-|cut -f1 -d.`
[andrew@master00 ~]$ oc login -u andrew --insecure-skip-tls-verify --server=https://master00-${guid}.oslab.opentlc.com:8443

----
+
You will See
+
----
Password: (Enter r3dh4t1!)
Login successful.
Welcome to OpenShift! See 'oc help' to get started.
----


.Create the Pod Definition

Run the following command to create the `hello-pod.json` file:

----

[andrew@master00 ~]$ cat <<EOF > hello-pod.json
{
  "kind": "Pod",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-openshift",
    "creationTimestamp": null,
    "labels": {
      "name": "hello-openshift"
    }
  },
  "spec": {
    "containers": [
      {
        "name": "hello-openshift",
        "image": "openshift/hello-openshift:v0.4.3",
        "ports": [
          {
            "hostPort": 36061,
            "containerPort": 8080,
            "protocol": "TCP"
          }
        ],
        "resources": {
          "limits": {
            "cpu": "10m",
            "memory": "16Mi"
          }
        },
        "terminationMessagePath": "/dev/termination-log",
        "imagePullPolicy": "IfNotPresent",
        "capabilities": {},
        "securityContext": {
          "capabilities": {},
          "privileged": false
        },
        "nodeSelector": {
          "region": "primary"
        }
      }
    ],
    "restartPolicy": "Always",
    "dnsPolicy": "ClusterFirst",
    "serviceAccount": ""
  },
  "status": {}
}
EOF

----

.Run the Pod

We will now create a simple pod without a *route* or a *service*
 
. Run the following commands to create and verify the pod:
+
----

[andrew@master00 ~]$ oc create -f hello-pod.json
pods/hello-openshift

[andrew@master00-3186 ~]$ oc get pods
NAME              READY     REASON    RESTARTS   AGE
hello-openshift   1/1       Running   0          2m

----

. Run the *oc describe* command to learn about your pod.
+
----
[andrew@master00-f4fc ~]$  oc describe pod hello-openshift
Name:                           hello-openshift
Image(s):                       openshift/hello-openshift:v0.4.3
Host:                           node01-f4fc.oslab.opentlc.com/192.168.0.201
Labels:                         name=hello-openshift
Status:                         Running
IP:                             10.1.1.2
Replication Controllers:        <none>
Containers:
  hello-openshift:
    Image:              openshift/hello-openshift:v0.4.3
    State:              Running
      Started:          Thu, 02 Jul 2015 02:42:50 -0400
    Ready:              True
    Restart Count:      0
Conditions:
  Type          Status
  Ready         True 
Events:
  .... "Successfully assigned hello-openshift to node01-f4fc.oslab.opentlc.com" .... 
 
----
+
. Test that your pod is responding with "Hello OpenShift"
+
----

[andrew@master00 ~]$ ip=`oc describe pod hello-openshift|grep IP:|awk '{print $2}'`
[andrew@master00 ~]$ curl http://${ip}:8080

----
+
You will see:
+
----
Hello OpenShift!
----

. Great, the pod works, Now, lets kill it and create a few moew 
+
----

[andrew@master00 ~]$ oc delete -f hello-pod.json 

----

. Create a new definition file that launches 4 hello-pods 
+
----
[andrew@master00 ~]$ cat << EOF > hello-many-pods.json
{
  "metadata":{
    "name":"quota-pod-deployment-test"
  },
  "kind":"List",
  "apiVersion":"v1",
  "items":[
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-1",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-2",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-3",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-4",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    }
  ]
}


EOF 

----

. Create the items in the *hello-many-pods.json* file
+
----
[andrew@master00 ~]$ oc create -f hello-many-pods.json
pods/hello-openshift-1
pods/hello-openshift-2
pods/hello-openshift-3
Error from server: Pod "hello-openshift-4" is forbidden: Limited to 3 pods
----

. Lets delete the objects and move on
+
----
[andrew@master00 ~]$ oc delete  -f hello-many-pods.json
----

. *Optional* - Using what you have learned, create a new project, and set the quota so that the pods value is 10 and run the *hello-many-pods.json* again.

== Lab: Creating Services and Routes

. As root on the master host create a new project:
+
----

[andrew@master00 ~]$ exit
[root@master00 ~]# oadm new-project svcslab --display-name="Services Lab" \
    --description="This is the project we use to learn about services" \
    --admin=andrew
----

. Become the *andrew* user and log back into OpenShift and switch to the *svcslab* project:
+
----

[root@master00 ~]# su - andrew
[andrew@master00 ~]$ guid=`hostname|cut -f2 -d-|cut -f1 -d.`
[andrew@master00 ~]$ oc login -u andrew --insecure-skip-tls-verify --server=https://master00-${guid}.oslab.opentlc.com:8443
...
[andrew@master00 ~]$ oc project svcslab
Now using project "svcslab" on server "https://master00-GUID.oslab.opentlc.com:8443".

----

. Run the following command to create the `hello-service.json` file:
+
----

[andrew@master00 ~]$  cat <<EOF > hello-service.json
{
  "kind": "Service",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-service"
  },
  "spec": {
    "selector": {
      "name":"hello-openshift"
    },
    "ports": [
      {
        "protocol": "TCP",
        "port": 8888,
        "targetPort": 8080
      }
    ]
  }
}
EOF

----
+
. Run the following commands to create and verify the pod:
+
----

[andrew@master00 ~]$ oc create -f hello-service.json
services/hello-service

----
+
. Display the running services (under the current project)
+
----

[andrew@master00 ~]$ oc get services
NAME            LABELS    SELECTOR               IP(S)          PORT(S)
hello-service   <none>    name=hello-openshift   172.30.xxx.yyy   8888/TCP

----
+
. Lets look at the details of our service, Please notice the *selector* and the *Endpoints* lines.
.. The *selector* describes which pods should be "selected" or "listed" by the service.
.. The *Endpoints* line lists all the pods that are currently listed, notice that we have none.   
+
----
[andrew@master00 ~]$ oc describe service hello-service
Name:                   hello-service
Labels:                 <none>
Selector:               name=hello-openshift
Type:                   ClusterIP
IP:                     172.30.xxx.yyy
Port:                   <unnamed>       8888/TCP
Endpoints:              <none>
Session Affinity:       None
No events.
----

. Lets create some pods 
----

[andrew@master00 ~]$ oc create -f hello-many-pods.json

----

. Now lets check the service again, you can see that the pods who share the label "name=hello-service" are all listed.
+
----

[andrew@master00 ~]$ oc get service
NAME            LABELS    SELECTOR               IP(S)          PORT(S)
hello-service   <none>    name=hello-openshift   172.30.5.240   8888/TCP

[andrew@master00 ~]$ oc describe service hello-service                                                                                                                                                
Name:                   hello-service
Labels:                 <none>
Selector:               name=hello-openshift
Type:                   ClusterIP
IP:                     172.30.5.240
Port:                   <unnamed>       8888/TCP
Endpoints:              10.1.0.4:8080,10.1.1.5:8080,10.1.1.7:8080
Session Affinity:       None
No events.

----

. Lets test our service 
+
----

[andrew@master00 ~]$ ip=`oc describe service hello-service|grep IP:|awk '{print $2}'`
[andrew@master00 ~]$ curl http://${ip}:8888
Hello OpenShift!

----

. Create the Router 
+
----
[andrew@master00 ~]$ guid=`hostname|cut -f2 -d-|cut -f1 -d.`
[andrew@master00 ~]$ oc expose service/hello-service --hostname=hello2-openshift.cloudapps-${guid}.oslab.opentlc.com
----

//// NO LONGER NEEDED
//// . Create a *route* for our service
//// +
//// ----
//// [andrew@master00-6b80 ~]$ oc create -f hello-route.json
//// routes/hello-openshift-route
//// ----

. Lets see our routes 
+
----
[andrew@master00-6b80 ~]$ oc get routes
NAME                    HOST/PORT                                          PATH      SERVICE                   LABELS
hello-openshift-route   hello-openshift.cloudapps-6b80.oslab.opentlc.com             hello-openshift-service   
----

. Test Route:
+
----

[andrew@master00 ~]$ curl http://hello2-openshift.cloudapps-${guid}.oslab.opentlc.com
Hello OpenShift!

----


