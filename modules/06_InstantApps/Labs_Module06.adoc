:icons: images/icons
:toc2:

:numbered:

== Lab	: Installing a Template

=== Templates, Instant Apps, and "Quickstarts"

This example will involve a build of another application, but also a service
that has two pods -- a "front-end" web tier and a "back-end" database tier. This
application also makes use of auto-generated parameters and other neat features
of OpenShift. One thing of note is that this project already has the
wiring/plumbing between the front- and back-end components pre-defined as part
of its JSON and embedded in the source code. Adding resources "after the fact"
will come in a later lab.

This example is effectively a "quickstart" -- a pre-defined application that
comes in a template that you can just fire up and start using or hacking on.

=== A Project for the Quickstart

. On the master host as `root`, create a new project:
+
----

[root@master00-GUID ~]$ oadm new-project quickstart --display-name="Quickstart" \
    --description='A demonstration of a "quickstart/template"' \
    --node-selector='region=primary' --admin=andrew
----

=== A Quick Aside on Templates

From the [OpenShift
documentation](http://docs.openshift.org/latest/dev_guide/templates.html):

    A template describes a set of resources intended to be used together that
    can be customized and processed to produce a configuration. Each template
    can define a list of parameters that can be modified for consumption by
    containers.

As we mentioned previously, this template has some auto-generated parameters.
For example, take a look at the following JSON:

    "parameters": [
      {
        "name": "ADMIN_USERNAME",
        "description": "administrator username",
        "generate": "expression",
        "from": "admin[A-Z0-9]{3}"
      },

This portion of the template's JSON tells OpenShift to generate an expression
using a regex-like string that will be presented as ADMIN_USERNAME.

=== Adding the Template

. On the master host as `root`, get and create the Example template:
+
----

[root@master00-GUID ~]# wget http://www.opentlc.com/download/ose_implementation/resources/Template_Example.json
[root@master00-GUID ~]# oc create -f Template_Example.json -n openshift

----

NOTE: What did you just do? The `Template_Example.json` file defined a template.
 By "creating" it, you have added it to the `openshift` project. if we wanted
to make the template available for only limited projects we would add it to them
and not the openshift project.

=== Create an Instance of the Template

. In the web console, log in as `andrew`.

. Find the "Quickstart" project, and then click the "Create..." button.
+
We've seen this page before, but now it contains something new -- an "instant app(lication)". An instant app is a "special" kind of template (relaly, it just has the "instant-app" tag). The idea behind an
"instant app" is that, when creating an instance of the template, you will have
a fully functional application. in this example, our "instant" app is just a
simple key-value storage and retrieval webpage.

. Click "quickstart-keyvalue-application", and you'll see a modal pop-up that
provides more information about the template.

. Click "Select template..."
+
The next page that you will see is the template "configuration" page. This is
where you can specify certain options for how the application components will be
insantiated.
+
It will:
+
* Show you what Docker images are used

* Let you add label:value pairs that can be used for other things

* Let you set specific values for any parameters, if you so choose

. Leave all of the defaults and simply click "Create".
+
Once you hit the "Create" button, the services and pods and
replicationcontrollers etc. will be instantiated.
+
The cool thing about the template is that it has a built-in route. The not so
cool thing is that route is not configurable at the moment. But, it's there!

. Click "Browse" and then "Services" you will see that there is a route for
the *frontend* service:
+
----

`frontend.quickstart.router.default.local`

----
+
The build was started for us immediately after creating an instance of the
template, so you can wait for it to finish. Feel free to check the build logs.

. Once the build is complete edit the router configuration:
+
----
[andrew@master00-GUID ~]$ oc project quickstart
[andrew@master00-GUID ~]$ oc edit route example-route
----

. Update the host: line to have the correct hostname:
+
----
  host: integrated.cloudapps-GUID.oslab.opentlc.com
----

=== Using Your App

Once the app is built, you should be able to visit the routed URL and
actually use the application!

    http://integrated.cloudapps-GUID.oslab.opentlc.com

[NOTE]
HTTPS will *not* work for this example because the form submission was
written with HTTP links. Be sure to use HTTP.

== Lab: Wiring Templates together

Quickstarts are great, but sometimes a developer wants to build up the various
components manually. Let's take our quickstart example and treat it like two
separate "applications" that we want to wire together.

=== Create a New Project

. On the *master* host as `root`, create a new project, *wiring*:
+
----

[root@master00-GUID ~]# oadm new-project wiring --display-name='Wiring' \
    --description='A demonstration of wiring components together' \
    --node-selector='region=primary' --admin=marina

----

. Authenticate user `marina` to Openshift Enterprise:
+
----

[root@master00~]# su - marina
[marina@master00~]$ oc login -u marina --insecure-skip-tls-verify --server=https://master00-${GUID}.oslab.opentlc.com:8443

----
+
You will See
+
----
Password: (Enter r3dh4t1!)
Login successful.
Welcome to OpenShift! See 'oc help' to get started.
----

. Log into the web console as `marina`. Can you see `andrew`'s projects and content?
+
NOTE: Of course you dont.

=== Stand Up the Frontend

The first step will be to stand up the frontend of our application. For
argument's sake, this could have just as easily been brand new vanilla code.
However, to make things faster, we'll start with an application that already is
looking for a DB, but won't fail spectacularly if one isn't found.

. Create a new app using the *https://github.com/openshift/ruby-hello-world* Git repository:
+
----
[marina@master00-GUID ~]$ oc new-app -i openshift/ruby https://github.com/openshift/ruby-hello-world#beta4
----

.. You should see something like
+
----
I0709 05:09:45.198010    9706 newapp.go:301] Image "openshift/ruby" is a builder, so a repository will be expected unless you also specify --strategy=docker
I0709 05:09:45.198822    9706 newapp.go:337] Using "https://github.com/openshift/ruby-hello-world" as the source for build
imagestreams/ruby-hello-world
buildconfigs/ruby-hello-world
deploymentconfigs/ruby-hello-world
services/ruby-hello-world
A build was created - you can run `oc start-build ruby-hello-world` to start it.
Service "ruby-hello-world" created at 172.30.96.14 with port mappings 8080.
----

. Before your build starts, lets look at the *BuildConfig* that was created and the *DeploymentConfig*
+
----
[marina@master00-GUID ~]$ oc get builds # if you see nothing, it's because the build isn't running yet.
NAME      TYPE      STATUS    POD
[marina@master00-GUID ~]$ oc get buildconfig
NAME               TYPE      SOURCE
ruby-hello-world   Source    https://github.com/openshift/ruby-hello-world
[marina@master00-GUID ~]$ oc get dc
NAME               TRIGGERS                    LATEST VERSION
ruby-hello-world   ConfigChange, ImageChange   1
----

. Since we know that we want to talk to a database eventually, let's take a moment to add the environment variables for it. Conveniently, there is an env subcommand to oc. As marina, we can use it like so:
+
----
[marina@master00-GUID ~]$ oc env dc/ruby-hello-world MYSQL_USER=root MYSQL_PASSWORD=redhat MYSQL_DATABASE=mydb
----

. If you want to double-check, you can verify using the following:
+
----
[marina@master00-GUID ~]$ oc env dc/ruby-hello-world --list
# deploymentconfigs ruby-hello-world, container ruby-hello-world
MYSQL_USER=root
MYSQL_PASSWORD=redhat
MYSQL_DATABASE=mydb
----

. Notice that your build might have already started before you changed the *DeploymentConfig* environment variables, this would trigger another deployment to start.
. Expose the *ruby-hello-world* Service
+
----
[marina@master00-GUID ~]$ oc expose service --name=frontend-route ruby-hello-world --hostname="frontwire.wiring.cloudapps-$guid.oslab.opentlc.com"
----

. Check that your route was created
+
----
[marina@master00-GUID ~]$ oc get route
NAME               HOST/PORT                                       PATH      SERVICE            LABELS
ruby-hello-world   frontwire.wiring.cloudapps-r2d2.oslab.opentlc.com             ruby-hello-world
----

. Now you should be able to access your application with your browser, Go ahead and do that now. link:http://frontwire.wiring.cloudapps-GUID.oslab.opentlc.com[http://frontwire.wiring.cloudapps-GUID.oslab.opentlc.com]
. Earlier we added a template to the openshift namespace to make it available for all users. Now we'll demonstrate adding a template to our own project.
+
----
[marina@master00-GUID ~]$ wget http://www.opentlc.com/download/ose_implementation/resources/mysql_template.json
[marina@master00-GUID ~]$ oc create -f mysql_template.json
----

. You'll see:
+
----
templates/mysql-ephemeral
----

. Create the Database From the Web Console
.. Go to the web console and make sure you are logged in as marina and using the Wirinig project. You should see your front-end already there.
.. Click the "Create..." button and then the "Browse all templates..." button.
.. You should see the mysql-ephemeral template. Click it and then click "Select template".
. You will need to edit the parameters of this template, because the defaults will not work for us.
.. Change the DATABASE_SERVICE_NAME to be "*database*", because that is what service the frontend expects to connect to.
.. Change the MYSQL_USER to be "*root*", because that is what mysql user that the frontend will use.
.. Change the MYSQL_PASSWORD to be "*redhat*", because the password the frontend will use.
.. Change the MYSQL_DATABASE to be "*mydb*", because that is what database the frontend expects to connect to.

.. Make sure that the MySQL user, password and database match whatever values you specified in the previous steps.
. Click the "Create" button when you are ready.

. It may take a little while for the MySQL container to download (if you didn't pre-fetch it). It's a good idea to verify that the database is running before continuing. If you don't happen to have a MySQL client installed you can still verify MySQL is running with curl:
+
----
[marina@master00-GUID ~]$ curl `oc get services | grep mysql | awk '{print $4}'`:3306
----

. MySQL doesn't speak HTTP so you will see garbled output like this (however, you'll know your database is running!):
+
----
5.6.2K\l-7mA<��F/T:emsy'TR~mysql_native_password!��#08S01Got packets out of order
----

. Lets see on which nodes our pods are hosted
+
----
[marina@master00-GUID ~]$ oc get pod -t '{{range .items}}{{.metadata.name}} {{.spec.host}}{{"\n"}}{{end}}'
mysql-3-4rk55 node00-GUID.oslab.opentlc.com
ruby-hello-world-2-build node01-GUID.oslab.opentlc.com
ruby-hello-world-5-9doo2 node01-GUID.oslab.opentlc.com
----

. As *root* connect to the node where the pod is running, and find the Docker container id
+
----
[root@node0X-GUID ~] docker inspect `docker ps | grep hello-world | grep run | awk '{print $1}'` | grep -i mysql
            "MYSQL_USER=root",
            "MYSQL_PASSWORD=redhat",
            "MYSQL_DATABASE=database",
            "MYSQL_PORT=tcp://172.30.168.254:3306",
            "MYSQL_PORT_3306_TCP=tcp://172.30.168.254:3306",
            "MYSQL_SERVICE_PORT=3306",
            "MYSQL_SERVICE_PORT_MYSQL=3306",
            "MYSQL_PORT_3306_TCP_PROTO=tcp",
            "MYSQL_PORT_3306_TCP_PORT=3306",
            "MYSQL_PORT_3306_TCP_ADDR=172.30.168.254",
            "MYSQL_SERVICE_HOST=172.30.168.254",
----

. Visit Your Application Again, link:http://frontwire.wiring.cloudapps-GUID.oslab.opentlc.com[http://frontwire.wiring.cloudapps-GUID.oslab.opentlc.com]

NOTE:
Why does it still say that there is no database?
When the frontend was first built and created, there was no service called "database", so the environment variable DATABASE_SERVICE_HOST did not get populated with any values.
Our database does exist now, and there is a service for it, but OpenShift could not "inject" those values into the frontend container.


=== Replication Controllers

The easiest way to get this going? Just nuke the existing pod.


. We need to kill our front-end pods so they retry the database
+
----
[marina@master00-GUID ~]$ oc delete pods -l deploymentconfig=ruby-hello-world
----

. Wait a few seconds and see that a new pod was created, thanks to our trusty *Replication Controller*
. Get the replication controller that is running for both the frontend and backend:
+
----

[marina@master00-GUID ~]$ oc get replicationcontroller # or "oc get rc"

----

. The replication controller is configured to ensure that we always have the
desired number of replicas (instances) running. We can look at how many that
should be:
+
----

[marina@master00-GUID ~]$ oc describe rc ruby-hello-world-1

----
+
So, if we kill the pod, the RC will detect that, and fire it back up. When it
gets fired up this time, it will then have the `DATABASE_SERVICE_HOST` value,
which means it will be able to connect to the DB, which means that we should no
longer see the database error!

. As `marina`, Lets try another way to kill the pod
+
----

[marina@master00-GUID ~]$ oc delete pod `oc get pod | grep -e "hello-world-[0-9]" | grep -v build | awk '{print $1}'`

----
+
You'll see something like:
+
----

pods/ruby-hello-world-1-wcxiw

----
+
 That was the generated name of the pod when the replication controller stood it
up the first time. You also see some deployment hook pods. We will talk about
deployment hooks a bit later.

. After a few moments, we can look at the list of pods again:
+
----

[marina@master00-GUID ~]$ oc get pod | grep world

----
+
We should see a different name for the pod this time:
+
----

ruby-hello-world-1-4ikbl

----
+
This shows that, underneath the covers, the RC restarted our pod. Since it was
restarted, it should have a value for the `DATABASE_SERVICE_HOST` environment
variable.

. Go to the *node* where the *pod is running*, and find the Docker container
id as `root`:
+
----

[marina@node0X-GUID ~]$ docker inspect `docker ps | grep hello-world | grep run | awk \
   '{print $1}'` | grep mysql

----
+
The output will look something like:
+
----

"MYSQL_DATABASE=mydb",
"DATABASE_SERVICE_PORT_MYSQL=3306",
"DATABASE_SERVICE_PORT=3306",
"DATABASE_PORT=tcp://172.30.249.174:3306",
"DATABASE_PORT_3306_TCP=tcp://172.30.249.174:3306",
"DATABASE_PORT_3306_TCP_PROTO=tcp",
"DATABASE_SERVICE_HOST=172.30.249.174",
"DATABASE_PORT_3306_TCP_PORT=3306",
"DATABASE_PORT_3306_TCP_ADDR=172.30.249.174",

----

=== Revisit the Webpage

Go ahead and revisit `http://frontwire.wiring.cloudapps-GUID.oslab.opentlc.com` in your browser, and you should see that the application is now fully
functional!
