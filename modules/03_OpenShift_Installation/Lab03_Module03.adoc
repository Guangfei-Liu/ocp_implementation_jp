:scrollbar:
:data-uri:
:icons: images/icons
:toc2:	

=== Connect to the Environment

. If not already connected, connect to your administration host `oselab-GUID.oslab.opentlc.com` using your OPENTLC login and private SSH key:
+
----

yourdesktop$ ssh -i ~/.ssh/id_rsa your-opentlc-login@oselab-GUID.oslab.opentlc.com

. SSH to the master host as the `root` user:
+
----

[yourlogin@oselab-GUID ~]$ ssh root@master00-GUID.oslab.opentlc.com

----
+
[NOTE]
If prompted for a password use *r3dh4t1!*
+
----

root@master00-GUID.oslab.opentlc.com's password: ******** (r3dh4t1!) 

----

### Scheduler and Defaults

The "scheduler" is essentially the OpenShift master. Any time a pod needs to be
created (instantiated) somewhere, the master needs to figure out where to do
this. This is called "scheduling". The default configuration for the scheduler
looks like the following JSON (although this is embedded in the OpenShift code
and you won't find this in a file):

    {
      "predicates" : [
        {"name" : "PodFitsResources"},
        {"name" : "MatchNodeSelector"},
        {"name" : "HostName"},
        {"name" : "PodFitsPorts"},
        {"name" : "NoDiskConflict"}
      ],"priorities" : [
        {"name" : "LeastRequestedPriority", "weight" : 1},
        {"name" : "ServiceSpreadingPriority", "weight" : 1}
      ]
    }

When the scheduler tries to make a decision about pod placement, first it goes
through "predicates", which essentially filter out the possible nodes we can
choose. Note that, depending on your predicate configuration, you might end up
with no possible nodes to choose. This is totally OK (although generally not
desired).

These default options are documented in the JSON code above.  Here is a quick overview of what it means:

* *PodFitsResources* - Place pod on a node that has enough resources for it

* *PodFitsPorts* - Place pod on a node that doesn't have a port conflict

* *NoDiskConflict* - Place pod on a node that doesn't have a storage conflict

And some more obscure ones:

* *MatchNodeSelector* - Place pod on a node whose `NodeSelector` matches

* *HostName* - Place pod on a node whose hostname matches the `Host` attribute value

The next thing is, of the available nodes after the filters are applied, how do
we select the "best" one. This is where "priorities" come in. Long story short,
the various priority functions each get a score, multiplied by the weight, and
the node with the highest score is selected to host the pod.

The defaults are:

* Choose the node that is "least requested" (the least busy)

* Spread services around - minimize the number of pods in the same service on the same node

[NOTE]
For an extremely detailed explanation about what these various
configuration flags are doing, check out: http://docs.openshift.org/latest/admin_guide/scheduler.html

In a small environment, the defaults are pretty sane. Let's look at one of the
important predicates (filters) before we move on to "regions" and "zones".

### The NodeSelector
`NodeSelector` is a part of the Pod data model. And, if we think back to our pod
definition, there was a "label", which is just a key:value pair. In the case of
a `NodeSelector`, our labels (key:value pairs) are used to help us try to find
nodes that match, assuming that:

* The scheduler is configured to *MatchNodeSelector*

* The end user creating the pod knows which labels are out there

This use case is pretty simplistic.  It doesn't really allow for a
topology, and there's not a lot of logic behind it. For instance, if you specify a
*NodeSelector* label when using *MatchNodeSelector* and there are no matching nodes,
my workload will *never* get scheduled.

We can make this more intelligent by using "regions" and "zones".

### Customizing the Scheduler Configuration

The Ansible installer is configured to understand "regions" and "zones" as a
matter of convenience. However, for the master (scheduler) to actually do
something with them requires changing from the default configuration.

. On the master host take a look at `/etc/openshift/master/master-config.yaml` and find the line with `schedulerConfigFile`.
+
----

[root@master00-GUID ~]# less /etc/openshift/master/master-config.yaml

----
+
You should see:
+
----

    schedulerConfigFile: "/etc/openshift/master/scheduler.json"

----

. On the master host take a look at `/etc/openshift/master/scheduler.json`:

----

[root@master00-GUID ~]# less /etc/openshift/master/scheduler.json

----
+
You should see:
+
----

    {
      "predicates" : [
        {"name" : "PodFitsResources"},
        {"name" : "PodFitsPorts"},
        {"name" : "NoDiskConflict"},
        {"name" : "Region", "argument" : {"serviceAffinity" : { "labels" : ["region"]}}}
      ],"priorities" : [
        {"name" : "LeastRequestedPriority", "weight" : 1},
        {"name" : "ServiceSpreadingPriority", "weight" : 1},
        {"name" : "Zone", "weight" : 2, "argument" : {"serviceAntiAffinity" : { "label" : "zone" }}}
      ]
    }

----

To quickly review the JSON code above:

* Filter out nodes that don't fit the resources, don't have the ports, or have disk conflicts

* If the pod specifies a label with the key "region", filter nodes by the value.

So, if we have the following nodes and the following labels:

* Node 1 -- "region":"infra"

* Node 2 -- "region":"primary"

* Node 3 -- "region":"primary"

If we try to schedule a pod that has a `NodeSelector` of "region":"primary",
then only Node 1 and Node 2 would be considered.

That takes care of the "region" part.  What about the "zone" part?

Our priorities tell us to:

* Score the least-busy node higher

* Score any nodes who don't already have a pod in this service higher

* Score any nodes whose zone label's value **does not** match higher

Why do we score a zone that **doesn't** match higher? Note that the definition
for the Zone priority is a `serviceAntiAffinity` -- anti affinity. In this case,
our anti affinity rule helps to ensure that we try to get nodes from *different*
zones to take our pod.

If we consider that our "primary" region might be a certain datacenter, and that
each "zone" in that datacenter might be on its own power system with its own
dedicated networking, this would ensure that, within the datacenter, pods of an
application would be spread across power/network segments.

The documentation link provided earlier has some more complicated examples. The topoligical
possibilities are endless!

### Node Labels

The assignments of "regions" and "zones" at the node-level are handled by labels
on the nodes. 

. On the master host look at how the labels were implemented with `osc get nodes`:
+
----

[root@master00-GUID ~]# osc get nodes

----
+
You should see:
+
----

NAME                              LABELS                                                                             STATUS
master00-GUID.oslab.opentlc.com   kubernetes.io/hostname=master00-GUID.oslab.opentlc.com,region=infra,zone=default   Ready
node00-GUID.oslab.opentlc.com     kubernetes.io/hostname=node00-GUID.oslab.opentlc.com,region=primary,zone=east      Ready
node01-GUID.oslab.opentlc.com     kubernetes.io/hostname=node01-GUID.oslab.opentlc.com,region=primary,zone=west      Ready

----

At this point we have a running OpenShift environment across three hosts, with
one master and three nodes, divided up into two regions -- "infrastructure"
and "primary".

From here we will start to deploy "applications" and other resources into
OpenShift.

### Useful OpenShift Logs

RHEL 7 uses `systemd` and `journal`. As such, looking at logs is not a matter of
`/var/log/messages` any longer. You will need to use `journalctl`.

Since we are running all of the components in higher loglevels, it is suggested
that you use your terminal emulator to set up windows for each process.

On the master host you should run each of the following in its own
window:

----

[root@master00-GUID ~]# journalctl -f -u openshift-master
[root@master00-GUID ~]# journalctl -f -u openshift-node

----

[NOTE]
You will want to do this on the other nodes, but you won't need the
`openshift-master` service. You may also wish to watch the Docker logs, too.
