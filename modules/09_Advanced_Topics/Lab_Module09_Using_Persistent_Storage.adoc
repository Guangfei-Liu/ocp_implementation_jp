== Using Persistent Storage (Optional)

Having a database for development is nice, but what if you actually want the
data you store to stick around after the DB pod is redeployed? Pods are
ephemeral, and so is their storage by default. For shared or persistent
storage, we need a way to specify that pods should use external volumes.

We can do this a number of ways. [Kubernetes provides methods for directly
specifying the mounting of several different volume
types.](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/user-guide/volumes.md)
This is perfect if you want to use known external resources. But that's
not very PaaS-y. If I'm using a PaaS, I might really just rather request a
chunk of storage and not need a side channel to provision that. OpenShift 3
provides a mechanism for doing just this.

=== Export an NFS Volume

For the purposes of this training, we will just demonstrate the master
exporting an NFS volume for use as storage by the database. **You would
almost certainly not want to do this in production.** If you happen
to have another host with an NFS export handy, feel free to substitute
that instead of the master.

. As `root` on the master host ensure that nfs-utils is installed on the nodes:
+
----

[root@master00-GUID ~]# ]# for node in infranode00-$guid.oslab.opentlc.com node00-$guid.oslab.opentlc.com node01-$guid.oslab.opentlc.com; do yum -y install nfs-utils ; done

----

. On the **oselab** admin host we will create a directory that we will export via NFS:
+
----

[root@oselab-GUID ~]# mkdir -p /var/export/vol1
[root@oselab-GUID ~]# chown nfsnobody:nfsnobody /var/export/vol1
[root@oselab-GUID ~]# chmod 700 /var/export/vol1

----

. Add the following line to `/etc/exports`:
+
----

[root@oselab-GUID ~]# echo "/var/export/vol1 *(rw,sync,all_squash)" >> /etc/exports

----

. Enable and start NFS services:
+
----

[root@oselab-GUID ~]# systemctl enable rpcbind nfs-server
[root@oselab-GUID ~]# systemctl start rpcbind nfs-server nfs-lock nfs-idmap
[root@oselab-GUID ~]# systemctl stop iptables firewalld
[root@oselab-GUID ~]# systemctl disable iptables firewalld

----
+
Note that the volume is owned by `nfsnobody` and access by all remote users
is "squashed" to be access by this user. This essentially disables user
permissions for clients mounting the volume. While another configuration
might be preferable, one problem you may run into is that the container
cannot modify the permissions of the actual volume directory when mounted.
In the case of MySQL below, MySQL would like to have the volume belong to
the `mysql` user, and assumes that it is, which causes problems later.
Arguably, the container should operate differently. In the long run, we
probably need to come up with best practices for use of NFS from containers.

=== NFS Firewall (Info only, do not do the steps here!)

**In our lab environment, the firewall is disabled on the oselab host, so these steps are not necessary.**

We will need to open ports on the firewall on the master to enable NFS to
communicate from the nodes. First, let's add rules for NFS to the running state
of the firewall:

    iptables -I OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 111 -j ACCEPT
    iptables -I OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2049 -j ACCEPT
    iptables -I OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 20048 -j ACCEPT
    iptables -I OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 50825 -j ACCEPT
    iptables -I OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 53248 -j ACCEPT

Next, let's add the rules to `/etc/sysconfig/iptables`. Put them at the top of
the `OS_FIREWALL_ALLOW` set:

    -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 53248 -j ACCEPT
    -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 50825 -j ACCEPT
    -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 20048 -j ACCEPT
    -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2049 -j ACCEPT
    -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 111 -j ACCEPT

Now, we have to edit NFS' configuration to use these ports. First, let's edit
`/etc/sysconfig/nfs`. Change the RPC option to the following:

    RPCMOUNTDOPTS="-p 20048"

Change the STATD option to the following:

    STATDARG="-p 50825"

Then, edit `/etc/sysctl.conf`:

    fs.nfs.nlm_tcpport=53248
    fs.nfs.nlm_udpport=53248

Then, persist the `sysctl` changes:

    sysctl -p

Lastly, restart NFS:

    systemctl restart nfs

=== Allow NFS Access in SELinux Policy

. By default policy, containers are not allowed to write to NFS mounted
directories.  We want to do just that with our database, so enable that on
all nodes where the pod could land (i.e. all of them) with:
+
----

[root@node0X-GUID ~]# setsebool -P virt_use_nfs=true

----
+
[NOTE]
Once the ansible-based installer does this automatically, we can remove this
section from the document.

=== Create a PersistentVolume

It is the PaaS administrator's responsibility to define the storage that is
available to users. Storage is represented by a PersistentVolume that
encapsulates the details of a particular volume which can be backed by any
of the [volume types available via
Kubernetes](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/volumes.md).
In this case it will be our NFS volume.

Currently PersistentVolume objects must be created "by hand". Modify the
`beta4/persistent-volume.json` file as needed if you are using a different
NFS mount:

    {
      "apiVersion": "v1",
      "kind": "PersistentVolume",
      "metadata": {
        "name": "pv0001"
      },
      "spec": {
        "capacity": {
            "storage": "5Gi"
            },
        "accessModes": [ "ReadWriteMany" ],
        "nfs": {
            "path": "/var/export/vol1",
            "server": "ose3-master.example.com"
        }
      }
    }

. Create this object as the `root` (administrative) user on the master host:
+
----

[root@master00-GUID ~]# GUID=`hostname|cut -f2 -d-|cut -f1 -d.`
[root@master00-GUID ~]# wget https://raw.githubusercontent.com/openshift/training/master/beta4/persistent-volume.json
[root@master00-GUID ~]# sed -i "s/ose3-master.example.com/oselab-$GUID.oslab.opentlc.com/" persistent-volume.json
[root@master00-GUID ~]# oc create -f persistent-volume.json

----
+
----

persistentvolumes/pv0001

----
+
This defines a volume for OpenShift projects to use in deployments. The
storage should correspond to how much is actually available (make each
volume a separate filesystem if you want to enforce this limit).

. Take a look at it the voume:
+
----

[root@master00-GUID ~]# oc describe persistentvolumes/pv0001

----
+
----

Name:   pv0001
Labels: <none>
Status: Available
Claim:

----

=== Claim the PersistentVolume

Now that the administrator has provided a PersistentVolume, any project can
make a claim on that storage. We do this by creating a PersistentVolumeClaim
that specifies what kind and how much storage is desired:

    {
      "apiVersion": "v1",
      "kind": "PersistentVolumeClaim",
      "metadata": {
        "name": "claim1"
      },
      "spec": {
        "accessModes": [ "ReadWriteMany" ],
        "resources": {
          "requests": {
            "storage": "5Gi"
          }
        }
      }
    }

. Have `marina` do this in the `wiring` project:
+
----

[marina@master00-GUID ~]$ wget https://raw.githubusercontent.com/openshift/training/master/beta4/persistent-volume-claim.json
[marina@master00-GUID ~]$ oc create -f persistent-volume-claim.json

----
+
----

persistentVolumeClaim/claim1

----

. This claim will be bound to a suitable PersistentVolume (one that is big
enough and allows the requested accessModes). The user does not have any
real visibility into PersistentVolumes, including whether the backing
storage is NFS or something else; they simply know when their claim has
been filled ("bound" to a PersistentVolume).
+
----

[marina@master00-GUID ~]$ oc get pvc

----
+
----

NAME      LABELS    STATUS    VOLUME
claim1    map[]     Bound     pv0001

----

. As `root` we now go back and look at our PV, we will also see that it has been claimed:
+
----

[root@master00-GUID ~]# oc describe pv/pv0001

----
+
----

Name:   pv0001
Labels: <none>
Status: Bound
Claim:  wiring/claim1

----

The PersistentVolume is now claimed and can't be claimed by any other project.

Although this flow assumes the administrator pre-creates volumes in
anticipation of their use later, it would be possible to create an external
process that watches the API for a PersistentVolumeClaim to be created,
dynamically provisions a corresponding volume, and creates the API object
to fulfill the claim.

=== Use the Claimed Volume

. Finally, we need to modify our `database` DeploymentConfig to specify that
this volume should be mounted where the database will use it. As `marina`:
+
----

[marina@master00-GUID ~]$ oc edit dc/database

----

. The part we will need to edit is the pod template. We will need to add two
parts:
+
* a definition of the volume

* where to mount it inside the container

First, directly under the `template` `spec:` line, add this YAML (indented from the `spec:` line):

          volumes:
          - name: pvol
            persistentVolumeClaim:
              claimName: claim1

Then to have the container mount this, add this YAML after the
`terminationMessagePath:` line:

            volumeMounts:
            - mountPath: /var/lib/mysql/data
              name: pvol

Remember that YAML is sensitive to indentation. The final template should
look like this:

    template:
      metadata:
        creationTimestamp: null
        labels:
          deploymentconfig: database
      spec:
        volumes:
        - name: pvol
          persistentVolumeClaim:
            claimName: claim1
        containers:
        - capabilities: {}
    [...]
          terminationMessagePath: /dev/termination-log
          volumeMounts:
          - mountPath: /var/lib/mysql/data
            name: pvol
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        serviceAccount: ""

Save and exit. This change to configuration will trigger a new deployment
of the database, and this time, it will be using the NFS volume we exported
from master.

=== Restart the Frontend

Any values or data we had inserted previously just got blown away. The
`deploymentConfig` update caused a new MySQL pod to be launched. Since this is
the first time the pod was launched with persistent data, any previous data was
lost.

Additionally, the Frontend pod will perform a database initialization when it
starts up. Since we haven't restarted the frontend, our database is actually
bare. If you try to use the app now, you'll get "Internal Server Error".

. Kill the Frontend pod like we did previously to cause it to
restart:
+
----

[marina@master00-GUID ~]$ oc delete pod `oc get pod | grep front | awk {'print $1'}`

----

. Once the new pod has started, go ahead and visit the web page.

. Add a few values via the application.

. Delete the database pod and wait for it to come back.
You should be able to retrieve the same values you entered.
+
To quickly delete the Database pod you can do the following:
+

----

[marina@master00-GUID ~]$ oc delete pod/`oc get pod | grep -e "database-[0-9]" | awk {'print $1'}`

----

[NOTE]
This doesn't seem to work right now, but we're not sure why. I think
it has to do with Ruby's persistent connection to the MySQL service not going
away gracefully, or something. Killing the frontend again will definitely work.

. For further confirmation that your database pod is in fact using the NFS
volume, simply check what is stored there on `oselab`:
+
----

[marina@oselab-GUID ~]$ ls /var/export/vol1

----
+
----

database-3-n1i2t.pid  ibdata1  ib_logfile0  ib_logfile1  mysql  performance_schema  root

----

Further information on use of PersistentVolumes is available in the
[OpenShift Origin documentation](http://docs.openshift.org/latest/dev_guide/volumes.html).
This is a very new feature, so it is very manual for now, but look for more tooling
taking advantage of PersistentVolumes to be created in the future.
