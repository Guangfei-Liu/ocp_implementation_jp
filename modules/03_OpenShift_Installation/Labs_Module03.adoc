:toc2:
:icons: images/icons

= Labs: Install OpenShift

toc::[]

== Labs Overview


* Lab: Prepare to Deploy OpenShift Enterprise
- Lab Scenario: In this lab we will prepare our hosts for the OpenShift Enerprise Installation,
 We will configure DNS and NFS ervers on our Administration Host, Configure the
  settings and Install docker.

* Lab: Install OpenShift Enterprise
- Lab Scenario: In this lab we will Install OpenShift Enterprise by using the
 Quick installer.

* Lab: OpenShift Configuration and Setup
- Lab Scenario: In this lab we will configure OpenShift Enterise, We will
 "label" our nodes, Configure Authentication and deploy the `Registry` and
  `Default Router` containers on our "Infranode" node host.

* Lab: Setting Up Persistent Storage
- Lab Scenario: In this lab we will prepare the OpenShift Cluster to use NFS
 storage as a `Persistent Volume` provider.


== Lab: Prepare to Deploy OpenShift Enterprise

=== Overview
* Lab: Prepare to Deploy OpenShift Enterprise
- Lab Scenario: In this lab we will prepare our hosts for the OpenShift Enerprise Installation,
 We will configure DNS and NFS ervers on our Administration Host, Configure the
  network settings and Install docker.

In this lab we will Deploy OpenShift Enterprise on a Master and 2 Nodes.

* Configure a DNS on the `oselab` server to serve your OpenShift Enterprise environment
* Configure SSH keys
* Configure repositories
* Configure network settings
* Install Docker on all the hosts

[NOTE]
A reminder: You are only allowed to use SSH to access the administration host
 from outside the lab environment. All other hosts have external SSH blocked.
  Once on the administration host, you can use SSH to access the other hosts
   internally. As described earlier, you must use your private SSH key and
    OPENTLC login to access the system (not `root`).

Each student lab is assigned a global unique identifier (GUID) that consists of
 4 characters.  This GUID is provided in the email that is sent to you when you
  provision your lab environment.
  _Whenever you see GUID from this point on, replace it with your lab's GUID._

=== Lab Environment

The lab environment consists of 5 VMs:

* `oselab-GUID.oslab.opentlc.com` (administration host, will be used as a DNS
   Server, NFS Server and Host we will be installing the environment from)

* `master00-GUID.oslab.opentlc.com` (master host, contains Etcd and the
   management console)

* `infranode00-GUID.oslab.opentlc.com` (infranode host, A regular Node that is
  dedicated to only runs the infrastructure containers: Registry and Router)

* `node00-GUID.oslab.opentlc.com` (node host, Region: primary, Zone: east)

* `node01-GUID.oslab.opentlc.com` (node host, Region: primary, Zone: west)

NOTE: Through the labs in this section we will be using "oselab" host as our DNS
 and NFS Server, and our provisioning/staging host to run remote commands on our
  OpenShift Environment.

NOTE: "oselab" host is not an OpenShift Cluster member, and is not part of the
OpenShift environment, it mimics our client's infrastructure or our
 laptop/desktop connected to the client's LAN.

=== Important information

.Following instructions

* We will run *most, but not all*, of our commands from the 'oselab' host.
* When instructions are to be executed on all nodes/hosts:
- We will usually run the command on a specific server to study the output.
- We will execute the command on the rest of the nodes/hosts using a "for" loop
 to save some typing.
- Feel free run the commands directly on the nodes/hosts instead of the "for"
 loop, in some cases that could save you some time.
* The $guid/$GUID environment variables are already defined on all the hosts.
- If you see "GUID" in links or file definitions, you will need to replace those
 with your GUID value yourself.

* Administration host example:
+
----
[root@oselab-GUID ~]# command
----

* Master host example:
+
----
[root@master00-GUID ~]# command
----

IMPORTANT: In each step, be especially careful to make sure that you are running
 the step on the required host.  Each step contains the name of the host to use,
  and the example code contains the host name in the shell prompt.

TIP: It is highly recommended that you use a terminal multiplexing tool such as
   `tmux` or `screen` in case you lose connectivity to your environment.
    This keeps your place in your session if you are disconnected. You can
     install packages after we set up the RHEL Repositories.

TIP: To enter "scroll mode" in `tmux` press CTRL+B and then use "PgUp" and
 "PgDn" buttons to scroll and "ESC" to exit scroll mode.

=== Configure Wildcard DNS on "oselab" host to Service the OpenShift Environment

OpenShift Enterprise requires a "wildcard DNS A record". The "wildcard A record"
 should point to the publicly available IP of a node or nodes that are hosting
 the OpenShift Default Router container.

NOTE: In our environment the OpenShift Default Router will be deployed on the
 "infranode00" host.

. Connect to your administration host `oselab-GUID.oslab.opentlc.com` (your private key location may vary).
+
----
yourdesktop$ ssh -i ~/.ssh/id_rsa your-opentlc-login@oselab-GUID.oslab.opentlc.com
----

. Here is an example of a successful connection:
+
----
[sborenst@desktop01 ~]$ ssh shacharb-redhat.com@oselab-c0fe.oslab.opentlc.com
#############################################################################
#############################################################################
#############################################################################
Environment Deployment Is Completed : Wed Nov 25 20:03:55 EST 2015
#############################################################################
#############################################################################
#############################################################################

-bash-4.2$

----

. Use the "sudo" command to become the `root` user on the administration host.
+
----
-bash-4.2$ sudo bash
----

. Install the `bind` and `bind-utils` package on the administration host.
+
----
[root@oselab-GUID ~]# yum -y install bind bind-utils
----

. Quickly check that the $GUID and $guid environment variables have been configured correctly:
+
----
[root@oselab-GUID ~]# echo GUID is $GUID and guid is $GUID
----

.. You should see output similar to this:
+
----
GUID is c0fe and guid is c0fe
----

.. If the $GUID and $guid environment variables are not set, use the following commands:
+
----
[root@oselab-GUID ~]# export GUID=`hostname|cut -f2 -d-|cut -f1 -d.`
[root@oselab-GUID ~]# export guid=`hostname|cut -f2 -d-|cut -f1 -d.`

----
. On the administration host, collect and define the environment's information. You define the public IP of `InfraNode00` as the target of the wildcard record.
+
----
[root@oselab-GUID ~]# host infranode00-$GUID.oslab.opentlc.com  ipa.opentlc.com |grep infranode | awk '{print $4}'
[root@oselab-GUID ~]# HostIP=`host infranode00-$GUID.oslab.opentlc.com  ipa.opentlc.com |grep infranode | awk '{print $4}'`
[root@oselab-GUID ~]# domain="cloudapps-$GUID.oslab.opentlc.com"
----

. On the administration host, create the zone file with the wildcard DNS.
+
----
[root@oselab-GUID ~]# mkdir /var/named/zones
[root@oselab-GUID ~]# echo "\$ORIGIN  .
\$TTL 1  ;  1 seconds (for testing only)
${domain} IN SOA master.${domain}.  root.${domain}.  (
  2011112904  ;  serial
  60  ;  refresh (1 minute)
  15  ;  retry (15 seconds)
  1800  ;  expire (30 minutes)
  10  ; minimum (10 seconds)
)
  NS master.${domain}.
\$ORIGIN ${domain}.
test A ${HostIP}
* A ${HostIP}"  >  /var/named/zones/${domain}.db
----

. Configure `named.conf` on the administration host.
+
----
[root@oselab-GUID ~]# echo "// named.conf
options {
  listen-on port 53 { any; };
  directory \"/var/named\";
  dump-file \"/var/named/data/cache_dump.db\";
  statistics-file \"/var/named/data/named_stats.txt\";
  memstatistics-file \"/var/named/data/named_mem_stats.txt\";
  allow-query { any; };
  recursion yes;
  /* Path to ISC DLV key */
  bindkeys-file \"/etc/named.iscdlv.key\";
};
logging {
  channel default_debug {
    file \"data/named.run\";
    severity dynamic;
  };
};
zone \"${domain}\" IN {
  type master;
  file \"zones/${domain}.db\";
  allow-update { key ${domain} ; } ;
};" > /etc/named.conf
----

. On the administration host, correct file permissions and start the DNS server.
+
----
[root@oselab-GUID ~]#  chgrp named -R /var/named ; \
 chown named -R /var/named/zones ; \
 restorecon -R /var/named ; \
 chown root:named /etc/named.conf ; \
 restorecon /etc/named.conf ;
----

. Enable and start `named` on the administration host.
+
----
[root@oselab-GUID ~]# systemctl enable named ; \
 systemctl start named
----

. Configure `firewalld` on the administration host to allow inbound DNS queries.
+
----
[root@oselab-GUID bin]# firewall-cmd --zone=public --add-service=dns --permanent ; \
 firewall-cmd --reload

----

=== Verify DNS Configuration

A test DNS entry was created called `test.cloudapps-GUID.oslab.opentlc.com`.

. First, test the DNS server running on the administration host.
+
----
[root@oselab-GUID ~]# host test.cloudapps-$GUID.oslab.opentlc.com 127.0.0.1
----

. Second, test with an external name server.
+
----
[root@oselab-GUID ~]# host test.cloudapps-$GUID.oslab.opentlc.com 8.8.8.8
----
+
[NOTE]
The first time you query 8.8.8.8 you may notice lag and an error "connection
 timed out; trying next origin Host test.cloudapps-GUID.oslab.opentlc.com not
  found: 3(NXDOMAIN)". This is normal.  If you run the test again, it will go
  faster and not error out.

. Test DNS from your laptop/desktop. It might take a few minutes to update.
 Be sure to replace GUID with the correct GUID.
+
----
Desktop$ nslookup test.cloudapps-$GUID.oslab.opentlc.com
----

=== Configure SSH Keys

The OpenShift Enterprise installer uses SSH to configure hosts.
In this lab you create and install an SSH key pair on the *"oselab"* host and
 add the public key to the `authorized_hosts` file on all the OpenShift Hosts.

. Create an SSH key pair for the `root` user, overwrite the existing Key.
+
----
[root@oselab-GUID ~]# ssh-keygen -f /root/.ssh/id_rsa -N ''
----
+
NOTE: In different environments you may use a non-root user that has "sudo"
 capabilities, for example, in AWS you would use the "ec2-user" user.

. On the "oselab" host, locally add the public SSH key to `/root/.ssh/authorized_keys`.
+
----
[root@oselab-GUID ~]# cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys
----

. Configure `/etc/ssh/ssh_conf` to disable `StrictHostKeyChecking` on the
 "oselab" host and the "master" host:
+
----
[root@oselab-GUID ~]# echo StrictHostKeyChecking no >> /etc/ssh/ssh_config
[root@oselab-GUID ~]# ssh master00-$guid "echo StrictHostKeyChecking no >> /etc/ssh/ssh_config"
----
NOTE: This configurations saves us do disable strict host checking and needing
 to answer "yes" when we remote commands on unknown hosts. You will run many
  commands from both the "oselab" and the "master00" hosts.
+

. From the "oselab" host, test the new SSH key by connecting it to itself over
 the loopback interface without a keyboard prompt.
+
----
[root@oselab-GUID ~]# ssh 127.0.0.1
...[output omitted]...
[root@oselab-GUID ~]# exit
----

. Copy the SSH key to the rest of the nodes in the environment, you will be
 prompted for the root password for each of the nodes.
+
----
[root@oselab-GUID ~]# for node in   master00-$GUID.oslab.opentlc.com \
                                    infranode00-$guid.oslab.opentlc.com \
                                    node00-$guid.oslab.opentlc.com \
                                    node01-$guid.oslab.opentlc.com; \
                                    do \
                                    ssh-copy-id root@$node ; \
                                    done
----
+
[NOTE]
Remember: The default `root` password is `r3dh4t1!`.


=== Configure the Repositories on the Master Host

OpenShift Enterprise requires several software repositories:

* `rhel-7-server-rpms`

* `rhel-7-server-extras-rpms`

* `rhel-7-server-optional-rpms`

* `rhel-7-server-ose-3.x-rpms`

Normally you obtain these repositories via `subscription-manager` but we have
 provided a mirror that you will configure in the following steps:


. On the "oselab" host, set up the `yum` repository configuration file
 `/etc/yum.repos.d/open.repo` with the following repositories:
+
----
cat << EOF > /etc/yum.repos.d/open.repo
[rhel-x86_64-server-7]
name=Red Hat Enterprise Linux 7
baseurl=http://www.opentlc.com/repos/rhel-7-server-rpms
enabled=1
gpgcheck=0

[rhel-x86_64-server-extras-7]
name=Red Hat Enterprise Linux 7 Extras
baseurl=http://www.opentlc.com/repos/rhel-7-server-extras-rpms
enabled=1
gpgcheck=0

[rhel-x86_64-server-optional-7]
name=Red Hat Enterprise Linux 7 Optional
baseurl=http://www.opentlc.com/repos/rhel-7-server-optional-rpms
enabled=1
gpgcheck=0

# This repo is added for the OPENTLC environment not OSE
[rhel-x86_64-server-rh-common-7]
name=Red Hat Enterprise Linux 7 Common
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-rh-common-7
enabled=1
gpgcheck=0

EOF
----

. Add the OpenShift Enterprise repository mirror to the "oselab" host.
+
----
[root@oselab-GUID ~]# cat << EOF >> /etc/yum.repos.d/open.repo
[rhel-7-server-ose-3.1-rpms]
name=Red Hat Enterprise Linux 7 OSE 3.1
baseurl=http://www.opentlc.com/repos/rhel-7-server-ose-3.1-rpms
enabled=1
gpgcheck=0

EOF
----

. List the available repositories on the "oselab" host.
+
-----
[root@oselab-GUID ~]# yum repolist
-----

* You should see the following:
+
----
Loaded plugins: product-id
...[output omitted]...
repo id                                        repo name                                           status
rhel-7-server-ose-3.1-rpms                     Red Hat Enterprise Linux 7 OSE 3                      323
rhel-x86_64-server-7                           Red Hat Enterprise Linux 7                          4,391
rhel-x86_64-server-extras-7                    Red Hat Enterprise Linux 7 Extras                      45
rhel-x86_64-server-optional-7                  Red Hat Enterprise Linux 7 Optional                 4,220
rhel-x86_64-server-rh-common-7                 Red Hat Enterprise Linux 7 Common                      19
repolist: 8,998

...[output omitted]...
----

. Configure the master and nodes by copying the `open.repo` file to all the nodes
 directly from the "oselab" host.
+
-----
[root@oselab-GUID ~]# for node in master00-$guid.oslab.opentlc.com \
                                    infranode00-$guid.oslab.opentlc.com \
                                    node00-$guid.oslab.opentlc.com \
                                    node01-$guid.oslab.opentlc.com; \
                                    do \
                                      echo Copying open.repo to $node ; \
                                      scp /etc/yum.repos.d/open.repo ${node}:/etc/yum.repos.d/open.repo ;
                                      yum repolist
                                   done
-----

=== Verify Network Configuration

In this section of the lab you verify that the master host is configured
 correctly for internal and external DNS name resolution.

. Connect to the "master00" host
+
----
[root@oselab-GUID ~]# ssh master00-$guid
----

. Verify the `hostname` for the master host.
+
----
[root@master00-GUID ~]# hostname -f
----

* You should see the following:
+
----
master00-GUID.oslab.opentlc.com
----

. Take note of the master host's internal IP address.
+
----
[root@master00-GUID ~]# ip address show dev eth0|grep "inet "|awk '{print $2}'|cut -f1 -d/
----

. Make sure the master host's internal DNS entry matches the internal IP address.
+
----
[root@master00-GUID ~]# host `hostname -f`
----

. Take note of the master host's external IP address.
+
----
[root@master00-GUID ~]# curl http://www.opentlc.com/getip
----

. Make sure the master host's external DNS entry matches the external IP address.
+
----
[root@master00-GUID ~]# host `hostname -f` 8.8.8.8
----
+
NOTE: If this does not work on the first try, wait a short while and try again.
 It may take some time for the global DNS servers to update.

=== Install and Remove packages

. Back on our "oselab" host, Run the following for-loop to remove
 `NetworkManager` from all the the nodes and the master
+
----
[root@oselab-GUID ~]# for node in   master00-$guid.oslab.opentlc.com \
                                    infranode00-$guid.oslab.opentlc.com \
                                    node00-$guid.oslab.opentlc.com \
                                    node01-$guid.oslab.opentlc.com; \
                                    do \
                                    echo removing NetworkManager on $node ; \
                                      ssh $node "yum -y  remove NetworkManager*"
                                   done
----

NOTE: It is possible to configure `NetworkManager`  so it doesn't need to be removed.

. Install the following tools and utilities on the "master00" host
+
----
[root@oselab-GUID ~]# ssh master00-$guid "yum -y install wget git net-tools bind-utils iptables-services bridge-utils python-virtualenv gcc"
----

. Its highly recommended to also install "bash-completion" on the "oselab" host and the "master" host
----
[root@oselab-GUID ~]# yum -y install "bash-completion"
[root@oselab-GUID ~]# ssh master00-$guid "yum -y install bash-completion"

----

TIP: Bash Completion will only work the next time you start the "bash" shell.

. Run "yum update" on the master and all the nodes
+
----
[root@oselab-GUID ~]# for node in master00-$guid.oslab.opentlc.com \
                                    infranode00-$guid.oslab.opentlc.com \
                                    node00-$guid.oslab.opentlc.com \
                                    node01-$guid.oslab.opentlc.com; \
                                    do \
                                    echo Running yum update on $node ; \
                                    ssh $node "yum -y update " ; \
                                    done

----

=== Install Docker

OpenShift Enterprise uses Docker to store and manage container images.
 In this lab, you install Docker.

. Connect to the "master00" host
+
----
[root@oselab-GUID ~]# ssh master00-$guid
----

. Install the `docker` package on the master host
+
----
[root@master00-GUID ~]# yum -y install docker
----

. Install the `docker` package on the rest of the nodes
+
----
[root@master00-GUID ~]# for node in   infranode00-$guid.oslab.opentlc.com \
                                    node00-$guid.oslab.opentlc.com \
                                    node01-$guid.oslab.opentlc.com; \
                                    do \
                                    echo Installing docker on $node ; \
                                    ssh $node "yum -y install docker"  ;
                                    done
----

. Configure the Docker registry on the "master" host to allow insecure
 (no Certificate) connections to Docker registries within our network.
+
----
[root@master00-GUID ~]# sed -i "s/OPTIONS.*/OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0\/16'/" /etc/sysconfig/docker
----
+
[NOTE]
Openshift default "service" network is 172.30.0.0, you are using this value
 because the local registry will be deployed under this subnet.

. Configure the Docker registry on the rest of the nodes.
+
----
[root@master00-GUID ~]# for node in infranode00-$guid.oslab.opentlc.com \
                                    node00-$guid.oslab.opentlc.com \
                                    node01-$guid.oslab.opentlc.com; \
                                    do \
                                    echo Overwriting docker configuration file on $node ; \
                                    scp  /etc/sysconfig/docker $node:/etc/sysconfig/docker ;
                                    done
----

=== Configure Docker Storage

In this lab you configure the Docker storage pool.

NOTE: The default Docker storage configuration uses loopback devices and is not
 appropriate for production. Red Hat considers the `dm.thinpooldev` storage
  option to be the only appropriate configuration for production use.

. Stop the Docker daemon and remove any files from "/var/lib/docker"
+
----
[root@master00-GUID ~]# systemctl stop docker
[root@master00-GUID ~]# rm -rf /var/lib/docker/*
----

. Do the same for the rest of the nodes.
+
----
[root@master00-GUID ~]# for node in infranode00-$guid.oslab.opentlc.com \
                                    node00-$guid.oslab.opentlc.com \
                                    node01-$guid.oslab.opentlc.com; \
                                    do
                                    echo Cleaning up Docker on $node ; \
                                    ssh $node "systemctl stop docker ; rm -rf /var/lib/docker/*"  ;
                                    done
----

. Configure "docker-storage setup" to use the "/dev/vdb" hard drive as the
 docker volume group:
+
----
[root@master00-GUID ~]# cat <<EOF > /etc/sysconfig/docker-storage-setup
DEVS=/dev/vdb
VG=docker-vg
EOF

----

. Run `docker-storage-setup` on the *master00* host to create logical volumes
 for Docker:
+
----
[root@master00-GUID ~]#  docker-storage-setup
----
+
. You should see the following:
+
----

Checking that no-one is using this disk right now ...
OK

Disk /dev/vdb: 20805 cylinders, 16 heads, 63 sectors/track
sfdisk:  /dev/vdb: unrecognized partition table type

Old situation:
sfdisk: No partitions found

New situation:
Units: sectors of 512 bytes, counting from 0

   Device Boot    Start       End   #sectors  Id  System
/dev/vdb1          2048  20971519   20969472  8e  Linux LVM
/dev/vdb2             0         -          0   0  Empty
/dev/vdb3             0         -          0   0  Empty
/dev/vdb4             0         -          0   0  Empty
Warning: partition 1 does not start at a cylinder boundary
Warning: partition 1 does not end at a cylinder boundary
Warning: no primary partition is marked bootable (active)
This does not matter for LILO, but the DOS MBR will not boot this disk.
Successfully wrote the new partition table

Re-reading the partition table ...

If you created or changed a DOS partition, /dev/foo7, say, then use dd(1)
to zero the first 512 bytes:  dd if=/dev/zero of=/dev/foo7 bs=512 count=1
(See fdisk(8).)
  Physical volume "/dev/vdb1" successfully created
  Volume group "docker-vg" successfully created
  Rounding up size to full physical extent 12.00 MiB
  Logical volume "docker-poolmeta" created.
  Logical volume "docker-pool" created.
  WARNING: Converting logical volume docker-vg/docker-pool and docker-vg/docker-poolmeta to pool's data and metadata volumes.
  THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
  Converted docker-vg/docker-pool to thin pool.
  Logical volume "docker-pool" changed.

----
+
[NOTE]
In a real environment, be careful with `docker-storage-setup` as it will, by
 default, find any unused extents in the volume group that contains your root
  filesystem to create the pool. You can also specify a specific volume group or
   block device.  This can be a destructive process to the specified VG or block
    device!  Consult the OpenShift documentation for more information.

. On the *master* host examine the newly created logical volume `docker-pool`:
+
----
[root@master00-GUID ~]#  lvs
----
+
You should see the following:
+
----
LV          VG                    Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
docker-pool docker-vg             twi-a-t---  3.99g             0.00   0.29
root        rhel_host2cc260760b15 -wi-ao---- 17.51g
swap        rhel_host2cc260760b15 -wi-ao----  2.00g
----

. On the *master* host, examine the docker storage configuration:
+
----
[root@master00-GUID ~]# cat /etc/sysconfig/docker-storage
----
+
You should see the following:
+
----
DOCKER_STORAGE_OPTIONS=--storage-driver devicemapper --storage-opt dm.fs=xfs --storage-opt dm.thinpooldev=/dev/mapper/docker--vg-docker--pool
----

. You can use this for-loop to configure docker storage on the rest of the nodes,
 enable docker and restart the node.
+
----
[root@master00-GUID ~]# for node in infranode00-$guid.oslab.opentlc.com \
                                    node00-$guid.oslab.opentlc.com \
                                    node01-$guid.oslab.opentlc.com; \
                                    do
                                      echo Configuring Docker Storage and rebooting $node
                                      scp /etc/sysconfig/docker-storage-setup ${node}:/etc/sysconfig/docker-storage-setup
                                      ssh $node "
                                            docker-storage-setup ;
                                            systemctl enable docker;
                                            reboot"
                                    done
----

. Enable, start, and get status for the Docker service on the master host.
+
----
[root@master00-GUID ~]# systemctl enable docker
----

. Reboot the master host.
+
-----
[root@master00-GUID ~]# reboot
-----

=== Populate the Local Docker Registry

. Log back in to the "osehost" host after the nodes and the master complete the
 reboot.

. Check that the Docker service is started on all the nodes:
+
----
[root@oselab-GUID ~]# for node in   master00-$guid.oslab.opentlc.com \
                                    infranode00-$guid.oslab.opentlc.com \
                                    node00-$guid.oslab.opentlc.com \
                                    node01-$guid.oslab.opentlc.com; \
                                    do
                                      echo Checking docker status on $node
                                      ssh $node "
                                            systemctl status docker | grep Active"
                                    done
----

* You should see the following:
+
----
Checking docker status on master00-c0fe.oslab.opentlc.com
   Active: active (running) since Thu 2015-11-26 01:03:14 EST; 2min 24s ago
Checking docker status on infranode00-c0fe.oslab.opentlc.com
   Active: active (running) since Thu 2015-11-26 01:02:15 EST; 3min 24s ago
Checking docker status on node00-c0fe.oslab.opentlc.com
   Active: active (running) since Thu 2015-11-26 01:02:17 EST; 3min 23s ago
Checking docker status on node01-c0fe.oslab.opentlc.com
   Active: active (running) since Thu 2015-11-26 01:02:20 EST; 3min 21s ago

----
+
[NOTE]
Make sure the status is `enabled` and `active (running)`.

. From the "oselab" host, pre-fetch the Docker images to all the nodes in the
 primary region (`node00` and `node01`).
+
----
[root@oselab-GUID ~]# REGISTRY="registry.access.redhat.com";PTH="openshift3"
[root@oselab-GUID ~]# for node in  node00-$guid.oslab.opentlc.com \
                                   node01-$guid.oslab.opentlc.com; \
do
ssh $node "
docker pull $REGISTRY/$PTH/ose-deployer:v3.1.0.4 ; \
docker pull $REGISTRY/$PTH/ose-sti-builder:v3.1.0.4 ; \
docker pull $REGISTRY/$PTH/ose-sti-image-builder:v3.1.0.4 ; \
docker pull $REGISTRY/$PTH/ose-docker-builder:v3.1.0.4 ; \
docker pull $REGISTRY/$PTH/ose-pod:v3.1.0.4 ; \
docker pull $REGISTRY/$PTH/ose-keepalived-ipfailover:v3.1.0.4 ; \
docker pull $REGISTRY/$PTH/ruby-20-rhel7 ; \
docker pull $REGISTRY/$PTH/mysql-55-rhel7 ; \
docker pull openshift/hello-openshift:v1.0.6
"
done
----
TIP: We are only downloading these images to save time later, if a node doesn't
have an image locally it will try to download it. (unless otherwise configured)
+
[NOTE]
This will take about 10 minutes to complete on *each node*, you don't have to wait
for this to complete, just connect to each node, run the pull and continue with
other tasks.

. On *Infranode00*, pull only the basic images and the *Registry* and *Router*
 images.
+
----
[root@oselab-GUID ~]# REGISTRY="registry.access.redhat.com";PTH="openshift3"
[root@oselab-GUID ~]# ssh infranode00-$guid.oslab.opentlc.com "
docker pull $REGISTRY/$PTH/ose-haproxy-router:v3.1.0.4  ; \
docker pull $REGISTRY/$PTH/ose-deployer:v3.1.0.4 ; \
docker pull $REGISTRY/$PTH/ose-pod:v3.1.0.4 ; \
docker pull $REGISTRY/$PTH/ose-docker-registry:v3.1.0.4 ;
"

NOTE: We aren't "pulling" any images on the Master host because it is not meant
do run any containers.

----
. Examine Docker pool info on the `node0X` (i.e., `node00`, `node01`, etc.) host.
+
----
[root@oselab-GUID ~]# ssh node00-$guid docker info
----

* You should see something similar to the following:
+
----
Containers: 0
Images: 15
Storage Driver: devicemapper
Pool Name: docker--vg-docker--pool
Pool Blocksize: 524.3 kB
Backing Filesystem: xfs
Data file:
Metadata file:
Data Space Used: 1.481 GB
Data Space Total: 10.72 GB
Data Space Available: 9.24 GB
Metadata Space Used: 323.6 kB
Metadata Space Total: 29.36 MB
Metadata Space Available: 29.04 MB
Udev Sync Supported: true
Deferred Removal Enabled: false
Library Version: 1.02.93-RHEL7 (2015-01-28)
Execution Driver: native-0.2
Logging Driver: json-file
Kernel Version: 3.10.0-229.el7.x86_64
Operating System: Red Hat Enterprise Linux Server 7.1 (Maipo)
CPUs: 2
Total Memory: 1.797 GiB
Name: node00-c0fe.oslab.opentlc.com
ID: RXVI:JKOO:3U4X:LHDE:QXPN:FSQC:TTBL:UCWP:MCEH:2KU6:GWSD:IRIN
...
----

. On the `node0X` host, examine the `docker-pool` logical volume again.
+
----
[root@oselab-GUID ~]# ssh node00-$guid.oslab.opentlc.com "lvs"
----

* You should see something similar to the following:
+
----
LV          VG                    Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
docker-pool docker-vg             twi-a-t---  9.98g             13.81  1.10
root        rhel_host2cc260760b15 -wi-ao---- 17.51g
swap        rhel_host2cc260760b15 -wi-ao----  2.00g
----

== Lab: Install OpenShift Enterprise

=== Download the Installer

. On the "oselab" host, install the Openshift utility package.
+
----
[root@oselab-GUID ~]# yum -y install atomic-openshift-utils
----

. (Optional) Copy and paste the master and node names to a local file:
+
----
[root@oselab-GUID ~]#  for node in master00-$guid.oslab.opentlc.com \
                                    infranode00-$guid.oslab.opentlc.com \
                                    node00-$guid.oslab.opentlc.com \
                                    node01-$guid.oslab.opentlc.com; do
                                    echo $node ;
                                    done
master00-GUID.oslab.opentlc.com
infranode00-GUID.oslab.opentlc.com
node00-GUID.oslab.opentlc.com
node01-GUID.oslab.opentlc.com

----

=== Run the Installer

. Execute the installation utility to interactively configure one or more hosts
+
----
[root@oselab-GUID ~]# atomic-openshift-installer install
----

. Follow the instructions of the Installer
----

Welcome to the OpenShift Enterprise 3 installation.

Please confirm that following prerequisites have been met:

* All systems where OpenShift will be installed are running Red Hat Enterprise
  Linux 7.
* All systems are properly subscribed to the required OpenShift Enterprise 3
  repositories.
* All systems have run docker-storage-setup (part of the Red Hat docker RPM).
* All systems have working DNS that resolves not only from the perspective of
  the installer but also from within the cluster.

When the process completes you will have a default configuration for Masters
and Nodes.  For ongoing environment maintenance it's recommended that the
official Ansible playbooks be used.

For more information on installation prerequisites please see:
https://docs.openshift.com/enterprise/latest/admin_guide/install/prerequisites.html

Are you ready to continue? [y/N]:

----

. Enter *y* and you should see:
+
----

This installation process will involve connecting to remote hosts via ssh.  Any
account may be used however if a non-root account is used it must have
passwordless sudo access.

User for ssh access [root]: root


----
+
CAUTION: Please pay attention to your inputs, if you make a mistake use CTRL+C
 to exit the installer and try again.

. Answer *root*, and you should see:
+
----

***Master Configuration***

The OpenShift Master serves the API and web console.  It also coordinates the
jobs that have to run across the environment.  It can even run the datastore.
For wizard based installations the database will be embedded.  It's possible to
change this later using etcd from Red Hat Enterprise Linux 7.

Any Masters configured as part of this installation process will also be
configured as Nodes.  This is so that the Master will be able to proxy to Pods
from the API.  By default this Node will be unscheduleable but this can be changed
after installation with 'oadm manage-node'.

The OpenShift Node provides the runtime environments for containers.  It will
host the required services to be managed by the Master.

http://docs.openshift.com/enterprise/latest/architecture/infrastructure_components/kubernetes_infrastructure.html#master
http://docs.openshift.com/enterprise/latest/architecture/infrastructure_components/kubernetes_infrastructure.html#node

Enter hostname or IP address: []: master00-GUID.oslab.opentlc.com
Will this host be an OpenShift Master? [y/N]: y
Do you want to add additional hosts? [y/N]: y
Enter hostname or IP address: []: infranode00-GUID.oslab.opentlc.com
Will this host be an OpenShift Master? [y/N]: n
Do you want to add additional hosts? [y/N]: y
Enter hostname or IP address: []: node00-GUID.oslab.opentlc.com
Will this host be an OpenShift Master? [y/N]: n
Do you want to add additional hosts? [y/N]: y
Enter hostname or IP address: []: node01-GUID.oslab.opentlc.com
Will this host be an OpenShift Master? [y/N]: n
Do you want to add additional hosts? [y/N]: n


----
+
. Answer the questions:
.. Paste or type the name of your *master* host "master00-GUID.oslab.opentlc.com"
.. Answer "y" to confirm that this host should be an OpenShift Master
.. Answer "y" to Add more hosts
.. Paste or type the name of your *infra* host "infranode00-GUID.oslab.opentlc.com"
.. Answer "n" to indicate that this host is not an OpenShift Master
.. Answer "y" to Add more hosts
.. Paste or type the name of your *node00* host "node00-GUID.oslab.opentlc.com"
.. Answer "n" to indicate that this host is not an OpenShift Master
.. Answer "y" to Add more hosts
.. Paste or type the name of your *node01* host "node01-GUID.oslab.opentlc.com"
.. Answer "n" to indicate that this host is not an OpenShift Master
.. Answer "n" to stop adding OpenShift hosts.

. Answer "2" in the next step to select OpenShift 3.1 in the varient selection:
+
----
Which variant would you like to install?


(1) OpenShift Enterprise 3.0
(2) OpenShift Enterprise 3.1
(3) Atomic Enterprise Platform 3.1
Choose a variant from above:  [1]: 2
----

. The Installer will collect information about your environment and display the
 following:
+
----
Gathering information from hosts...
...This might take a few minutes...
A list of the facts gathered from the provided hosts follows. Because it is
often the case that the hostname for a system inside the cluster is different
from the hostname that is resolveable from command line or web clients
these settings cannot be validated automatically.

For some cloud providers the installer is able to gather metadata exposed in
the instance so reasonable defaults will be provided.

Plese confirm that they are correct before moving forward.


master00-GUID.oslab.opentlc.com,192.168.0.100,192.168.0.100,master00-GUID.oslab.opentlc.com,master00-GUID.oslab.opentlc.com
infranode00-GUID.oslab.opentlc.com,192.168.0.101,192.168.0.101,infranode00-GUID.oslab.opentlc.com,infranode00-GUID.oslab.opentlc.com
node00-GUID.oslab.opentlc.com,192.168.0.200,192.168.0.200,node00-GUID.oslab.opentlc.com,node00-GUID.oslab.opentlc.com
node01-GUID.oslab.opentlc.com,192.168.0.201,192.168.0.201,node01-GUID.oslab.opentlc.com,node01-GUID.oslab.opentlc.com


Format:

connect_to,IP,public IP,hostname,public hostname

Notes:
 * The installation host is the hostname from the installer's perspective.
 * The IP of the host should be the internal IP of the instance.
 * The public IP should be the externally accessible IP associated with the instance
 * The hostname should resolve to the internal IP from the instances
   themselves.
 * The public hostname should resolve to the external ip from hosts outside of
   the cloud.

Do the above facts look correct? [y/N]: y
Ready to run installation process.
If changes are needed to the values recorded by the installer please update /root/.config/openshift/installer.cfg.yml.
Are you ready to continue? [y/N]: y
----

. Enter *y* to confirm the collected facts and answer *y* again to continue
 after learning the location of the configuration file location.
+
. Watch the installer run:
+
----
[Omitted long output]

PLAY RECAP ********************************************************************
infranode00-GUID.oslab.opentlc.com : ok=58   changed=22   unreachable=0    failed=0
localhost                  : ok=11   changed=0    unreachable=0    failed=0
master00-GUID.oslab.opentlc.com : ok=206  changed=58   unreachable=0    failed=0
node00-GUID.oslab.opentlc.com : ok=58   changed=22   unreachable=0    failed=0
node01-GUID.oslab.opentlc.com : ok=58   changed=22   unreachable=0    failed=0


The installation was successful!

If this is your first time installing please take a look at the Administrator
Guide for advanced options related to routing, storage, authentication and much
more:

http://docs.openshift.com/enterprise/latest/admin_guide/overview.html

Press any key to continue .

----

. Take a look at the install configuration file, we could have created this file
 instead of going through the interactive setup
+
----
[root@master00-GUID ~]# cat  /root/.config/openshift/installer.cfg.yml
ansible_config: /usr/share/atomic-openshift-utils/ansible.cfg
ansible_log_path: /tmp/ansible.log
ansible_ssh_user: root
hosts:
- connect_to: master00-GUID.oslab.opentlc.com
  hostname: master00-GUID.oslab.opentlc.com
  ip: 192.168.0.100
  master: true
  node: true
  public_hostname: master00-GUID.oslab.opentlc.com
  public_ip: 192.168.0.100
- connect_to: infranode00-GUID.oslab.opentlc.com
  hostname: infranode00-GUID.oslab.opentlc.com
  ip: 192.168.0.101
  node: true
  public_hostname: infranode00-GUID.oslab.opentlc.com
  public_ip: 192.168.0.101
- connect_to: node00-GUID.oslab.opentlc.com
  hostname: node00-GUID.oslab.opentlc.com
  ip: 192.168.0.200
  node: true
  public_hostname: node00-GUID.oslab.opentlc.com
  public_ip: 192.168.0.200
- connect_to: node01-GUID.oslab.opentlc.com
  hostname: node01-GUID.oslab.opentlc.com
  ip: 192.168.0.201
  node: true
  public_hostname: node01-GUID.oslab.opentlc.com
  public_ip: 192.168.0.201
variant: openshift-enterprise
variant_version: '3.1'
version: v1

----

. After the installer has completed, restart all the nodes and the master.
+
----
[root@oselab-GUID ~]# for node in   master00-$guid.oslab.opentlc.com \
                                    infranode00-$guid.oslab.opentlc.com \
                                    node00-$guid.oslab.opentlc.com \
                                    node01-$guid.oslab.opentlc.com; \
                                    do \
                                      echo Rebooting $node ; \
                                      ssh $node "reboot"
                                   done
----

=== Verify Your Environment

. Connect to the "master00" host
+
----
[root@oselab-GUID ~]# ssh master00-$guid
----

. Run `oc get nodes` to check the status of your hosts.
+
----

[root@master-GUID ~]# oc get nodes
NAME                                 LABELS                                 STATUS                     AGE
infranode00-GUID.oslab.opentlc.com   kubernetes.io/hostname=192.168.0.101   Ready                      1m
master00-GUID.oslab.opentlc.com      kubernetes.io/hostname=192.168.0.100   Ready,SchedulingDisabled   1m
node00-GUID.oslab.opentlc.com        kubernetes.io/hostname=192.168.0.200   Ready                      1m
node01-GUID.oslab.opentlc.com        kubernetes.io/hostname=192.168.0.201   Ready                      1m
----

. Use your browser to connect to the OpenShift Web Console at : link:https://master00-GUID.oslab.opentlc.com:8443[https://master00-GUID.oslab.opentlc.com:8443]
.. Accept the "Untrusted Certificate"

NOTE: You can't login yet because we didn't set up authentication yet.

== Labs: Configure and Set Up OpenShift Enterprise

=== Set Regions and Zones

Labels on the nodes handle the assignments of _regions_ and _zones_ at the node level.

. Connect to the "master00" host
+
----
[root@oselab-GUID ~]# ssh master00-$guid
----

. Label the nodes.
+
----
[root@master00-GUID ~]# oc label node infranode00-$GUID.oslab.opentlc.com region="infra" zone="infranodes"
[root@master00-GUID ~]# oc label node node00-$GUID.oslab.opentlc.com region="primary" zone="east"
[root@master00-GUID ~]# oc label node node01-$GUID.oslab.opentlc.com region="primary" zone="west"
----


. On the "master00" host, run `oc get nodes` to see how the labels were
 implemented.
+
----

[root@oselab-GUID ~]# oc get nodes

----

* You should see the following:
+
----

NAME                                 LABELS                                                              STATUS                     AGE
infranode00-GUID.oslab.opentlc.com   kubernetes.io/hostname=192.168.0.101,region=infra,zone=infranodes   Ready                      6m
master00-GUID.oslab.opentlc.com      kubernetes.io/hostname=192.168.0.100                                Ready,SchedulingDisabled   6m
node00-GUID.oslab.opentlc.com        kubernetes.io/hostname=192.168.0.200,region=primary,zone=east       Ready                      6m
node01-GUID.oslab.opentlc.com        kubernetes.io/hostname=192.168.0.201,region=primary,zone=west       Ready                      6m

----

At this point, you have a running OpenShift Enterprise environment across three
 hosts, with one master and three nodes, divided into two regions: _infra_
  and _primary_.

From here you start to deploy applications and other resources into OpenShift
 Enterprise.

=== Configure OpenShift Enterprise: Tips

.Setting the Default Subdomain

. To set a _default Route_, you can do that by changing the
`routingConfig` attribute `subdomain`:
commands:
+
[source,bash]
----
[root@master00-GUID ~]# sed  -i "s/subdomain:  \"\"/subdomain: \"cloudapps-${GUID}.oslab.opentlc.com\"/g" /etc/origin/master/master-config.yaml
[root@master00-GUID ~]# systemctl restart atomic-openshift-master

----

.Setting Default NodeSelector

. To set a default `NodeSelector`, you can do that by changing the
 `projectConfig` attribute `defaultNodeSelector`:
+
[source,bash]
----
[root@master00-GUID ~]# sed -i 's/defaultNodeSelector: ""/defaultNodeSelector: "region=primary"/' /etc/origin/master/master-config.yaml
[root@master00-GUID ~]# systemctl restart atomic-openshift-master
----

.Configure the "default" namespace to use the "infra" region

. Add the following annotation line in the "default" namespace object, in the
 annotations section:
+
----
openshift.io/node-selector: region=infra
----

. To edit the "default" namespace, use this command, to exit press ":wq"
+
----
[root@master00-GUID ~]#  oc edit namespace default
----

. Your object should look similar to this:
+
----
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    openshift.io/node-selector: region=infra
    openshift.io/sa.initialized-roles: "true"
    openshift.io/sa.scc.mcs: s0:c3,c2
    openshift.io/sa.scc.supplemental-groups: 1000010000/10000
    openshift.io/sa.scc.uid-range: 1000010000/10000
  creationTimestamp: 2015-11-20T02:10:35Z
  name: default
  resourceVersion: "217"
  selfLink: /api/v1/namespaces/default
  uid: e304c204-8f2b-11e5-9223-2cc260072896
spec:
  finalizers:
  - kubernetes
  - openshift.io/origin
status:
  phase: Active
----

.Logs and `journalctl`  (Reference Only)
Red Hat Enterprise Linux 7 uses `systemd` and `journal`. Because of this, you no
 longer use `/var/log/messages` to look at logs. You now use `journalctl`.

Because Red Hat Enterprise Linux 7 runs all components in higher log levels,
 your instructor recommends that you use your terminal emulator to set up
  windows for each process.

On the master host, run each of the following in its own window:

----
[root@master00-GUID ~]# journalctl -f -u atomic-openshift-master
[root@master00-GUID ~]# journalctl -f -u atomic-openshift-node
----

[NOTE]
You might want to run this on the other nodes, but you do not need the
 `atomic-openshift-master` service. You might also want to watch the Docker
  logs.

=== Configure Authentication

CAUTION: This next section is case sensitive, it might take a few tries to get
the file to be parsed correctly if you are new to YAML.

. Create a copy of your master's configuration file.
+
----
[root@master00-GUID ~]# cp /etc/origin/master/master-config.yaml /etc/origin/master/master-config.yaml.original
----

. Edit `/etc/origin/master/master-config.yaml` so that the `oauthConfig` section looks like the following:
+
----
oauthConfig:
  assetPublicURL: https://master00-GUID.oslab.opentlc.com:8443/console/
  grantConfig:
    method: auto
  identityProviders:
  - name: htpasswd_auth
    challenge: true
    login: true
    provider:
      apiVersion: v1
      kind: HTPasswdPasswordIdentityProvider
      file: /etc/origin/openshift-passwd
  masterPublicURL: https://master00-GUID.oslab.opentlc.com:8443
  masterURL: https://master00-GUID.oslab.opentlc.com:8443
  sessionConfig:
    sessionMaxAgeSeconds: 3600
    sessionName: ssn
    sessionSecretsFile:
  tokenConfig:
    accessTokenMaxAgeSeconds: 86400
    authorizeTokenMaxAgeSeconds: 500
----

CAUTION: Make sure you replace "GUID" with your GUID number.
. Use the sed command to replace the GUID with your actual GUID:
+
----
[root@master00-GUID ~]# sed -i s/GUID/${guid}/g  /etc/origin/master/master-config.yaml
----

=== Add Development Users

In the real world, your developers are likely to use the OpenShift Enterprise
 tools (`oc` and the web console) on their own machines . For this course,
  you create user accounts for two nonprivileged OpenShift Enterprise users,
   `andrew` and `marina`, on the master. You do this both for convenience and
    because you are using `htpasswd` for authentication.

. On the master host, add two Linux accounts.
+
----
[root@master00-GUID ~]# useradd andrew
[root@master00-GUID ~]# useradd marina
----

NOTE: Feel free to create these users on any machine that has the "oc" command
 available.

=== Configure `htpasswd` Authentication

OpenShift Enterprise 3 supports a number of authentication mechanisms. The
 simplest use case for testing purposes is authentication based on `htpasswd`.

To start, you need the `htpasswd` binary available in the `httpd-tools` package.

. Install `httpd-tools` on the master host.
+
----
[root@master00-GUID ~]# yum -y install httpd-tools
----

. Create a password for users `andrew` and `marina` on the master host.
+
----
[root@master00-GUID ~]# htpasswd -cb /etc/origin/openshift-passwd andrew r3dh4t1!
[root@master00-GUID ~]# htpasswd -b /etc/origin/openshift-passwd marina r3dh4t1!
----

. Restart `atomic-openshift-master` for changes to take effect
+
----
[root@master00-GUID ~]# systemctl restart atomic-openshift-master
----




=== The Registy and Router

In the scenario you are simulating in the lab, you are using `infranode00` as
 the target for both the _registry_ and the _default router_.

.Deploying the Registry

. Deploy the `registry`.
+
----
[root@master00-GUID ~]# oadm registry --create --credentials=/etc/origin/master/openshift-registry.kubeconfig
----
+
NOTE: If we wanted to pin down the registry to a specific region we could do
 that with the "--selector" flag, we don't need to do this because we set the
 "default" namespace default `nodeSelector`.

. You can look at the status of your pod using the following commands, This can
  take a few minutes the first time around as the images are being pulled from the registry:
+
----
 [root@master00-GUID ~]# oc get pods
 NAME                       READY     STATUS    RESTARTS   AGE
 docker-registry-1-deploy   1/1       Pending   0          11s

... Wait a few seconds ...
 [root@master00-GUID ~]# oc get pods

 NAME                       READY     STATUS    RESTARTS   AGE
 docker-registry-1-deploy   1/1       Running   0          31s
 docker-registry-1-diqlc    0/1       Pending   0          4s

... Wait a few seconds ...
 [root@master00-GUID ~]# oc get pods
 NAME                      READY     STATUS    RESTARTS   AGE
 docker-registry-1-diqlc   1/1       Running   0          14s

----

. Run the "oc status" command
+
----
[root@master00-GUID master]# oc status
 In project default on server https://master00-GUID.oslab.opentlc.com:8443

 svc/docker-registry - 172.30.41.32:5000
   dc/docker-registry deploys docker.io/openshift3/ose-docker-registry:v3.1.0.4
     #1 deployed 5 minutes ago - 1 pod

 svc/kubernetes - 172.30.0.1 ports 443, 53, 53

 To see more, use 'oc describe <resource>/<name>'.
 You can use 'oc get all' to see a list of other objects.

----

. To test the status of the of Registry you can use the *curl* command to
  communicate to the Registry service port (Example : curl -v 172.30.41.32:5000/healthz).
.. you can use these commands to test your registry for connectivity
+
----
 [root@master00-GUID ~]# echo `oc get service docker-registry --template '{{.spec.portalIP}}:{{index .spec.ports 0 "port"}}/healthz'`
 172.30.42.118:5000/healthz
 [root@master00-GUID ~]# curl -v `oc get service docker-registry --template '{{.spec.portalIP}}:{{index .spec.ports 0 "port"}}/healthz'`
----

. You should see something similar to this:
+
----
 * About to connect() to 172.30.42.118 port 5000 (#0)
*   Trying 172.30.42.118...
* Connected to 172.30.42.118 (172.30.42.118) port 5000 (#0)
> GET /healthz HTTP/1.1
> User-Agent: curl/7.29.0
> Host: 172.30.42.118:5000
> Accept: */*
>
< HTTP/1.1 200 OK
< Content-Type: application/json; charset=utf-8
< Docker-Distribution-Api-Version: registry/2.0
< Date: Thu, 26 Nov 2015 06:56:11 GMT
< Content-Length: 3
<
{}
* Connection #0 to host 172.30.42.118 left intact

----

.Deploy the default router


. Create A CA Certificate for the default router
----
[root@master00-GUID ~]# CA=/etc/origin/master
[root@master00-GUID ~]# oadm ca create-server-cert --signer-cert=$CA/ca.crt \
       --signer-key=$CA/ca.key --signer-serial=$CA/ca.serial.txt \
       --hostnames='*.cloudapps-$guid.oslab.opentlc.com' \
       --cert=cloudapps.crt --key=cloudapps.key
----

. Combine `cloudapps.crt` and `cloudapps.key` with the CA into a single PEM
  format file that the router needs in the next step.
+
----
[root@master00-GUID ~]# cat cloudapps.crt cloudapps.key $CA/ca.crt > /etc/origin/master/cloudapps.router.pem
----

. Deploy the *Default Router*
+
----
[root@master00-GUID ~]#  oadm router trainingrouter --replicas=1 \
  --credentials='/etc/origin/master/openshift-router.kubeconfig' \
  --service-account=router --stats-password='r3dh@t1!'

----


. You should see the following output:
+
----
password for stats user admin has been set to r3dh@t1!
DeploymentConfig "trainingrouter" created
Service "trainingrouter" created

----

.. In the seperate terminal watch the status of your pods:
+
----
[root@master00-06d0 ~]# oc get pods -w
NAME                      READY     STATUS    RESTARTS   AGE
docker-registry-1-diqlc   1/1       Running   0          11m
router-1-mpzxx            1/1       Running   0          23s


----

.. You would probably also have the Docker registry pods listed in the output above.

.. Press *CTRL+C* to exit the watch on `oc get pods`.




=== Populate OpenShift Enterprise (Reference Only)

OpenShift Enterprise ships with _image streams_ and _templates_.
 They reside in `/usr/share/openshift/examples/`.  The installer imports all the
  image streams and templates for you from this directory.

* Take a look at the JSON files in `/usr/share/openshift/examples`.

[IMPORTANT]
The remaining steps in this lab are for reference only. You would run the commands shown only if you needed to perform the described task for some reason.

* To create or remove the core set of image streams that use images based on Red Hat Enterprise Linux 7:
+
----

 oc create|delete -f /usr/share/openshift/examples/image-streams/image-streams-rhel7.json -n openshift
----

* To create or remove the core set of database templates:
+
----
 oc create|delete or remove -f /usr/share/openshift/examples/db-templates -n openshift
----

* To create or remove the core QuickStart templates:
+
----
 oc create|delete -f /usr/share/openshift/examples/quickstart-templates -n openshift
----


== Lab: Setting Up Persistent Storage

Having a database for development is nice, but what if you actually want the
 data you store to persist after you redeploy the database pod? Pods are
  ephemeral, and, by default, so is their storage. For shared or persistent
   storage, you need a way to specify that pods should use external volumes.

For the purposes of this training, we will just demonstrate the *oselab* host
 exporting an NFS volume for use as storage by the database.

=== Prepare for NFS Persistent Storage back-end

. As `root` on the "oselab" host, ensure that `nfs-utils` is installed on _all_ nodes.
+
----
[root@oselab-GUID ~]# for node in infranode00-$guid.oslab.opentlc.com \
                                    node00-$guid.oslab.opentlc.com \
                                    node01-$guid.oslab.opentlc.com; \
                                    do \
                                     echo installing nfs-utils on $node
                                     ssh $node "yum -y install nfs-utils" ;
                                    done

----

=== Export an NFS Volume for Persistent Storage

On the `oselab` admin host, create a directory for each volume that you wish to export via NFS.

. Create 100 directory exports to use as persistent volumes.
+
----
[root@oselab-GUID ~]# mkdir -p /var/export/pvs/pv{1..100}
[root@oselab-GUID ~]# chown -R nfsnobody:nfsnobody /var/export/pvs/
[root@oselab-GUID ~]# chmod -R 700 /var/export/pvs/

----

. Add a line for each export directory to `/etc/exports`:
+
----

[root@oselab-GUID ~]# for volume in pv{1..100} ; do
echo Creating export for volume $volume;
echo "/var/export/pvs/${volume} 192.168.0.0/24(rw,sync,all_squash)" >> /etc/exports;
done;

----

. Enable and start NFS services.
+
----

[root@oselab-GUID ~]# systemctl enable rpcbind nfs-server
[root@oselab-GUID ~]# systemctl start rpcbind nfs-server nfs-lock nfs-idmap
[root@oselab-GUID ~]# systemctl stop firewalld
[root@oselab-GUID ~]# systemctl disable firewalld

----
+
NOTE: the volume is owned by `nfsnobody`, and access by all remote users is
 "squashed" (using the `all_squash` command) to be access by this user. This
  essentially disables user permissions for clients mounting the volume. While
   another configuration might be preferable, one problem you may run into is
    that the container cannot modify the permissions of the actual volume
     directory when mounted. In the case of MySQL below, MySQL wants the volume
      to belong to the `mysql` user and assumes that it is, which causes
       problems later. Arguably, the container should operate differently.
        In the long run, Red Hat may work to come up with best practices for
         use of NFS from containers.


=== Allow NFS Access in SELinux Policy

By policy default, containers are not allowed to write to NFS mounted
 directories. You want to allow this for some of your pods.

. To allow containers to write to NFS mounted directories on all nodes where the
 pod could land (i.e., all of them):
+
----

[root@oselab-GUID ~]#  for node in infranode00-$guid.oslab.opentlc.com \
                                   node00-$guid.oslab.opentlc.com \
                                   node01-$guid.oslab.opentlc.com; \
                                   do
                                     echo Setting SElinux Policy on $node
                                     ssh $node " setsebool -P virt_use_nfs=true;"
                                   done
----
+
[NOTE]
Once the Ansible-based installer performs this task automatically, this step
 will be removed from the lab.


 === Verify NFS Access

. Connect to one of your nodes, and check that you can succesfully mount a
  volume from the "oselab" host
+
----
[root@oselab-GUID ~]# ssh 192.168.0.20x
[root@node0X-GUID ~]# mkdir /tmp/test
[root@node0X-GUID ~]# mount -v 192.168.0.254:/var/export/pvs/pv98 /tmp/test
# Check if any errors accure and unmount.
[root@node0X-GUID ~]# umount /tmp/test
[root@node0X-GUID ~]# exit
----


=== Create Definition Files for Your Volumes

. Connect to the "master00" host
+
----
[root@oselab-GUID ~]# ssh master00-$guid
----

. Create a directory to store definition files for persistent volumes (`pvs`) in
 your environment.
+
----
[root@master00-GUID ~]# mkdir /root/pvs
----
. Create 25 `PersistentVolumes` (`pv1` to `pv25`) with the size of 5 gigabytes.
+
----

[root@master00-GUID ~]# export volsize="5Gi"
[root@master00-GUID ~]# for volume in pv{1..25} ; do
cat << EOF > /root/pvs/${volume}
{
  "apiVersion": "v1",
  "kind": "PersistentVolume",
  "metadata": {
    "name": "${volume}"
  },
  "spec": {
    "capacity": {
        "storage": "${volsize}"
    },
    "accessModes": [ "ReadWriteOnce" ],
    "nfs": {
        "path": "/var/export/pvs/${volume}",
        "server": "192.168.0.254"
    },
    "persistentVolumeReclaimPolicy": "Recycle"
  }
}
EOF
echo "Created def file for ${volume}";
done;
----

. Create 25 additional `PersistentVolumes` (`pv26` to `pv50`) with the size of 10 gigabytes.
+
----

[root@master00-GUID ~]# export volsize="10Gi"
[root@master00-GUID ~]# for volume in pv{26..50} ; do
cat << EOF > /root/pvs/${volume}
{
  "apiVersion": "v1",
  "kind": "PersistentVolume",
  "metadata": {
    "name": "${volume}"
  },
  "spec": {
    "capacity": {
        "storage": "${volsize}"
    },
    "accessModes": [ "ReadWriteOnce" ],
    "nfs": {
        "path": "/var/export/pvs/${volume}",
        "server": "192.168.0.254"
    },
    "persistentVolumeReclaimPolicy": "Recycle"
  }
}
EOF
echo "Created def file for ${volume}";
done;
----

. Create 50 `PersistentVolumes` (`pv51` to `pv100`) with the size of 1 gigabyte.
+
----

[root@master00-GUID ~]# export volsize="1Gi"
[root@master00-GUID ~]# for volume in pv{51..100} ; do
cat << EOF > /root/pvs/${volume}
{
  "apiVersion": "v1",
  "kind": "PersistentVolume",
  "metadata": {
    "name": "${volume}"
  },
  "spec": {
    "capacity": {
        "storage": "${volsize}"
    },
    "accessModes": [ "ReadWriteOnce" ],
    "nfs": {
        "path": "/var/export/pvs/${volume}",
        "server": "192.168.0.254"
    },
    "persistentVolumeReclaimPolicy": "Recycle"
  }
}
EOF
echo "Created def file for ${volume}";
done;
----

. Allocate three volumes, 5 gigabytes each, to the `default` project.
+
----
[root@master00-GUID ~]# cd /root/pvs
[root@master00-GUID ~]# cat pv21 pv22 pv23 | oc create -f - -n default
----

. To see that your `pvs` were added and are available, run `oc get pvs`.
+
----
[root@master00-GUID pvs]# oc get pv
NAME               LABELS    CAPACITY      ACCESSMODES   STATUS      CLAIM                    REASON
pv21               <none>    5368709120    RWO           Available
pv22               <none>    5368709120    RWO           Available
pv23               <none>    5368709120    RWO           Available
----

[NOTE]
Although this process is fairly manual now, one could easily automate this
 process to create a volume on request.

NOTE: At this point, you have created the infrastructure for using persistent volumes
 but have not used it. You will use these exports (volumes) in upcoming labs.
