:toc2:

:numbered:

== Resource Management

In this module we have the following labs:

* Lab: Users, Projects and Quotas
* Creating Services and Routes
* Taking a tour into a container

== Lab: Users, Projects and Quotas
=== Create Projects

. On the master host use the `oadm` command to create a project, and assign an administrative user to it:
+
----

[root@master00-GUID ~]# oadm new-project resourcemanagement --display-name="Resources Management" \
    --description="This is the project we use to learn about resource management" \
    --admin=andrew  --node-selector='region=primary'

----

=== A look at resources in the Web Console

Now that you have a project created, it's time to look at the web console, which
has been completely redesigned for V3.

. Open your desktop/laptop web browser and visit the following URL:
+
----

https://master00-GUID.oslab.opentlc.com:8443

----
+
[NOTE]
Be aware that it may take up to 90 seconds for the web console to be available
any time you restart the master.

. On your first visit your browser will need to accept the self-signed SSL
certificate.

. You will be asked for a username and a password. Remembering
that we created a user previously, `andrew`, go ahead and enter that and use
the password (`r3dh4t1!`) you set earlier.

. Once you are in, click the *"Resources Management"* project. There really isn't
anything of interest at the moment, because we haven't put anything into our
project.


=== Applying Quota to Projects

At this point we have created our "Resource Management" project, so let's apply the quota above
to it.

. Create a Quota definition file
+
----

[root@master00-GUID ~]# cat << EOF > quota.json
{
  "apiVersion": "v1",
  "kind": "ResourceQuota",
  "metadata": {
    "name": "test-quota"
  },
  "spec": {
    "hard": {
      "memory": "1Gi",
      "cpu": "20",
      "pods": "3",
      "services": "5",
      "replicationcontrollers":"5",
      "resourcequotas":"1"
    }
  }
}
EOF

----

. On the master host apply the file you just created with the `oc create` command:
+
----

[root@master00-GUID ~]# oc create -f quota.json --namespace=resourcemanagement

----

. On the master host make sure it was created:
+
----

[root@master00-GUID ~]# oc get -n resourcemanagement quota

----
+
----

NAME
test-quota

----

. On the master host verify limits and examine usage:
+
----

[root@master00-GUID ~]# oc describe quota test-quota -n resourcemanagement

----
+
----

Name:                   test-quota
Resource                Used    Hard
--------                ----    ----
cpu                     0       20
memory                  0       1Gi
pods                    0       3
replicationcontrollers  0       5
resourcequotas          1       1
services                0       5

----

. Go back into the web console and click into the "Resource Management"
project.

. Click on the *Settings* tab and you'll see that the quota information
is displayed.

[NOTE]
Once creating the quota, it can take a few moments for it to be fully
processed. If you get blank output from the `get` or `describe` commands, wait a
few moments and try again.

=== Applying Limit Ranges to Projects

In order for quotas to be effective you need to also create Limit Ranges
which set the maximum, minimum, and default allocations of memory and cpu at
both a pod and container level. Without default values for containers projects
with quotas will fail because the deployer and other infrastructure pods are
unbounded and therefore forbidden.

. Create the Limits file
+
----
[root@master00-GUID ~]# cat << EOF > limits.json
{
    "kind": "LimitRange",
    "apiVersion": "v1",
    "metadata": {
        "name": "limits",
        "creationTimestamp": null
    },
    "spec": {
        "limits": [
            {
                "type": "Pod",
                "max": {
                    "cpu": "500m",
                    "memory": "750Mi"
                },
                "min": {
                    "cpu": "10m",
                    "memory": "5Mi"
                }
            },
            {
                "type": "Container",
                "max": {
                    "cpu": "500m",
                    "memory": "750Mi"
                },
                "min": {
                    "cpu": "10m",
                    "memory": "5Mi"
                },
                "default": {
                    "cpu": "100m",
                    "memory": "100Mi"
                }
            }
        ]
    }
}
EOF


----

. On the master host run `oc create` against the `limits.json` file and the "resourcemanagement" project
+
----

[root@master00-GUID ~]# oc create -f limits.json --namespace=resourcemanagement

----

. Review your limit ranges on the master host:
+
----

[root@master00-GUID ~]# oc describe limitranges limits -n resourcemanagement

----
+
----

Name:           limits
Type            Resource        Min     Max     Default
----            --------        ---     ---     ---
Pod             memory          5Mi     750Mi   -
Pod             cpu             10m     500m    -
Container       cpu             10m     500m    100m
Container       memory          5Mi     750Mi   100Mi

----

=== Test your Quotas

.Authenticate to OpenShift Enterprise and Choose Your Project

. Connect to the OpenShift Enterprise master by following the same steps you used previously.
. Authenticate user `andrew` to Openshift Enterprise (Password is: `r3dh4t1!`)

+
----

[root@master00-GUID ~]# su - andrew
[andrew@master00-GUID ~]$ oc login -u andrew --insecure-skip-tls-verify --server=https://master00-${guid}.oslab.opentlc.com:8443

----
+
You will See
+
----
Password: (Enter r3dh4t1!)
Login successful.
Welcome to OpenShift! See 'oc help' to get started.
----


.Create the Pod Definition

Run the following command to create the `hello-pod.json` file:

----

[andrew@master00-GUID ~]$ cat <<EOF > hello-pod.json
{
  "kind": "Pod",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-openshift",
    "creationTimestamp": null,
    "labels": {
      "name": "hello-openshift"
    }
  },
  "spec": {
    "containers": [
      {
        "name": "hello-openshift",
        "image": "openshift/hello-openshift:v0.4.3",
        "ports": [
          {
            "hostPort": 36061,
            "containerPort": 8080,
            "protocol": "TCP"
          }
        ],
        "resources": {
          "limits": {
            "cpu": "10m",
            "memory": "16Mi"
          }
        },
        "terminationMessagePath": "/dev/termination-log",
        "imagePullPolicy": "IfNotPresent",
        "capabilities": {},
        "securityContext": {
          "capabilities": {},
          "privileged": false
        },
        "nodeSelector": {
          "region": "primary"
        }
      }
    ],
    "restartPolicy": "Always",
    "dnsPolicy": "ClusterFirst",
    "serviceAccount": ""
  },
  "status": {}
}

EOF

----

=== Run the Pod

We will now create a simple pod without a *route* or a *service*

. Run the following commands to create and verify the pod:
+
----

[andrew@master00-GUID ~]$ oc create -f hello-pod.json
pods/hello-openshift

[andrew@master00-GUID ~]$ oc get pods
NAME              READY     REASON    RESTARTS   AGE
hello-openshift   1/1       Running   0          2m

----

. Run the *oc describe* command to learn about your pod.
+
----
[andrew@master00-GUID ~]$  oc describe pod hello-openshift
Name:                           hello-openshift
Image(s):                       openshift/hello-openshift:v0.4.3
Host:                           node01-f4fc.oslab.opentlc.com/192.168.0.201
Labels:                         name=hello-openshift
Status:                         Running
IP:                             10.1.1.2
Replication Controllers:        <none>
Containers:
  hello-openshift:
    Image:              openshift/hello-openshift:v0.4.3
    State:              Running
      Started:          Thu, 02 Jul 2015 02:42:50 -0400
    Ready:              True
    Restart Count:      0
Conditions:
  Type          Status
  Ready         True
Events:
  .... "Successfully assigned hello-openshift to node01-f4fc.oslab.opentlc.com" ....

----
+
. Test that your pod is responding with "Hello OpenShift"
+
----

[andrew@master00-GUID ~]$ ip=`oc describe pod hello-openshift|grep IP:|awk '{print $2}'`
[andrew@master00-GUID ~]$ curl http://${ip}:8080

----
+
You will see:
+
----
Hello OpenShift!
----

. Great, the pod works, Now, lets kill it and create a few more
+
----

[andrew@master00-GUID ~]$ oc delete -f hello-pod.json

----

. Create a new definition file that launches 4 hello-pods
+
----
[andrew@master00-GUID ~]$ cat << EOF > hello-many-pods.json
{
  "metadata":{
    "name":"quota-pod-deployment-test"
  },
  "kind":"List",
  "apiVersion":"v1",
  "items":[
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-1",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-2",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-3",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-4",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    }
  ]
}


EOF

----

. Create the items in the *hello-many-pods.json* file
+
----
[andrew@master00-GUID ~]$ oc create -f hello-many-pods.json
pods/hello-openshift-1
pods/hello-openshift-2
pods/hello-openshift-3
Error from server: Pod "hello-openshift-4" is forbidden: Limited to 3 pods
----

NOTE: Because we created a quota, the forth pod will not be created.

. Lets delete the objects and move on
+
----
[andrew@master00-GUID ~]$ oc delete  -f hello-many-pods.json
----

. *Optional* - Using what you have learned, create a new project, and set the quota so that the pods value is 10 and run the *hello-many-pods.json* again.

== Lab: Creating Services and Routes

. As root on the master host create a new project:
+
----

[andrew@master00-GUID ~]$ exit
[root@master00-GUID ~]# oadm new-project svcslab --display-name="Services Lab" \
    --description="This is the project we use to learn about services" \
    --admin=andrew  --node-selector='region=primary'
----

. Become the *andrew* user and log back into OpenShift and switch to the *svcslab* project:
+
----

[root@master00-GUID ~]# su - andrew
[andrew@master00-GUID ~]$ oc project svcslab
Now using project "svcslab" on server "https://master00-GUID.oslab.opentlc.com:8443".

----

. Run the following command to create the `hello-service.json` file:
+
----

[andrew@master00-GUID ~]$  cat <<EOF > hello-service.json
{
  "kind": "Service",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-service"
  },
  "spec": {
    "selector": {
      "name":"hello-openshift"
    },
    "ports": [
      {
        "protocol": "TCP",
        "port": 8888,
        "targetPort": 8080
      }
    ]
  }
}
EOF

----
+
. Run the following commands to create and verify the pod:
+
----

[andrew@master00-GUID ~]$ oc create -f hello-service.json
services/hello-service

----
+
. Display the running services (under the current project)
+
----

[andrew@master00-GUID ~]$ oc get services
NAME            LABELS    SELECTOR               IP(S)          PORT(S)
hello-service   <none>    name=hello-openshift   172.30.xxx.yyy   8888/TCP

----
+
. Lets look at the details of our service, Please notice the *selector* and the *Endpoints* lines.
.. The *selector* describes which pods should be "selected" or "listed" by the service.
.. The *Endpoints* line lists all the pods that are currently listed, notice that we have none.
+
----
[andrew@master00-GUID ~]$ oc describe service hello-service
Name:                   hello-service
Labels:                 <none>
Selector:               name=hello-openshift
Type:                   ClusterIP
IP:                     172.30.xxx.yyy
Port:                   <unnamed>       8888/TCP
Endpoints:              <none>
Session Affinity:       None
No events.
----

. Lets create some pods
----

[andrew@master00-GUID ~]$ oc create -f hello-many-pods.json

----

. Now lets check the service again, you can see that the pods who share the label "name=hello-service" are all listed.
+
----

[andrew@master00-GUID ~]$ oc get service
NAME            LABELS    SELECTOR               IP(S)          PORT(S)
hello-service   <none>    name=hello-openshift   172.30.5.240   8888/TCP

[andrew@master00-GUID ~]$ oc describe service hello-service
Name:                   hello-service
Labels:                 <none>
Selector:               name=hello-openshift
Type:                   ClusterIP
IP:                     172.30.5.240
Port:                   <unnamed>       8888/TCP
Endpoints:              10.1.0.4:8080,10.1.1.5:8080,10.1.1.7:8080
Session Affinity:       None
No events.

----

. Lets test our service
+
----

[andrew@master00-GUID ~]$ ip=`oc describe service hello-service|grep IP:|awk '{print $2}'`
[andrew@master00-GUID ~]$ curl http://${ip}:8888
Hello OpenShift!

----

. Create the Route
+
----
[andrew@master00-GUID ~]$ oc expose service/hello-service --hostname=hello2-openshift.cloudapps-${guid}.oslab.opentlc.com
----
+

. Lets see our routes
+
----
[andrew@master00-6b80 ~]$ oc get routes
NAME            HOST/PORT                                           PATH      SERVICE         LABELS
hello-service   hello2-openshift.cloudapps-GUID.oslab.opentlc.com             hello-service
----

. Test Route:
+
----

[andrew@master00-GUID ~]$ curl http://hello2-openshift.cloudapps-${guid}.oslab.opentlc.com
Hello OpenShift!

----

== Lab: Taking a tour into a container

We can have a look into the container using the *oc exec* command, In this
section we will have a look at our *Router* and *Registry*

. As Andrew, Lets run the following commands, We will learn more about this later,
for now, just enjoy the ride. (Make sure you are in the *svcslab* project)
----
[andrew@master00-d9b2 ~]$ oc new-app https://github.com/openshift/simple-openshift-sinatra-STI.git -l "todelete=yes" -o yaml |  sed 's/replicas: 1/replicas: 3/g' | oc create -f -
imagestreams/simple-openshift-sinatra-sti
buildconfigs/simple-openshift-sinatra-sti
deploymentconfigs/simple-openshift-sinatra-sti
services/simple-openshift-sinatra

[andrew@master00-d9b2 ~]$ oc expose service simple-openshift-sinatra --hostname=whatever.com

----


. As *root*, Lets connect to the *Router*, by finding out it's name and running the
*oc exec* command.
+
----
[root@master00-d9b2 ~]# oc get pods
NAME                      READY     REASON    RESTARTS   AGE
docker-registry-2-snarn   1/1       Running   0          17h
trainingrouter-1-jm5zk    1/1       Running   0          18h
[root@master00-d9b2 ~]# oc exec -ti -p trainingrouter-1-jm5zk /bin/bash
bash-4.2$

#Another option is:
[root@master00-d9b2 ~]# oc exec -ti -p `oc get pods |  awk '/route/ { print $1; }'` /bin/bash
bash-4.2$
----

. Once you are running *bash* inside the container you can explore
.. Run the *id* command
.. Run *pwd* and *ls*, what directory are you in?
.. Run *cat haproxy.config* to see your empty configuration file.
+
----

bash-4.2$ id
uid=1000010000 gid=0(root)

bash-4.2$ pwd
/var/lib/haproxy/conf

bash-4.2$ ls
default_pub_keys.pem     os_edge_http_be.map  os_sni_passthrough.map
haproxy-config.template  os_http_be.map       os_tcp_be.map
haproxy.config           os_reencrypt.map

bash-4.2$ grep SERVERID *
haproxy.config:    cookie OPENSHIFT_default-docker-registry_SERVERID insert indirect nocache httponly
haproxy.config:    cookie OPENSHIFT_resourcemanagement-hello-service_SERVERID insert indirect nocache httponly
haproxy.config:    cookie OPENSHIFT_svcslab-simple-openshift-sinatra_SERVERID insert indirect nocache httponly

bash-4.2$ cat haproxy.config

----

. You will see output similar to this:
.. You will see the *route* that was created in the previous lab.
.. Notice that the *route* points to the endpoints directly
.. Notice the *svcslab* project route we created exists, but would probably not
have any end-points until the build is complete
+
----
backend be_http_resourcemanagement-hello-service

  mode http
  balance leastconn
  timeout check 5000ms

    cookie OPENSHIFT_resourcemanagement-hello-service_SERVERID insert indirect n
ocache httponly

  server 10.1.2.2:8080 10.1.2.2:8080 check inter 5000ms cookie 10.1.2.2:8080
  server 10.1.2.3:8080 10.1.2.3:8080 check inter 5000ms cookie 10.1.2.3:8080
  server 10.1.2.4:8080 10.1.2.4:8080 check inter 5000ms cookie 10.1.2.4:8080
...
...

----

. Make changes to your service/route (add or remove pods or create another route)
and see the changes in the *haproxy.config* file.

. Before we start looking at the *registry* container, lets make sure our build
(from earlier in this lab) has completed.
. As Andrew,run this command to see the build (More on this later), this will take a while
on our hardware, If its not completed, it's good time for a quick break.
+
----
[andrew@master00-d9b2 ~]$ oc build-logs simple-openshift-sinatra-sti-1
...
...
...
I0810 21:50:39.236169       1 sti.go:134] Pushing 172.30.236.109:5000/svcslab/simple-openshift-sinatra-sti image ...
I0810 21:53:13.659295       1 sti.go:138] Successfully pushed 172.30.236.109:5000/svcslab/simple-openshift-sinatra-sti

----

. Once the build has completed and Successfully pushed the image to the
registry, we can continue the lab and go have a look inside the registry.
. As root, run the following commands:
.
----
[root@master00-d9b2 ~]# oc exec -ti -p `oc get pods |  awk '/registry/ { print $1; }'` /bin/bash

----

. Once you are running *bash* inside the container you can explore
.. Run the *id* command
.. Run *pwd* and *ls*, what directory are you in?
.. Run *cat haproxy.config* to see your empty configuration file.
+
----
bash-4.2$ id
uid=1000010000 gid=0(root)
bash-4.2$ pwd
/
bash-4.2$ ls
bin   config.yml  etc   lib    media  opt   registry  run   srv  tmp  var
boot  dev         home  lib64  mnt    proc  root      sbin  sys  usr
bash-4.2$ cat config.yml
version: 0.1
log:
  level: debug
http:
  addr: :5000
storage:
  cache:
    layerinfo: inmemory
  filesystem:
    rootdirectory: /registry
auth:
  openshift:
    realm: openshift
middleware:
  repository:
    - name: openshift

----

. You can have a look at the repositories and images available using the following:
+
----
bash-4.2$ cd /registry/docker/registry/v2/repositories
bash-4.2$ ls
svcslab
bash-4.2$ ls svcslab/
simple-openshift-sinatra-sti
bash-4.2$ ls svcslab/simple-openshift-sinatra-sti/
_layers  _manifests  _uploads
bash-4.2$ ls svcslab/simple-openshift-sinatra-sti/_layers/
sha256
bash-4.2$ ls svcslab/simple-openshift-sinatra-sti/_layers/sha256/
39886d6f6998b59a31e853bf1fcc642e40a711d67248904b23647afcb2dae085
c0e305bb0b350a4efcaeb33e1f99efe5235728747d3695b16b111fff7fb40e74
f1689e5704ab6738da07deea58081b784b9e43675063d1e98402ef3c745cd631
/var/export/registry-storage/docker/registry/v2/blobs
----
. If you want to see the size of the "blobs" that the layers are saves as:
.. Note that the *blob file* name is the same as your layer *link name*
+
----
bash-4.2$ cd /registry/docker/registry/v2/blobs/sha256/
bash-4.2$ du -sh *
54M     39
4.0K    7b
1.3M    c0
81M     f1
bash-4.2$ ls f1
f1689e5704ab6738da07deea58081b784b9e43675063d1e98402ef3c745cd631

----

NOTE: If you configured Persistent storage for your registry, you could see the
same in:  */var/export/registry-storage/docker/registry/v2/*

. As Andrew, lets have a look at one of the pods we started earlier in this lab.
+
----
[andrew@master00-d9b2 ~]$ oc get pods
NAME                                   READY     REASON       RESTARTS   AGE
simple-openshift-sinatra-sti-1-build   0/1       ExitCode:0   0          32m
simple-openshift-sinatra-sti-2-2ppvr   1/1       Running      0          29m
simple-openshift-sinatra-sti-2-ehdke   1/1       Running      0          29m
simple-openshift-sinatra-sti-2-qesjy   1/1       Running      0          29m
----

. Run the following command to connect to the container
+
----
[andrew@master00-d9b2 ~]$  oc exec -ti -p  simple-openshift-sinatra-sti-2-ehdke "/bin/bash"
bash-4.2$
----

. Explore the container:
.. Run the *id* command
.. Run *pwd* and *ls*, what directory are you in?
.. Run *ps -ef* to see what processes are running
+

+
----

bash-4.2$ id
uid=1000040000 gid=0(root)

bash-4.2$ ls
Gemfile  Gemfile.lock  app.rb  bundle  config.ru

bash-4.2$ pwd
/opt/openshift/src

bash-4.2$ ps -ef
UID         PID   PPID  C STIME TTY          TIME CMD
1000040+      1      0  0 21:53 ?        00:00:01 ruby /opt/openshift/src/bundle
1000040+     34      0  0 22:21 ?        00:00:00 /bin/bash
1000040+     62     34  0 22:21 ?        00:00:00 ps -ef

----

NOTE: Your pod names and output will differ slightly.



== Lab: Create a Persistent Volume for the Registry

In this lab we will create a persistent volume for our registry, attach it to
the deploymentConfiguration and redeploy the Registry.

=== Create NFS export for registry

. as root, on the *oselab* host, create a directory for our *NFS export*
+
----
[root@oselab-GUID ~]# export volname=registry-storage
[root@oselab-GUID ~]# mkdir -p /var/export/pvs/${volname}
[root@oselab-GUID ~]# chown nfsnobody:nfsnobody /var/export/pvs/${volname}
[root@oselab-GUID ~]# chmod 700 /var/export/pvs/${volname}
----

. Add the following line to `/etc/exports`:
+
----
[root@oselab-GUID ~]# echo "/var/export/pvs/${volname} *(rw,sync,all_squash)" >> /etc/exports
----

. Restart NFS services:
+
----
[root@oselab-GUID ~]# systemctl restart rpcbind nfs-server nfs-lock nfs-idmap
----


. Back on the *master* host, create a Persistent volume definition file:
+
[source,json]
----
[root@master00-GUID ~]# cat << EOF > registry-volume.json
    {
      "apiVersion": "v1",
      "kind": "PersistentVolume",
      "metadata": {
        "name": "registry-storage"
      },
      "spec": {
        "capacity": {
            "storage": "15Gi"
            },
        "accessModes": [ "ReadWriteMany" ],
        "nfs": {
            "path": "/var/export/pvs/registry-storage",
            "server": "oselab-${GUID}.oslab.opentlc.com"
        }
      }
    }

EOF

----

. Create the Persistent Volume from definition file (in the `default` project):
- Note that we are creating the PV in the `default` project because its the
project that the registry runs in.
+
----
[root@master00-GUID ~]# oc create -f registry-volume.json -n default
persistentvolumes/registry-storage
----

. View your create Persistent Volume
+
----
[root@master00-GUID ~]# oc get pv
NAME               LABELS    CAPACITY      ACCESSMODES   STATUS      CLAIM     REASON
registry-storage   <none>    16106127360   RWX           Available
----

. Create a claim definition file to claim your volume
+
----

[root@master00-GUID ~]# cat << EOF > registry-volume-claim.json
    {
      "apiVersion": "v1",
      "kind": "PersistentVolumeClaim",
      "metadata": {
        "name": "registry-claim"
      },
      "spec": {
        "accessModes": [ "ReadWriteMany" ],
        "resources": {
          "requests": {
            "storage": "15Gi"
          }
        }
      }
    }

EOF

----

. Create the Claim from definition file:
+
----
[root@master00-GUID ~]# oc create -f registry-volume-claim.json -n default
persistentvolumeclaims/registry-claim
----

. View your create Persistent Volume, notice that the status is "Bound"
+
----
[root@master00-GUID ~]# oc get pv
NAME               LABELS    CAPACITY      ACCESSMODES   STATUS    CLAIM                    REASON
registry-storage   <none>    16106127360   RWX           Bound     default/registry-claim

----

. View your create Persistent Volume Claims, notice that the status is "Bound"
+
----
[root@master00-GUID ~]# oc get pvc
NAME             LABELS    STATUS    VOLUME
registry-claim   map[]     Bound     registry-storage
----

=== Attach the Persistent Volume to the Registry

. Assuming that your registry is already running, get the names of your
available *deploymentConfigurations*
+
----
[root@master00-GUID ~]# oc get dc
NAME              TRIGGERS       LATEST VERSION
docker-registry   ConfigChange   1
trainingrouter    ConfigChange   1

----

. you can modify the *DeploymentConfiguration* using the *oc volume* command.
Add the volume to the Registry's _DeploymentConfiguration_, this will trigger
a redeployment of the registry.

----
oc volume dc/docker-registry --add --overwrite -t persistentVolumeClaim \
--claim-name=registry-claim --name=registry-storage
----

. Once you create an S2I build you could see the registry is using the persistent volume.
+
----

[root@oselab-GUID ~]# find /var/export/pvs/registry-storage
/var/export/registry-storage/docker/registry/v2/repositories
/var/export/registry-storage/docker/registry/v2/repositories/svcslab
/var/export/registry-storage/docker/registry/v2/repositories/svcslab/simple-openshift-sinatra-sti
/var/export/registry-storage/docker/registry/v2/repositories/svcslab/simple-openshift-sinatra-sti/_uploads
/var/export/registry-storage/docker/registry/v2/repositories/svcslab/simple-openshift-sinatra-sti/_layers
/var/export/registry-storage/docker/registry/v2/repositories/svcslab/simple-openshift-sinatra-sti/_layers/sha256
...
...
...

----
