== &nbsp;
:noaudio:

ifdef::revealjs_slideshow[]

[#cover,data-background-image="image/1156524-bg_redhat.png" data-background-color="#cc0000"]


[#cover-h1]
Red Hat OpenShift Enterprise Implementation

[#cover-h2]
OpenShift 3.0 Installation

[#cover-logo]
image::{revealjs_cover_image}[]

endif::[]

== Module Topics
:noaudio:

* Installation Overview
* Prepare your Hosts
* Installation Methods
* Scheduler - Regions and Zones
* The Registry and Router
* Populating OpenShift
* Configure Persistent Storage
* Lab : Prepare to Deploy OpenShift
* Lab : Install OpenShift Using the installer
* Lab : OpenShift Configuration and Setup
* Lab : Configuring Authentication
* Lab : Using Persistent Storage


ifdef::showscript[]


=== Transcript
Welcome to Module 03 of the OpenShift Enterprise Implementation course.

In this module we will review the installation process and learn how to
configure the Scheduler, Registry and Router containers and set-up persistent
storage

endif::showscript[]

== Installation Scenario
:noaudio:

In this training we will review the install of the following environment

* Master00 [192.168.0.100] - Our master will host the Web Console, *The API
service* and the *etcd store*. Our master is unschedulable so there is no need to
label it.
** Labels : Region=*NA*, Zone=*NA*, schedulable=`false`
* InfraNode00 [192.168.0.101] - The InfraNode00 is to be used only to host
"Infrastructure" containers such as the *Registry* and our HAproxy *Router*.
** Labels : Region=*Infra*, Zone=*infranode*, schedulable=*true*
* Node00 [192.168.0.200] - Part of the Primary Region, Will co-host all the
user workloads.
** Labels : Region=*Primary*, Zone=*East*, schedulable=*true*
* Node01  [192.168.0.201] - Part of the Primary Region, Will co-host all the
user workloads.
** Labels : Region=*Primary*, Zone=*West*, schedulable=*true*
* oselab  [192.168.0.254] - This server will simulate our corporate DNS server
and NFS backend.

ifdef::showscript[]

=== Transcript
In our lab we will be using the following hosts to simulate a regular
environment.

Our *master* host will be used to host some of the management components of
OpenShift Enterprise, Components such as the Web Console, *The API service* and
the *etcd store*

*Infranode* is a regular node like the others but by changing the labels applied
to the node we are going to dedicate it to be used only for "Infrastructure
containers". This is a design choice and not a mandatory constraint.

the *nodes*, nodes are hosts that are used to run containers (Pods) in the OSE
environment, we label the two nodes to be in the same region but different
zones. This is to simulate a use-case of an environment in a single region and
possibly two cloud availability zones

endif::showscript[]

== Installation Methods
:noaudio:

* Currently, There are two types of install methods you can use:
** *Quick Install* - The quick installation method allows you to use an
interactive CLI utility to install OpenShift across a set of hosts. The utility
is a self-contained wrapper intended for usage on a Red Hat Enterprise Linux 7
host, available at
link:https://install.openshift.com[https://install.openshift.com].
** *Advanced Install* - For more complex environments where deeper
customization of installation and maintenance is required, an installation
process using Ansible playbooks is available. Familiarity with Ansible is
assumed.

* In this training we will focus on the Quick Install Method.

ifdef::showscript[]

=== Transcript

There are two ways to install OSE3.0, the *Quick Install* method uses an
interactive CLI utility to install OpenShift, and the *Advanced Install* method
who uses the Ansible installer to configure and install OSE3.0

The *Quick install* also uses the Ansible install under the covers of the
interactive CLI utiliy.

endif::showscript[]

== Installation Workflow
:noaudio:

* Prerequisites
- System Requirements
- DNS Setup
- Host Preparation
* OpenShift Installation
- Downloading and Running the Installation Utility
* Post-Install tasks
- Deploy an integrated Docker registry.
- Deploy an HAProxy router.
- Populate your OpenShift installation with image streams and templates.
- Configure authentication and create project for your users.
- Set-up and configure NFS Server for use with `Persistent Volumes`

ifdef::showscript[]
=== Transcript

We will now review the OSE Installation workflow, first we will need to assess
if the System Prerequisites have been met, these include the basic requirements
for a viable OSE environment, these include setting up the DNS requirements, and
preparing the hosts for OpenShift deployment.

Next, we install the OpenShift software using the CLI installation utility.
Finally, we will deploy some containerized infrastructure components such as the
Default Router and the integrated Docker Registry. We will also configure
authentication and set-up an NFS Server to serve our persistent volume requests.

endif::showscript[]




== Prerequisites
:noaudio:

.System Requirements

* The following are the recommended system requirements for an OpenShift deployment.

* Masters
** Physical or virtual system
** Base OS: Red Hat Enterprise Linux (RHEL) 7.1 with "Minimal" installation option
** 2 vCPU
** Minimum 8 GB RAM
** Minimum 30 GB hard disk space

* Nodes

** Physical or virtual system, or an instance running on a public IaaS
** Base OS: Red Hat Enterprise Linux (RHEL) 7.1 with "Minimal" installation option
** Docker 1.6 or later
** 1 vCPU
** Minimum 8 GB RAM
** Minimum 15 GB hard disk space

* In our learning environment the servers are not configured with the
recommended settings to lower costs.


ifdef::showscript[]
=== Transcript

In this slide you can see the minimal requirements for a viable OSE environment.


endif::showscript[]
== Prerequisites
:noaudio:

.DNS Setup

* A wildcard for a DNS zone must ultimately resolve to the IP address of the
OpenShift router.
* For example, create a wildcard DNS entry for cloudapps in your DNS Server, or
something similar, that has a low TTL and points to the public IP address of the
host where the router will be deployed:
+
----
*.cloudapps.example.com. 300 IN  A 85.1.3.5
----

ifdef::showscript[]

=== Transcript

In order for our OSE environment to be accessible externally we will create a
wildcard DNS entry that points to the *node* that is hosting our *Default Router
Container*

In our lab and examples this is the *infranode00 server*, if your environments
uses multiple *routers* (HA-Proxy instances), which is likely, you will use an
external load balancer or round-robin setting to use them.

endif::showscript[]


== Host Preparation
:noaudio:

.Host Preparation - overview

* To prepare your hosts for OpenShift 3 Enterprise
** *Installing Red Hat Enterprise Linux 7.1* - A base installation of `Red Hat Enterprise Linux (RHEL) 7.1` is required for master or node hosts. See the link:https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Installation_Guide/index.html[Red Hat Enterprise Linux 7.1 Installation Guide]
 for more information.
** *Registering the Hosts with subscription-manager* - You will need to register all the hosts to RHEL7.1 and OpenShift Enterprise repositories.
** *Managing Base Packages* - You will need to install some utility packages (i.e git, net-tools, bind-utils, iptables-services
** *Managing Services* - You will need to disable firewalld and enable iptables-services
** *Install Docker 1.6.x* - Docker version 1.6 or later needs to be installed and storage backend configured for images.
** *Host Password-less communication* - You will ensure that the master hosts can issue remove commands on the nodes without requiring a password.

ifdef::showscript[]

=== Transcript

We will cover some of the basic steps that are required to prepare the hosts.

To prepare the hosts to be used with OpenShift its best to consult the
documentation as it would always have the most up-to-date information.

In case you want to create a scalable environment you will probably create a
image template for the node (depending on your infrastructure provider) and spin
up a new instance/VM in which these steps have already been performed (to save
  time) and then run the installer script to add the new node to the OSE
  environment/cluster.


endif::showscript[]

== Host Preparation
:noaudio:

.Host Preparation - Password-less communication

* Ensure Installer has password-less access to all Hosts.
* Ansible, which is used to run the installation process, requires a user that has access to all hosts. For running the installer as a non-root user, passwordless sudo rights must also be configured on each destination host.
* For example, you can generate an SSH key on the host where you will invoke the installation process:
+
----
# ssh-keygen
# ssh-copy-id root@node00.example.com
----

NOTE: Do not use a password when generating the key.

* An easy way to distribute your SSH keys is by using a bash loop:

ifdef::showscript[]

=== Transcript
For the installation to be successful the installer will need to be able to run
commands on each on of the members in the OSE environment without a user typing
in the password every time.

Do achieve this we will generate a key and copy it to the root user's id on each
member of the OSE cluster.

endif::showscript[]

== Host Preparation
:noaudio:

.Host Preparation - Firewalls
OpenShift relies heavily on iptables under the covers. As such, it must be
running, and various ports will need to be opened to allow communication between
 OpenShift components.

* Node-To-Node
- 4789 : required between nodes for SDN communication between pods on separate
 hosts

* Nodes-To-Master
- 53: DNS services within the environment (This is not the DNS for extarnal
  access)
- 4789 : required between nodes for SDN communication between pods on separate
 hosts
- 8443 : Access to the API

* Master-To-Node
- 10250 : endpoint for master communication with nodes
- 4789 : required between nodes for SDN communication between pods on separate
 hosts

* Master to Master
- 4789 : required between nodes for SDN communication between pods on separate
 hosts

ifdef::showscript[]

=== Transcript

The following ports need to be allowed between the different components of the
environment, note that port 4789 is required to be accessiable from any to any
host in the cluster as it is required for the SDN overlay.

The master is running a local DNS Server, do not confuse this server with the
DNS server that holds the wildcard DNS entry. This is a DNS server (SkyDNS) that
is used to resolve local resources, for example, each *service* we define in OSE
will have a dns entry that can be resolved locally.

endif::showscript[]




== Host Preparation
:noaudio:

.Host Preparation - Firewalls Continued
In case you want to allow external access to your environment, open the
following ports:

* External - Master
- 8443: CLI and IDE plugins communicate via REST to this port. Web console
runs on this port.

* External - Node (or nodes) hosting the *default Router* (HA-Proxy) container
- 80,443: Ports opened and bound to the *Default Router* container to proxy
communication from the external world to the *Pods* (Containters) internally.

ifdef::showscript[]

=== Transcript
If you want to allow users from outside your LAN to access the Web console or
make API calls to OSE, you need to expose the 8443 port of your master to the
desired networks.

The *default router* will be listening on its host's ports 80 and 443 for
incoming requests, to allow external access to your pods you only need to expose
the node hosting the *default router*.

Consider a topology where only the Infrastructure *nodes* are running in a DMZ
and the application hosting *nodes*, master and other supporting infrastructure
are running in a more secure network.

endif::showscript[]



== Host Preparation
:noaudio:

.Host Preparation - Networking and misc

* You would need to install the following software packages
+
----
# yum install wget git net-tools iptables-services python-virtualenv gcc
----

*  Update your software before installation
+
----
# yum update -y
----

ifdef::showscript[]

=== Transcript

You will need to install the following software packages  and run a "yum update"
 on your master before using the installer and installing OSE3.0.

endif::showscript[]




== Docker Install
:noaudio:


* Docker version 1.6 or later from the rhel-7-server-ose-3.0-rpms repository
must be installed and running on master and node hosts before installing
OpenShift.
* We will run through the following procedure:
** Install Docker:
+
----
# yum install docker
----

** Edit the /etc/sysconfig/docker file and add --insecure-registry 172.30.0.0/16
to the OPTIONS parameter. For example:
+
----
OPTIONS=--selinux-enabled --insecure-registry 172.30.0.0/16
----

** The --insecure-registry option instructs the Docker daemon to trust any
Docker registry on the 172.30.0.0/16 subnet, rather than requiring a certificate.
** Our local registry will be deployed under this subnet.


ifdef::showscript[]

=== Transcript
Each node will require *Docker* to be installed and configured, version 1.6 and
above is available from the OpenShift repository and must be used.
The ""--insecure-registry" option instructs the Docker daemon to trust any
Docker registry on the 172.30.0.0/16 subnet, rather than requiring a certificate.

Our local registry will be deployed under this subnet.


endif::showscript[]



== Docker Install
:noaudio:

.Configuring Docker Storage

* Docker’s default loopback storage mechanism is not supported for production
use and is only appropriate for proof of concept environments. For production
environments, you must create a thin-pool logical volume and re-configure docker
 to use that volume.
* You can use the "`docker-storage-setup`" script to create a thin-pool device
and configure docker’s storage driver after installing docker but before you
start using it.
* The script reads configuration options from the
"/etc/sysconfig/docker-storage-setup" file.

ifdef::showscript[]

=== Transcript

Docker’s default loopback storage mechanism is not supported for production
use and is only appropriate for proof of concept environments. For production
environments, you must create a thin-pool logical volume and re-configure docker
 to use that volume.

You can use the "`docker-storage-setup`" script to create a thin-pool device
and configure docker’s storage driver after installing docker but before you
start using it.

endif::showscript[]



== Docker Install
:noaudio:

* To Configure `docker-storage-setup` script for your environment. There are a
few options available based on your storage configuration:

* Before you start the `docker-storage-setup` script, re-initialize docker
+
----
# systemctl stop docker
# rm -rf /var/lib/docker/*
----

.Storage Options
* Create a thin-pool volume from the remaining free space in the volume group
where your root filesystem resides; this requires no configuration:
+
----
# docker-storage-setup
----

* Use an existing volume group, in this example docker-vg, to create a thin-pool:
+
----

# cat /etc/sysconfig/docker-storage-setup
VG=docker-vg
SETUP_LVM_THIN_POOL=yes
# docker-storage-setup
----

ifdef::showscript[]

=== Transcript

You must edit the */etc/sysconfig/docker-storage-setup* file to work as an
answer file for *docker-storage-setup*

endif::showscript[]


== Docker Install
:noaudio:

.Storage Options - Continued

* Use an unpartitioned block device to create a new volume group and thinpool.
In this example, the /dev/vdc device is used to create the docker-vg volume group:
+
----
# cat /etc/sysconfig/docker-storage-setup
DEVS=/dev/vdc
VG=docker-vg
SETUP_LVM_THIN_POOL=yes
# docker-storage-setup
----

* Verify your configuration. You should have dm.thinpooldev value in the
*/etc/sysconfig/docker-storage* file and a docker-pool device:
+
----
# lvs
LV                  VG        Attr       LSize  Pool Origin Data%  Meta% Move
docker-pool         docker-vg twi-a-tz-- 48.95g             0.00   0.44

# cat /etc/sysconfig/docker-storage
DOCKER_STORAGE_OPTIONS=--storage-opt dm.fs=xfs --storage-opt
dm.thinpooldev=/dev/mapper/docker--vg-docker--pool

----

* After you verified the setup, restart the *Docker* daemon

ifdef::showscript[]

=== Transcript

In this example we are using the /dev/vdc device create the docker-vg volume
group to be used by the docker Daemon.

We can verify that the volume is created and configured using the *lvs* command
and viewing the "/etc/sysconfig/docker-storage" file.

endif::showscript[]





== Installing OpenShift
:noaudio:

* The quick installer is provided at:
link:https://install.openshift.com[https://install.openshift.com]. Visit that
page for the latest information.

* Run the installation utility by executing the following commands on a host
that has SSH access to your intended master and nodes:
+
----
$ curl -o oo-install-ose.tgz \
    https://install.openshift.com/portable/oo-install-ose.tgz
$ tar -zxf oo-install-ose.tgz
$ ./oo-install-ose
----

* Follow the on-screen instructions to install a new OpenShift instance.
* The installer will ask you for Internal and Public IPs of your Masters and
Nodes and will configure them accordingly.

* If you want, you can just use the following command to download and run
the script in a single command:
+
----
$ sh <(curl -s https://install.openshift.com/ose/)
----

ifdef::showscript[]

=== Transcript
The latest installer is available in the link shown in this slide, download the
latest installer, unzip and run the installer CLI utility to install OSE3.0.

For stability's sake, and if you are intending to add more nodes down the line,
consider saving the installer script you used and using it for all future nodes
as well.

endif::showscript[]




== Regions and Zones
:noaudio:

* In OpenShift 2, we introduced the specific concepts of "regions" and "zones"
to enable organizations to provide some topologies for application resiliency.
** Apps would be spread throughout the zones within a region and, depending on
the way you configured OpenShift, you could make different regions accessible
to users.
* OpenShift 3 doesn’t actually care about your topology or is "topology
agnostic".
* OpenShift 3 provides advanced controls for implementing whatever topologies
you can dream up.
** For the purposes of a simple example, we’ll be sticking with the "regions"
and "zones" theme. (There are many other options you can use, "Prod and Dev",
"Secure and Insecure", "Rack and Power")
* The assignments of "regions" and "zones" at the node-level are handled by
labels on the nodes.
+
----
# oc label node master00-$guid.oslab.opentlc.com region="infra" zone="na"
# oc label node infranode00-$guid.oslab.opentlc.com region="infra" zone="infranodes"
# oc label node node00-$guid.oslab.opentlc.com region="primary" zone="east"
# oc label node node01-$guid.oslab.opentlc.com region="primary" zone="west"
----

ifdef::showscript[]

=== Transcript

After we install OpenShift, we need to label the nodes, labeling the nodes
allows the scheduler to use "logic" defined in the *scheduler.json* file when
trying to provision pods in our environment.

In this example we use "Regions" and "Zones", but many other types of topologies
can be used, we will discuss this more later in the training.

endif::showscript[]




== Deploying the Registry
:noaudio:

* OpenShift can build Docker images from your source code, deploy them, and
manage their lifecycle. To enable this, an internal, integrated Docker registry
should be deployed in your OpenShift environment.
* OpenShift runs the registry in a pod on a node, just like any other workload.
* Using the command below creates a *service* and a *deployment configuration*,
both called "docker-registry".
Once deployed successfully, a pod is created with a name similar to docker-registry-1-cpty9.

* If you wanted to control where your registry gets deployed, you can specify
desired target using the "--selector" flag by picking the labels you want to
match.

ifdef::showscript[]

=== Transcript

The *Regisrty container*

endif::showscript[]

== Deploying the Registry
:noaudio:

* In our environment, we created a region called *infra* and dedicated the
*infranode00* host for it.
** This is a good practice for a highly scalable environment, perhaps we will
use better performing servers for these nodes or place them in the DMZ so only
they can be accessed from the external network.

.Deploying Registry
* To deploy a registry "anywhere" in the environment.
+
----
$ oadm registry --config=admin.kubeconfig \
    --credentials=openshift-registry.kubeconfig
----


* This will make sure that the *registry* pod will only be hosted in the
"*infra*" region.
+
----
$ oadm registry --config=admin.kubeconfig \
    --credentials=openshift-registry.kubeconfig \
	   --selector='region=infra'
----

ifdef::showscript[]

=== Transcript

The *Registry container* can be deployed anywhere in the OSE environment, In our
 environment, we created a region called *infra* and dedicated the *infranode00*
  host for it.

This is a good practice for a highly scalable environment, perhaps we will use
better performing servers for these nodes or place them in the DMZ so only they
can be accessed from the external network.

endif::showscript[]




== Deploying the Registry
:noaudio:

.NFS Storage for the Registry

* The registry stores Docker images and metadata. If you simply deploy a pod
with the registry, it uses an ephemeral volume that is destroyed if the pod
exits. Any images anyone has built or pushed into the registry would disappear.
* For production use, you should use persistent storage using PersistentVolume
and PersistentVolumeClaim objects for storage for the registry.
* For non-production use, other options exist to provide persistent storage for
the registry, like the --mount-host option.
+
----
$ oadm registry --config=admin.kubeconfig \
    --credentials=openshift-registry.kubeconfig \
	   --selector='region=infra' \
     --mount-host host:/export/dirname
----
+
** The --mount-host option mounts a directory from the node on which the
registry container lives. If you scale up the docker-registry deployment
configuration, it is possible that your registry pods and containers will
run on different nodes.
+
**
ifdef::showscript[]

=== Transcript

The registry stores Docker images and metadata.
If you simply deploy a pod with the registry, it uses an ephemeral volume that
is destroyed if the pod exits.
Any images anyone has built or pushed into the registry would disappear.

For production use, you should use persistent storage using PersistentVolume
and PersistentVolumeClaim objects for storage for the registry.


endif::showscript[]

== Deploying the Default HAProxy Router
:noaudio:

* The OpenShift router is the ingress point for all traffic destined for
services in your OpenShift installation.
* An HAProxy based-router implementation is provided as the default template
router plug-in.
** uses the *openshift3/ose-haproxy-router* image to run an HAProxy instance
alongside and a router plug-in.
** currently supports only HTTP(S) traffic and TLS-enabled traffic via SNI.
** is hosted inside OpenShift like any other workload (eg: the registry)
** *While it is called a "router", it is essentially a proxy*.

* The default router’s pod listens on its hosts network interface on port 80
and 443.
** unlike most containers that listen only on private IPs, the default router's
container listens on external/public ports.
** The router proxies external requests for route names to the IPs of actual
pods identified by the service associated with the route.

ifdef::showscript[]

=== Transcript

The Default Router, aka the Default HA-Proxy Router and many other similar
names, is a modified deployment of HA-Proxy, its role is to be the ingress point
 for all traffic destined for services in your OpenShift installation.


endif::showscript[]




== Populating OpenShift
:noaudio:

* You can populate your OpenShift installation with a useful set of
Red Hat-provided *image streams* and *templates* to make it easy for developers
to create new applications.
** Template: A template describes a set of resources intended to be used
together that can be customized and processed to produce a configuration.
Each template defines a list of parameters that can be modified for consumption
by containers.
** Image Streams: An image stream is similar to a Docker image repository in
that it contains one or more Docker images identified by tags. An image stream
presents a single virtual view of related images.

* The core set of image streams define images that can be used to build *Node.js*, *Perl*, *PHP*, *Python*, and *Ruby* applications. It also defines images for databases: *MongoDB*, *MySQL*, and *PostgreSQL*.
** To create the core set of image streams, that use the Red Hat Enterprise Linux (RHEL) 7 based images:
+
----
oc create -f \
    examples/image-streams/image-streams-rhel7.json \
    -n openshift
----


ifdef::showscript[]

=== Transcript
You can populate your OpenShift installation with a useful set of
Red Hat-provided *image streams* and *templates* to make it easy for developers
to create new applications. (Image Streams and Common Templates are added
  automatically by the installer)

A *template* describes a set of resources intended to be used together that can
be customized and processed to produce a configuration.
Each template defines a list of parameters that can be modified for consumption
by container. This is somewhat similar to a OSEv2 "quickstart".

An image stream is similar to a Docker image repository in that it contains one
or more Docker images identified by tags. An image stream presents a single
virtual view of related images.

endif::showscript[]

== Populating OpenShift
:noaudio:

* The xPaaS Middleware image streams provide images for *JBoss EAP*,
*JBoss EWS*, and *JBoss A-MQ*. They can be used to build applications for those
platforms.
** To create the Image Streams for xPaaS Middleware Images:
+
----
$ oc create -f \
    examples/xpaas-streams/jboss-image-streams.json
    -n openshift
----
* The database service templates make it easy to run a database instance which can be utilized by other components.
* For each database (*MongoDB*, *MySQL*, and *PostgreSQL*), two templates are provided.
** To create the core set of database templates:
+
----
$ oc create -f \
    examples/db-templates -n openshift
----

** After creating the templates, users are able to easily instantiate the various templates, giving them quick access to a database deployment.


ifdef::showscript[]

=== Transcript
Red Hat provides xPaas Middleware image streams for  *JBoss EAP*, *JBoss EWS*,
and *JBoss A-MQ*.

you can also deploy database Templates that make it easy to run a database
instance which can be utilized by other components.

endif::showscript[]




== Populating OpenShift
:noaudio:

* The QuickStart templates define a full set of objects for a running application.
** These Include:
*** Build configurations to build the application from source located in a
GitHub public repository
*** Deployment configurations to deploy the application image after it is built.
*** Services to provide load balancing (Internally) for the application pods.
*** Routes to provide external access and load balancing to the application.
** To create the core QuickStart templates:
+
----

$ oc create -f \
    examples/quickstart-templates -n openshift

----


ifdef::showscript[]

=== Transcript
The QuickStart templates define a full set of objects for a running application,
 This will include: Build Configurations, Deployment Configurations, Services
 and Routes for the application.

 Later in the training we will learn more about templates and the  other resources
 mentioned in this slide.

endif::showscript[]

== Configure Persistent Storage Using NFS
:noaudio:


.Overview

* You can provision your OpenShift cluster with persistent storage using NFS.
* Persistent Volume framework allows administrators to provision a cluster with
persistent storage and gives users a way to request those resources without
having any knowledge of the underlying infrastructure.
* Storage must exist in the underlying infrastructure before it can be mounted
as a volume in OpenShift.
* All that is required for NFS is a distinct list of servers and paths and the
`*PersistentVolume*` API.


ifdef::showscript[]

=== Transcript
You can provision your OpenShift cluster with persistent storage using NFS.

Persistent Volume framework allows administrators to provision a cluster with
persistent storage and gives users a way to request those resources without
having any knowledge of the underlying infrastructure.


endif::showscript[]

== Configure Persistent Storage Using NFS
:noaudio:

.Persistent Volume Object Definition

[source,yaml]
----
{
  "apiVersion": "v1",
  "kind": "PersistentVolume",
  "metadata": {
    "name": "pv0001"
  },
  "spec": {
    "capacity": {
        "storage": "5Gi"
    },
    "accessModes": [ "ReadWriteOnce" ],
    "nfs": {
        "path": "/tmp",
        "server": "172.17.0.2"
    },
    "persistentVolumeReclaimPolicy": "Recycle"
  }
}
----

ifdef::showscript[]

=== Transcript
Have a look at the provided "Persistent Volume Object Definition"
Notice that we will need to specify the storage capacity, access mode and the
details of our NFS host.

endif::showscript[]

== Configure Persistent Storage Using NFS
:noaudio:

.Enforcing Disk Quotas
* Use disk partitions to enforce disk quotas and size constraints.
** Each partition can be its own export.
** Each export is one persistent volume.
* Unique names enforces for persistent volumes, but the uniqueness of the NFS
volume's server and path is up to the administrator.

* Enforcing quotas in this way allows the end user to request persistent storage
by a specific amount (e.g,, 10Gi) and be matched with a corresponding volume of
equal or greater capacity.

ifdef::showscript[]

=== Transcript

To define disk quotas you will need to use disk partitions to create size
constraints,  Enforcing quotas in this way allows the end user to request
persistent storage by a specific amount (e.g,, 10Gi) and be matched with a
corresponding volume of equal or greater capacity.

endif::showscript[]

== Configure Persistent Storage Using NFS
:noaudio:

.Volume Security

* `PersistentVolume` Objects are created in the context of a specific *project*
* Users request storage with a `PersistentVolumeClaim` object in the same
*project*.
** This claim only lives in the user's namespace (project) and can only be
referenced by a pod within that same namespace.
* Any attempt to access a persistent volume across a project causes the pod to
fail.
* Each NFS volume must be mountable by all nodes in the cluster.

ifdef::showscript[]

=== Transcript
`PersistentVolume` Objects are created in the context of a specific *project*,
A user can request storage with a `PersistentVolumeClaim` object in the same
*project*, the claim only lives in the user's namespace (project) and can only be
referenced by a pod within that same namespace.

Any attempt to access a persistent volume across a project causes the pod to
fail.

endif::showscript[]

== Configure Persistent Storage Using NFS
:noaudio:

.Reclaiming Resources
* OpenShift implements the Kubernetes *Recyclable* plug-in interface.
** reclamation tasks are based on policies set by the
"persistentVolumeReclaimPolicy" key in the *persistent volume* object definition.
* When a volume gets released from their claim (i.e, after the user's *PersistentVolumeClaim* bound
to the volume is deleted), the volume can be "reclaimed".

* The *persistentVolumeReclaimPolicy* key can be set to *Retain* or *Recycle*:
** *Retain* is the default setting for this key. Volumes will not be deleted.
** *Recycle* volumes are scrubbed (i.e., `rm -rf` is run on the volume) after being
released from their claim
* Once recycled, the NFS volume can be bound to a new claim.

ifdef::showscript[]

=== Transcript
By default, persistent volumes are set to *Retain*. NFS volumes which are set to
*Recycle* are scrubbed (i.e., `rm -rf` is run on the volume) after being
released from their claim (i.e, after the user's `*PersistentVolumeClaim*` bound
to the volume is deleted). Once recycled, the NFS volume can be bound to a new
claim.
endif::showscript[]

== Configure Persistent Storage Using NFS
:noaudio:

.Automation
* As discussed, OSE clusters can be provisioned with persistent storage using NFS in
the following way:
- Disk partitions can be used to enforce storage quotas.
- Security can be enforced by restricting volumes to the
namespace that has a claim to them.
- Reclamation of discarded resources can be
configured for each persistent volume.

NOTE: They are many ways that you can use scripts to automate the above tasks.
You can review this
link:https://github.com/openshift/openshift-ansible/tree/master/roles/kube_nfs_volumes[example
Ansible playbook] to help you get started.


ifdef::showscript[]

=== Transcript
After you setup your OSE environment/cluster to be able to use Persistenet
Storage its easy to create automation scripts and processes.


endif::showscript[]

== Configure Persistent Storage Using NFS
:noaudio:

.SELinux and NFS Export Settings

* By default, SELinux does not allow writing from a pod to a remote NFS server.
** The NFS volume mounts correctly, but is read-only.
* To enable writing in SELinux on each node:
----
# setsebool -P virt_use_nfs 1
----

* Additionally, each exported volume on the NFS server itself should conform to
the following:
- Each export options must be set like the following in the */etc/exports* file:
----
/example_fs *(rw,all_squash)
----
- Each export must be owned by *nfsnobody* and have the following permissions:
----
# chown -R nfsnobody:nfsnobody /example_fs
# chmod 777
----

ifdef::showscript[]

=== Transcript
To configure SElinux to allow the nodes to use NFS in the way required for OSE
`Persistent Volumes`, use the *setsebool* command as displayed on EACH node.

Also, you will need to configure your NFS server exports as described in this
slide.

endif::showscript[]
