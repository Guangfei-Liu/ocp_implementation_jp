:scrollbar:
:data-uri:
:toc2:
:icons: images/icons

== Manage Resources
:numbered:

In this lab you manage different types of OpenShift Enterprise resources.

* Manage Users, Projects, and Quotas
* Create Services and Routes
* Learn About Container Components 
* Create a Persistent Volume for the Registry

== Manage Users, Projects, and Quotas

=== Create a Project

. On the master host, use `oadm` to create and assign an administrative user `andrew` to a project.
+
----

[root@master00-GUID ~]# oadm new-project resourcemanagement --display-name="Resources Management" \
    --description="This is the project we use to learn about resource management" \
    --admin=andrew  --node-selector='region=primary'

----

=== View Resources in the Web Console

[NOTE]
Red Hat has completely redesigned the web console for OpenShift Enterprise 3.0.

. Open your web browser and go to https://master00-GUID.oslab.opentlc.com:8443.
+
[NOTE]
It may take up to 90 seconds for the web console to become available any time you restart the master.

. The first time you access the URL, you need to accept the self-signed SSL certificate.

. When prompted, enter the following username and a password:
** *Username*: `andrew`
** *Password*: `r3dh4t1!`

. In the web console, click the *Resources Management* project. Because you have not put anything into your project, there is nothing to view yet.


=== Apply a Quota to Your Project

. Create a quota definition file.
+
----

[root@master00-GUID ~]# cat << EOF > quota.json
{
  "apiVersion": "v1",
  "kind": "ResourceQuota",
  "metadata": {
    "name": "test-quota"
  },
  "spec": {
    "hard": {
      "memory": "1Gi",
      "cpu": "20",
      "pods": "3",
      "services": "5",
      "replicationcontrollers":"5",
      "resourcequotas":"1"
    }
  }
}
EOF

----

. On the master host, do the following:
.. Use `oc create` to apply the file you just created:
+
----

[root@master00-GUID ~]# oc create -f quota.json --namespace=resourcemanagement

----

.. Verify that the quota was created:
+
----

[root@master00-GUID ~]# oc get -n resourcemanagement quota

----
+
----

NAME
test-quota

----

.. Verify limits and examine usage:
+
----

[root@master00-GUID ~]# oc describe quota test-quota -n resourcemanagement

----
+
----

Name:                   test-quota
Resource                Used    Hard
--------                ----    ----
cpu                     0       20
memory                  0       1Gi
pods                    0       3
replicationcontrollers  0       5
resourcequotas          1       1
services                0       5

----
+
[NOTE]
After you create the quota, it can take a few moments for it to be fully processed. If you get blank output from the `get` or `describe` commands, wait a few minutes and try again.

. Go back into the web console and click the *Resource Management* project.

. Click the *Settings* tab. The quota information is displayed.

=== Apply Limit Ranges to Your Project

For quotas to be effective, you need to create _limit ranges_. Limit ranges set the maximum, minimum, and default allocations of memory and CPU at both the pod and container level. Without default values for containers, projects with quotas will fail, because the deployer and other infrastructure pods are unbounded and therefore forbidden.

. Create the limits file.
+
----
[root@master00-GUID ~]# cat << EOF > limits.json
{
    "kind": "LimitRange",
    "apiVersion": "v1",
    "metadata": {
        "name": "limits",
        "creationTimestamp": null
    },
    "spec": {
        "limits": [
            {
                "type": "Pod",
                "max": {
                    "cpu": "500m",
                    "memory": "750Mi"
                },
                "min": {
                    "cpu": "10m",
                    "memory": "5Mi"
                }
            },
            {
                "type": "Container",
                "max": {
                    "cpu": "500m",
                    "memory": "750Mi"
                },
                "min": {
                    "cpu": "10m",
                    "memory": "5Mi"
                },
                "default": {
                    "cpu": "100m",
                    "memory": "100Mi"
                }
            }
        ]
    }
}
EOF


----

. On the master host, run `oc create` against the `limits.json` file and the `resourcemanagement` project.
+
----

[root@master00-GUID ~]# oc create -f limits.json --namespace=resourcemanagement

----

. Review your limit ranges.
+
----

[root@master00-GUID ~]# oc describe limitranges limits -n resourcemanagement

----
+
----

Name:           limits
Type            Resource        Min     Max     Default
----            --------        ---     ---     ---
Pod             memory          5Mi     750Mi   -
Pod             cpu             10m     500m    -
Container       cpu             10m     500m    100m
Container       memory          5Mi     750Mi   100Mi

----

=== Test Your Quotas

. Authenticate to OpenShift Enterprise and choose your project:

.. Connect to the OpenShift Enterprise master following the same steps you used previously.
.. When prompted, enter the following username and a password:
** *Username*: `andrew`
** *Password*: `r3dh4t1!`
+
----

[root@master00-GUID ~]# su - andrew
[andrew@master00-GUID ~]$ oc login -u andrew --insecure-skip-tls-verify --server=https://master00-${guid}.oslab.opentlc.com:8443

----

* You will see the following:
+
----
Password: (Enter r3dh4t1!)
Login successful.
Welcome to OpenShift! See 'oc help' to get started.
----


. Create the `hello-pod.json` pod definition file.
+
----

[andrew@master00-GUID ~]$ cat <<EOF > hello-pod.json
{
  "kind": "Pod",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-openshift",
    "creationTimestamp": null,
    "labels": {
      "name": "hello-openshift"
    }
  },
  "spec": {
    "containers": [
      {
        "name": "hello-openshift",
        "image": "openshift/hello-openshift:v0.4.3",
        "ports": [
          {
            "hostPort": 36061,
            "containerPort": 8080,
            "protocol": "TCP"
          }
        ],
        "resources": {
          "limits": {
            "cpu": "10m",
            "memory": "16Mi"
          }
        },
        "terminationMessagePath": "/dev/termination-log",
        "imagePullPolicy": "IfNotPresent",
        "capabilities": {},
        "securityContext": {
          "capabilities": {},
          "privileged": false
        },
        "nodeSelector": {
          "region": "primary"
        }
      }
    ],
    "restartPolicy": "Always",
    "dnsPolicy": "ClusterFirst",
    "serviceAccount": ""
  },
  "status": {}
}

EOF

----

=== Run the Pod

Here you create a simple pod without a _route_ or a _service_.

. Create and verify the `hello-openshift` pod.
+
----

[andrew@master00-GUID ~]$ oc create -f hello-pod.json
pods/hello-openshift

[andrew@master00-GUID ~]$ oc get pods
NAME              READY     REASON    RESTARTS   AGE
hello-openshift   1/1       Running   0          2m

----

. Run `oc describe` to learn about your pod.
+
----
[andrew@master00-GUID ~]$  oc describe pod hello-openshift
Name:                           hello-openshift
Image(s):                       openshift/hello-openshift:v0.4.3
Host:                           node01-f4fc.oslab.opentlc.com/192.168.0.201
Labels:                         name=hello-openshift
Status:                         Running
IP:                             10.1.1.2
Replication Controllers:        <none>
Containers:
  hello-openshift:
    Image:              openshift/hello-openshift:v0.4.3
    State:              Running
      Started:          Thu, 02 Jul 2015 02:42:50 -0400
    Ready:              True
    Restart Count:      0
Conditions:
  Type          Status
  Ready         True
Events:
  .... "Successfully assigned hello-openshift to node01-f4fc.oslab.opentlc.com" ....

----
+
. Test that your pod is responding with `Hello OpenShift`.
+
----

[andrew@master00-GUID ~]$ ip=`oc describe pod hello-openshift|grep IP:|awk '{print $2}'`
[andrew@master00-GUID ~]$ curl http://${ip}:8080

----

* You will see the following:
+
----
Hello OpenShift!
----

. Delete this pod.
+
----

[andrew@master00-GUID ~]$ oc delete -f hello-pod.json

----

. Create a new definition file that launches four `hello-openshift` pods.
+
----
[andrew@master00-GUID ~]$ cat << EOF > hello-many-pods.json
{
  "metadata":{
    "name":"quota-pod-deployment-test"
  },
  "kind":"List",
  "apiVersion":"v1",
  "items":[
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-1",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-2",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-3",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-4",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    }
  ]
}


EOF

----

. Create the items in the `hello-many-pods.json` file.
+
----
[andrew@master00-GUID ~]$ oc create -f hello-many-pods.json
pods/hello-openshift-1
pods/hello-openshift-2
pods/hello-openshift-3
Error from server: Pod "hello-openshift-4" is forbidden: Limited to 3 pods
----
+
[NOTE]
Because you created a quota, the fourth pod is not created.

. Delete these objects.
+
----
[andrew@master00-GUID ~]$ oc delete  -f hello-many-pods.json
----

. (Optional) Using what you have learned, create a new project, set the quota so that the pod value is `10`, and run `hello-many-pods.json` again.

== Create Services and Routes

. As `root` on the master host, create a new `scvslab` project.
+
----

[andrew@master00-GUID ~]$ exit
[root@master00-GUID ~]# oadm new-project svcslab --display-name="Services Lab" \
    --description="This is the project we use to learn about services" \
    --admin=andrew  --node-selector='region=primary'
----

. Switch to user `andrew` and log back into OpenShift Enterprise.
. Switch to the `svcslab` project.
+
----

[root@master00-GUID ~]# su - andrew
[andrew@master00-GUID ~]$ oc project svcslab
Now using project "svcslab" on server "https://master00-GUID.oslab.opentlc.com:8443".

----

. Create the `hello-service.json` file.
+
----

[andrew@master00-GUID ~]$  cat <<EOF > hello-service.json
{
  "kind": "Service",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-service"
  },
  "spec": {
    "selector": {
      "name":"hello-openshift"
    },
    "ports": [
      {
        "protocol": "TCP",
        "port": 8888,
        "targetPort": 8080
      }
    ]
  }
}
EOF

----
+
. Create and verify the pod.
+
----

[andrew@master00-GUID ~]$ oc create -f hello-service.json
services/hello-service

----
+
. Display the services running under the current project.
+
----

[andrew@master00-GUID ~]$ oc get services
NAME            LABELS    SELECTOR               IP(S)          PORT(S)
hello-service   <none>    name=hello-openshift   172.30.xxx.yyy   8888/TCP

----
+
. Look at the details of your service. Note the following:
** *Selector*: Describes which pods the service should "select" or "list".
** *Endpoints*: Lists all the pods that are currently listed--none in your current project.
+
----
[andrew@master00-GUID ~]$ oc describe service hello-service
Name:                   hello-service
Labels:                 <none>
Selector:               name=hello-openshift
Type:                   ClusterIP
IP:                     172.30.xxx.yyy
Port:                   <unnamed>       8888/TCP
Endpoints:              <none>
Session Affinity:       None
No events.
----

. Create more pods.
+
----

[andrew@master00-GUID ~]$ oc create -f hello-many-pods.json

----

. Check the service again.

* You can see that the pods that share the label `name=hello-openshift` are all listed.
+
----

[andrew@master00-GUID ~]$ oc get service
NAME            LABELS    SELECTOR               IP(S)          PORT(S)
hello-service   <none>    name=hello-openshift   172.30.5.240   8888/TCP

[andrew@master00-GUID ~]$ oc describe service hello-service
Name:                   hello-service
Labels:                 <none>
Selector:               name=hello-openshift
Type:                   ClusterIP
IP:                     172.30.5.240
Port:                   <unnamed>       8888/TCP
Endpoints:              10.1.0.4:8080,10.1.1.5:8080,10.1.1.7:8080
Session Affinity:       None
No events.

----

. Test your service.
+
----

[andrew@master00-GUID ~]$ ip=`oc describe service hello-service|grep IP:|awk '{print $2}'`
[andrew@master00-GUID ~]$ curl http://${ip}:8888
Hello OpenShift!

----

. Create the route.
+
----
[andrew@master00-GUID ~]$ oc expose service/hello-service --hostname=hello2-openshift.cloudapps-${guid}.oslab.opentlc.com
----
+

. View your routes.
+
----
[andrew@master00-6b80 ~]$ oc get routes
NAME            HOST/PORT                                           PATH      SERVICE         LABELS
hello-service   hello2-openshift.cloudapps-GUID.oslab.opentlc.com             hello-service
----

. Test the route.
+
----

[andrew@master00-GUID ~]$ curl http://hello2-openshift.cloudapps-${guid}.oslab.opentlc.com
Hello OpenShift!

----

== Learn About Container Components

You use `oc exec` to look into a container. Here look at your _router_ and _registry_.

=== View the Route Container

. Make sure you are in the `svcslab` project.
. As user `andrew`, run the following:
+
----
[andrew@master00-d9b2 ~]$ oc new-app https://github.com/openshift/simple-openshift-sinatra-STI.git -l "todelete=yes" -o yaml |  sed 's/replicas: 1/replicas: 3/g' | oc create -f -
imagestreams/simple-openshift-sinatra-sti
buildconfigs/simple-openshift-sinatra-sti
deploymentconfigs/simple-openshift-sinatra-sti
services/simple-openshift-sinatra

[andrew@master00-d9b2 ~]$ oc expose service simple-openshift-sinatra --hostname=whatever.com

----


. As `root`, connect to the router by finding out its name and running `oc exec`.
+
----
[root@master00-d9b2 ~]# oc get pods
NAME                      READY     REASON    RESTARTS   AGE
docker-registry-2-snarn   1/1       Running   0          17h
trainingrouter-1-jm5zk    1/1       Running   0          18h
[root@master00-d9b2 ~]# oc exec -ti -p trainingrouter-1-jm5zk /bin/bash
bash-4.2$

#Another option is:
[root@master00-d9b2 ~]# oc exec -ti -p `oc get pods |  awk '/route/ { print $1; }'` /bin/bash
bash-4.2$
----

. After you are running `bash` inside the container, do the following:
.. Run `id`.
.. Run `pwd` and `ls`. Note the directory you are now in.
.. Run `grep SERVERID *`.
.. Run `cat haproxy.config` to see your empty configuration file.
+
----

bash-4.2$ id
uid=1000010000 gid=0(root)

bash-4.2$ pwd
/var/lib/haproxy/conf

bash-4.2$ ls
default_pub_keys.pem     os_edge_http_be.map  os_sni_passthrough.map
haproxy-config.template  os_http_be.map       os_tcp_be.map
haproxy.config           os_reencrypt.map

bash-4.2$ grep SERVERID *
haproxy.config:    cookie OPENSHIFT_default-docker-registry_SERVERID insert indirect nocache httponly
haproxy.config:    cookie OPENSHIFT_resourcemanagement-hello-service_SERVERID insert indirect nocache httponly
haproxy.config:    cookie OPENSHIFT_svcslab-simple-openshift-sinatra_SERVERID insert indirect nocache httponly

bash-4.2$ cat haproxy.config

----

* You will see output similar to that shown below. Note the following:

** The route is the one you created in the previous lab.
** The route points to the endpoints directly.
** The `svcslab` project route you created exists, but probably will not have any endpoints until the build is complete.
+
----
backend be_http_resourcemanagement-hello-service

  mode http
  balance leastconn
  timeout check 5000ms

    cookie OPENSHIFT_resourcemanagement-hello-service_SERVERID insert indirect n
ocache httponly

  server 10.1.2.2:8080 10.1.2.2:8080 check inter 5000ms cookie 10.1.2.2:8080
  server 10.1.2.3:8080 10.1.2.3:8080 check inter 5000ms cookie 10.1.2.3:8080
  server 10.1.2.4:8080 10.1.2.4:8080 check inter 5000ms cookie 10.1.2.4:8080
...
...

----

. Make changes to your service or route by adding or removing pods or creating another route.
. View the changes in the `haproxy.config` file.

=== View the Registry Container

Before you start looking at the registry container, make sure your build has completed.

. As user `andrew`, run the following to see the build.
+
[NOTE]
This takes a while on the lab environment hardware. If the build has not completed, you can take a quick break here.
+
----
[andrew@master00-d9b2 ~]$ oc build-logs simple-openshift-sinatra-sti-1
...
...
...
I0810 21:50:39.236169       1 sti.go:134] Pushing 172.30.236.109:5000/svcslab/simple-openshift-sinatra-sti image ...
I0810 21:53:13.659295       1 sti.go:138] Successfully pushed 172.30.236.109:5000/svcslab/simple-openshift-sinatra-sti

----

. After the build has completed, as `root`, run the following:
+
----
[root@master00-d9b2 ~]# oc exec -ti -p `oc get pods |  awk '/registry/ { print $1; }'` /bin/bash

----

. After you are running `bash` inside the container, do the following:
.. Run `id`.
.. Run `pwd` and `ls`. Note the directory you are now in.
.. Run `cat config.yml` to see your empty configuration file.
+
----
bash-4.2$ id
uid=1000010000 gid=0(root)
bash-4.2$ pwd
/
bash-4.2$ ls
bin   config.yml  etc   lib    media  opt   registry  run   srv  tmp  var
boot  dev         home  lib64  mnt    proc  root      sbin  sys  usr
bash-4.2$ cat config.yml
version: 0.1
log:
  level: debug
http:
  addr: :5000
storage:
  cache:
    layerinfo: inmemory
  filesystem:
    rootdirectory: /registry
auth:
  openshift:
    realm: openshift
middleware:
  repository:
    - name: openshift

----

. Look at the repositories and images available.
+
----
bash-4.2$ cd /registry/docker/registry/v2/repositories
bash-4.2$ ls
svcslab
bash-4.2$ ls svcslab/
simple-openshift-sinatra-sti
bash-4.2$ ls svcslab/simple-openshift-sinatra-sti/
_layers  _manifests  _uploads
bash-4.2$ ls svcslab/simple-openshift-sinatra-sti/_layers/
sha256
bash-4.2$ ls svcslab/simple-openshift-sinatra-sti/_layers/sha256/
39886d6f6998b59a31e853bf1fcc642e40a711d67248904b23647afcb2dae085
c0e305bb0b350a4efcaeb33e1f99efe5235728747d3695b16b111fff7fb40e74
f1689e5704ab6738da07deea58081b784b9e43675063d1e98402ef3c745cd631
/var/export/registry-storage/docker/registry/v2/blobs
----
. To see the size of the "blobs" that the layers are saved as:
+
[NOTE]
The _blob file_ name is the same as your layer _link name_.
+
----
bash-4.2$ cd /registry/docker/registry/v2/blobs/sha256/
bash-4.2$ du -sh *
54M     39
4.0K    7b
1.3M    c0
81M     f1
bash-4.2$ ls f1
f1689e5704ab6738da07deea58081b784b9e43675063d1e98402ef3c745cd631

----
+
[NOTE]
If you configured persistent storage for your registry, you could see the same in `/var/export/registry-storage/docker/registry/v2/`.

. As user `andrew`, look at one of the pods you started earlier in this lab.
+
----
[andrew@master00-d9b2 ~]$ oc get pods
NAME                                   READY     REASON       RESTARTS   AGE
simple-openshift-sinatra-sti-1-build   0/1       ExitCode:0   0          32m
simple-openshift-sinatra-sti-2-2ppvr   1/1       Running      0          29m
simple-openshift-sinatra-sti-2-ehdke   1/1       Running      0          29m
simple-openshift-sinatra-sti-2-qesjy   1/1       Running      0          29m
----

. Connect to the container.
+
----
[andrew@master00-d9b2 ~]$  oc exec -ti -p  simple-openshift-sinatra-sti-2-ehdke "/bin/bash"
bash-4.2$
----

. To explore the container, do the following:
.. Run `id`.
.. Run `pwd` and `ls`. Note the directory you are in.
.. Run `ps -ef` to see what processes are running.
+
----

bash-4.2$ id
uid=1000040000 gid=0(root)

bash-4.2$ ls
Gemfile  Gemfile.lock  app.rb  bundle  config.ru

bash-4.2$ pwd
/opt/openshift/src

bash-4.2$ ps -ef
UID         PID   PPID  C STIME TTY          TIME CMD
1000040+      1      0  0 21:53 ?        00:00:01 ruby /opt/openshift/src/bundle
1000040+     34      0  0 22:21 ?        00:00:00 /bin/bash
1000040+     62     34  0 22:21 ?        00:00:00 ps -ef

----
+
[NOTE]
Your pod names and output will differ slightly.



== Create a Persistent Volume for the Registry

In this lab, you create a persistent volume for your registry, attach it to the `deploymentConfiguration`, and redeploy the registry.

=== Create an NFS Export for the Registry

. As `root` on the `oselab` host, create a directory for your NFS export.
+
----
[root@oselab-GUID ~]# export volname=registry-storage
[root@oselab-GUID ~]# mkdir -p /var/export/pvs/${volname}
[root@oselab-GUID ~]# chown nfsnobody:nfsnobody /var/export/pvs/${volname}
[root@oselab-GUID ~]# chmod 700 /var/export/pvs/${volname}
----

. Add the following line to `/etc/exports`:
+
----
[root@oselab-GUID ~]# echo "/var/export/pvs/${volname} *(rw,sync,all_squash)" >> /etc/exports
----

. Restart NFS services.
+
----
[root@oselab-GUID ~]# systemctl restart rpcbind nfs-server nfs-lock nfs-idmap
----


. On the master host, create a persistent volume definition file named `registry-volume.json`.
+
[source,json]
----
[root@master00-GUID ~]# cat << EOF > registry-volume.json
    {
      "apiVersion": "v1",
      "kind": "PersistentVolume",
      "metadata": {
        "name": "registry-storage"
      },
      "spec": {
        "capacity": {
            "storage": "15Gi"
            },
        "accessModes": [ "ReadWriteMany" ],
        "nfs": {
            "path": "/var/export/pvs/registry-storage",
            "server": "oselab-${GUID}.oslab.opentlc.com"
        }
      }
    }

EOF

----

. In the `default` project, create the `registry-storage1 persistent volume from the definition file.
+
[NOTE]
You are creating the persistent volume in the `default` project, because that is the project in which the registry runs.
+
----
[root@master00-GUID ~]# oc create -f registry-volume.json -n default
persistentvolumes/registry-storage
----

. View the persistent volume you created.
+
----
[root@master00-GUID ~]# oc get pv
NAME               LABELS    CAPACITY      ACCESSMODES   STATUS      CLAIM     REASON
registry-storage   <none>    16106127360   RWX           Available
----

. Create a `registry-volume-claim.json` claim definition file to claim your volume.
+
----

[root@master00-GUID ~]# cat << EOF > registry-volume-claim.json
    {
      "apiVersion": "v1",
      "kind": "PersistentVolumeClaim",
      "metadata": {
        "name": "registry-claim"
      },
      "spec": {
        "accessModes": [ "ReadWriteMany" ],
        "resources": {
          "requests": {
            "storage": "15Gi"
          }
        }
      }
    }

EOF

----

. Create the `registry-claim` claim from the definition file.
+
----
[root@master00-GUID ~]# oc create -f registry-volume-claim.json -n default
persistentvolumeclaims/registry-claim
----

. View the persistent volume you created. Note that the status is `Bound`.
+
----
[root@master00-GUID ~]# oc get pv
NAME               LABELS    CAPACITY      ACCESSMODES   STATUS    CLAIM                    REASON
registry-storage   <none>    16106127360   RWX           Bound     default/registry-claim

----

. View the persistent volume claim you created. Note that the status is also `Bound`.
+
----
[root@master00-GUID ~]# oc get pvc
NAME             LABELS    STATUS    VOLUME
registry-claim   map[]     Bound     registry-storage
----

=== Attach the Persistent Volume to the Registry

. Assuming that your registry is already running, get the names of your available `deploymentConfigurations`.
+
----
[root@master00-GUID ~]# oc get dc
NAME              TRIGGERS       LATEST VERSION
docker-registry   ConfigChange   1
trainingrouter    ConfigChange   1

----
. (Optional) You can use `oc volume` to modify the `DeploymentConfiguration`.
. Add the `registry-storage` volume to the registry's `DeploymentConfiguration`. This triggers a redeployment of the registry.
+
----
oc volume dc/docker-registry --add --overwrite -t persistentVolumeClaim \
--claim-name=registry-claim --name=registry-storage
----

. After you create an S2I build, check that the registry is using the `registry-storage` volume.
+
----

[root@oselab-GUID ~]# find /var/export/pvs/registry-storage
/var/export/registry-storage/docker/registry/v2/repositories
/var/export/registry-storage/docker/registry/v2/repositories/svcslab
/var/export/registry-storage/docker/registry/v2/repositories/svcslab/simple-openshift-sinatra-sti
/var/export/registry-storage/docker/registry/v2/repositories/svcslab/simple-openshift-sinatra-sti/_uploads
/var/export/registry-storage/docker/registry/v2/repositories/svcslab/simple-openshift-sinatra-sti/_layers
/var/export/registry-storage/docker/registry/v2/repositories/svcslab/simple-openshift-sinatra-sti/_layers/sha256
...
...
...

----
