== &nbsp;
:noaudio:

ifdef::revealjs_slideshow[]
[#cover,data-background-image="image/1156524-bg_redhat.png" data-background-color="#cc0000"]

[#cover-h1]
Red Hat OpenShift Enterprise Implementation

[#cover-h2]
Resource Management

[#cover-logo]
image::{revealjs_cover_image}[]

endif::[]

== Module Topics
:noaudio:
:numbered!:

Module 05 : Resource Management

In This chapter we covered:

* OpenShift Resources
* Projects and Users
* Client tool authentication
* Resource Quota
* Service Accounts
* Routes
* Persistent Volumes
* Lab: Users, Projects and Quotas
* Lab: Creating Services and Routes
* Lab: Provisioning and Configuring Persistent Volumes


ifdef::showscript[]

=== Transcript
Welcome to Module 05 of the OpenShift Enterprise Implementation course.

endif::showscript[]




== OpenShift Resources
:noaudio:

* There are a number of different resource types in OpenShift 3.
* When creating/destroying apps, scaling, building and etc, we are manipulating OpenShift and Kubernetes resources under the covers.
* Resources can have quotas enforced against them.
* OpenShift Enterprise 3.0 includes different resource types
** Resources in this context can refer to:
*** Hardware Resources: Memory, CPU, other "platform" resources
*** OpenShift Resources: Pods, services, replication controllers


ifdef::showscript[]

=== Transcript
OpenShift Enterprise 3.0 includes a number of different resource types.

Actions such as creating and destroying apps, scaling, building, and so on all result in  manipulating OpenShift Enterprise and Kubernetes resources in the background.

You can enforce quotas against resources. The quota defines limits for multiple resources--for example, in the code sample shown here, the quota called `test-quota` defines limits for several resources.

Within a project, users cannot run actions that result in exceeding these resource limits. Because the quota is enforced at the project level, it is up to the users to allocate resources--specifically, memory and CPU--to their pods and containers.

Resources in this context can refer not only to memory, CPU, and other "platform" resources, but also to pods, services, and replication controllers.

endif::showscript[]


== Projects and Users
:noaudio:

.Users and User Types

* Users : Interaction with OpenShift is associated with a user.
* An OpenShift user object represents an "actor" which may be granted permissions in the system by adding roles to them or to their groups.
* Several types of users can exist:
** *Regular users* - This is the way most interactive OpenShift users will be represented.
*** Regular users are created automatically in the system upon first login, or can be created via the API.
*** Regular users are represented with the User object.
** *System users* - Many of these are created automatically when the infrastructure is defined, mainly for the purpose of enabling the infrastructure to interact with the API securely.
*** They include a cluster administrator (with access to everything), a per-node user, users for use by routers and registries, and various others.
*** There is also an anonymous system user that is used by default for unauthenticated requests.
*** For example : _system:admin_ , _system:openshift-registry_ and _system:node:node1.example.com_

** *Service accounts* - These are special system users associated with projects;
*** some are created automatically when the project is first created,
*** while project administrators can create more for the purpose of defining access to the contents of each project.
*** Service accounts are represented with the ServiceAccount object.
*** For example : _system:serviceaccount:default:deployer_ and  _system:serviceaccount:foo:builder_

* Every user must authenticate in some way in order to access OpenShift. API requests with no authentication or invalid authentication are authenticated as requests by the anonymous system user. Once authenticated, policy determines what the user is authorized to do.

ifdef::showscript[]

=== Transcript


endif::showscript[]


== Projects and Users
:noaudio:

.Concept: "Namespace"
* A Kubernetes namespace provides a mechanism to scope resources in a cluster. In OpenShift, a project is a Kubernetes namespace with additional annotations.
* Namespaces provide a unique scope for:
** Named resources to avoid basic naming collisions.
** Delegated management authority to trusted users.
** The ability to limit community resource consumption.
* Most objects in the system are scoped by namespace, but some are excepted and have no namespace, including nodes and users.



ifdef::showscript[]

=== Transcript


endif::showscript[]



== Projects and Users
:noaudio:

.What are Projects?
* A project is a Kubernetes namespace with additional annotations, and is the central vehicle by which access to resources for regular users is managed.
* A project allows a community of users to organize and manage their content in isolation from other communities.
* Users must be given access to projects by administrators, or if allowed to create projects, automatically have access to their own projects.

* Each project scopes its own set of:
** *Objects:* Pods, services, replication controllers, etc.
** *Policies:* Rules for which users can or cannot perform actions on objects.
** *Constraints:* Quotas for each kind of object that can be limited.
** *Service accounts:* Service accounts act automatically with designated access to objects in the project.

* Cluster administrators can create projects and delegate administrative rights for the project to any member of the user community. Cluster administrators can also allow developers to create their own projects.


ifdef::showscript[]

=== Transcript


endif::showscript[]

== Client tool authentication
.Web Console Authentication
:noaudio:

* When accessing the web console from a browser at `_<master-public-addr>_:8443`,
you are automatically redirected to a login page.

* You can provide your login credentials on this page to obtain a token to make
API calls. After logging in, you can navigate your projects using the web
console.


ifdef::showscript[]

=== Transcript


endif::showscript[]


== Client tool authentication
:noaudio:

.CLI Authentication

* You can authenticate from the command line using the CLI command `oc login`.
+
----
$ oc login
----

* The command's interactive flow helps you establish a session to an OpenShift
server with the provided credentials.

* All configuration options for the `oc login` command, listed in the `oc login
--help` command output, are optional. The following example shows usage with
some common options:

* Here is a handy example, lets say we wanted to authenticate as the Openshift
Cluster Administrator (Usually root User), we could use the following:
+
----
$ oc login -u system:admin -n openshift
----
NOTE: Notice that we are setting the user name and the *project* (_namespace_)
to log in to.


ifdef::showscript[]

=== Transcript


endif::showscript[]


== Client tool authentication
:noaudio:

.CLI Authentication - Continued

* Here is a an syntax brief:
[options="nowrap"]
----
$ oc login [--username=<username>]  [--password=<password>] [--server=<server>] [--certificate-authority=</path/to/file.crt>|--insecure-skip-tls-verify]
----


* The following table describes these common options for `oc login`:

.Common CLI Configuration Options
[cols="4,8",options="header"]
|===

|Option |Description
|`-s, --server`
|Specifies the host name of the OpenShift server. If a
server is provided through this flag, the command does not ask for it
interactively. This flag can also be used if you already have a CLI
configuration file and want to log in and switch to another server.

|`-u, --username` and `-p, --password`
|Allows you to specify the credentials to log in to the OpenShift
server. If user name or password are provided through these flags, the command
does not ask for it interactively. These flags can also be used if you already
have a configuration file with a session token established and want to log in and
switch to another user name.

|`--certificate-authority`
|Correctly and securely authenticates with an OpenShift
server that uses HTTPS. The path to a certificate authority file must be
provided.

|`--insecure-skip-tls-verify`
|Allows interaction with an HTTPS server bypassing the server
certificate checks; however, note that it is not secure. If you try to `oc
login` to a HTTPS server that does not provide a valid certificate, and this or
the `--certificate-authority` flags were not provided, `oc login` will prompt
for user input to confirm (`y/N` kind of input) about connecting insecurely.
|===



ifdef::showscript[]

=== Transcript


endif::showscript[]


== Resource Quota
:noaudio:

.What is ResourceQuota
* OpenShift can limit both the number of objects created in a Project , and the
total amount of resources requested across objects in a namespace/Project.
* This facilitates sharing of a single OpenShift cluster by several teams, each
in a Project of their own, as a mechanism of preventing one team from starving
another team of cluster resources.
* A ResourceQuota object enumerates hard resource usage limits per project.
** It can limit the total number of a particular type of object that may be
created in a project, and the total amount of compute resources that may be
consumed by resources in that project.


.Usage limits
|===
|Resource Name |Description
|cpu |Total cpu usage across all containers
|memory |Total memory usage across all containers
|pods |Total number of pods
|replicationcontrollers | Total number of replication controllers
|resourcequotas | Total number of resource quotas
| services | Total number of services
| secrets | Total number of secrets
| persistentvolumeclaims |Total number of persistent volume claims
|===


ifdef::showscript[]

=== Transcript


endif::showscript[]



== Resource Quota
:noaudio:

.Quota enforcement
* After a quota is first created in a project, the project restricts the ability
to create any new resources that may violate a quota constraint until it has
calculated updated usage statistics.

* Once a quota is created and usage statistics are up-to-date, the project
accepts the creation of new content. When you create resources, your quota
usage is incremented immediately upon the request to create or modify the
resource. When you delete a resource, your quota use is decremented during the
next full recalculation of quota statistics for the project. As a result, it
 may take a moment for your quota usage statistics to be reduced to their
 current observed system value when you delete resources.

* If your modification to a project would exceed a quota usage limit, the action is denied by the server, and an appropriate error message is returned to the end-user. The error explains what quota constraint was violated, and what their currently observed usage stats are in the system.

ifdef::showscript[]

=== Transcript


endif::showscript[]



== Resource Quota
:noaudio:

.Creating and applying a quota to a project

* Sample quota definition file

+
----
{
  "apiVersion": "v1",
  "kind": "ResourceQuota",
  "metadata": {
    "name": "quota" <1>
  },
  "spec": {
    "hard": {
      "memory": "1Gi", <2>
      "cpu": "20", <3>
      "pods": "10", <4>
      "services": "5", <5>
      "replicationcontrollers":"5", <6>
      "resourcequotas":"1" <7>
    }
  }
}
----
<1>  The name of this quota document
<2>  The total amount of memory consumed across all containers may not exceed 1Gi.
<3>  The total number of cpu usage consumed across all containers may not exceed 20 Kubernetes compute units.
<4>  The total number of pods in the project
<5>  The total number of services in the project
<6>  The total number of replication controllers in the project
<7>  The total number of resource quota documents in the project

ifdef::showscript[]

=== Transcript


endif::showscript[]


== Resource Quota
:noaudio:

.Creating and applying a quota to a project

* Apply a quota to a Project
+
----

$ oc create -f create_quota_def_file.json --namespace=your_project_name

----

ifdef::showscript[]

=== Transcript


endif::showscript[]


== Service Accounts
:noaudio:

.Overview

* When a person uses the command line or web console, their API token
authenticates them to the OpenShift API.
* However, when a regular user's
credentials are not available, it is common for components to make API calls
independently. For example:

** Replication controllers make API calls to create or delete pods
** Applications inside containers can make API calls for discovery purposes
** External applications can make API calls for monitoring or integration purposes

* Service accounts provide a flexible way to control API access without sharing a regular user's credentials.


ifdef::showscript[]

=== Transcript


endif::showscript[]

== Service Accounts
:noaudio:

.Usernames and groups

* Every service account has an associated username that can be granted roles,
just like a regular user.
* The username is derived from its project and name:
*system:serviceaccount:<project>:<name>*

* For example, to add the *view* role to the *monitor-agent* service account in the *monitored-project* project:
+
----
$ oc policy add-role-to-user view system:serviceaccount:monitored-project:monitor-agent
----

ifdef::showscript[]

=== Transcript


endif::showscript[]

== Service Accounts
:noaudio:

.Usernames and groups - Continued

* Every service account is also a member of two groups:

** *system:serviceaccounts*, which includes all service accounts in the system
** *system:serviceaccounts:<project>*, which includes all service accounts in
the specified project

* For example, to allow all service accounts in all projects to view resources
in the *top-secret* project:
+
----
$ oc policy add-role-to-group view system:serviceaccounts -n top-secret
----

* To allow all service accounts in the "monitor project" to edit resources in
the *top-secret* project:
+
----
$ oc policy add-role-to-group edit system:serviceaccounts:monitor -n top-secret
----

ifdef::showscript[]

=== Transcript


endif::showscript[]

== Service Accounts
:noaudio:

.Enable service account authentication

* Service accounts authenticate to the API using tokens signed by a private RSA key.
* The authentication layer verifies the signature using a matching public RSA key.

* To enable service account token generation, update the
master configuration file `serviceAccountConfig` stanza to specify a
`privateKeyFile` (for signing), and a matching public key file in the
`publicKeyFiles` list:
+
----
serviceAccountConfig:
  ...
  masterCA: ca.crt <1>
  privateKeyFile: serviceaccounts.private.key <2>
  publicKeyFiles:
  - serviceaccounts.public.key <3>
  - ...
----

<1> CA file used to validate the API server's serving certificate
<2> Private RSA key file (for token signing)
<3> Public RSA key files (for token verification). If private key files are
provided, then the public key component is used. Multiple public key files can
be specified, and a token will be accepted if it can be validated by one of
the public keys. This allows rotation of the signing key, while still
accepting tokens generated by the previous signer.


ifdef::showscript[]

=== Transcript


endif::showscript[]

== Service Accounts
:noaudio:

.Managed service accounts

* Service accounts are required in each project to run builds, deployments, and
other pods.
* The `managedNames` setting in the master configuration file controls which
service accounts are automatically created in every project:
+
----
serviceAccountConfig:
  ...
  managedNames: <1>
  - builder <2>
  - deployer <3>
  - default <4>
  - ...
----
<1> List of service accounts to automatically create in every project
<2> A *builder* service account in each project is required by build pods, and is given the *system:image-builder* role, which allows pushing images to any image stream in the project using the internal docker registry.
<3> A *deployer* service account in each project is required by deployment pods, and is given the *system:deployer* role, which allows viewing and modifying replication controllers and pods in the project.
<4> A *default* service account is used by all other pods unless they specify a different service account.


* All service accounts in a project are given the *system:image-puller* role,
which allows pulling images from any image stream in the project using the internal docker registry.

ifdef::showscript[]

=== Transcript


endif::showscript[]

== Service Accounts
:noaudio:

.Infrastructure service accounts

* Several infrastructure controllers run using service account credentials.
* The following service accounts are created in the OpenShift infrastructure
namespace at server start, and given the following roles cluster-wide:

** The *replication-controller* service account is assigned the
*system:replication-controller* role
** The *deployment-controller* service account is assigned the
*system:deployment-controller* role
** The *build-controller* service account is assigned the
*system:build-controller* role.

NOTE: Additionally, the *build-controller* service account is included in the
privileged security context constraint in order to create privileged build pods.
 More on that later


ifdef::showscript[]

=== Transcript


endif::showscript[]


== Routes
:noaudio:
.Overview

* An OpenShift route is a way to expose a _service_ by giving it an
externally-reachable hostname like `www.example.com`.

* A defined route and the endpoints identified by its service can be consumed by
a router to provide named connectivity that allows external clients to reach
your applications.
* Each route consists of a route name, service selector, and (optionally)
security configuration.

ifdef::showscript[]
=== Transcript

An OpenShift route is a way to expose a _service_ by giving it an
externally-reachable hostname like `www.example.com`.

* A defined route and the endpoints identified by its service can be consumed by
a router to provide named connectivity that allows external clients to reach
your applications.
* Each route consists of a route name, service selector, and (optionally)
security configuration.
endif::showscript[]


== Routes
:noaudio:

.Route Types
* Routes can be either secured or unsecured.
* Secure routes provide the ability to use several types of TLS termination to
serve certificates to the client.
* Routers support  edge, passthrough, and re-encryption termination.

ifdef::showscript[]
=== Transcript
endif::showscript[]

== Routes
:noaudio:

.Route Types - Unsecured Route Object YAML Definition

[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: route-unsecured
spec:
  host: www.example.com
  to:
    kind: Service
    name: service-name
----

* Unsecured routes are simplest to configure, as they require no key
or certificates, but secured routes offer security for connections to
remain private.

* A secured route is one that specifies the TLS termination of the route.
The available types of termination described below.

ifdef::showscript[]
=== Transcript
endif::showscript[]

== Routes
:noaudio:

.Route Types - Path Based Routes

* Path based routes specify a path component that can be compared against
a URL (which requires that the traffic for the route be HTTP based) such
that multiple routes can be served using the same hostname, each with a
different path.
* Routers should match routes based on the most specific
path to the least; however, this depends on the router implementation. The
following table shows example routes and their accessibility:

.Route Availability
[cols="3*", options="header"]
|===
|Route |When Compared to |Accessible

.2+|_www.example.com/test_ |_www.example.com/test_ |Yes

|_www.example.com_ |No

.2+|_www.example.com/test_ and _www.example.com_ |_www.example.com/test_ |Yes

|_www.example.com_ |Yes

.2+|_www.example.com_ |_www.example.com/test_ |Yes (Matched by the host, not the
  route)

|_www.example.com_ |Yes
|===

ifdef::showscript[]
=== Transcript
endif::showscript[]

== Routes
:noaudio:

.Route Types - An Unsecured Route with a Path:

[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: route-unsecured
spec:
  host: www.example.com
  path: "/test"   <1>
  to:
    kind: Service
    name: service-name
----

<1> The path is the only added attribute for a path-based route.

[NOTE]
====
Path-based routing is not available when using passthrough TLS, as
the router does not terminate TLS in that case and cannot read the contents
of the request.
====

ifdef::showscript[]
=== Transcript
endif::showscript[]

== Routes
:noaudio:

.Route Types - Secured Routes

* Secured routes specify the TLS termination of the route and, optionally,
provide a key and certificate(s).

NOTE: TLS termination in OpenShift relies on
link:https://en.wikipedia.org/wiki/Server_Name_Indication[SNI] for serving
custom certificates. Any non-SNI traffic received on port 443 is handled with TLS
termination and a default certificate (which may not match the requested hostname,
resulting in validation errors).

ifdef::showscript[]
=== Transcript
endif::showscript[]

== Routes
:noaudio:

.Secured TLS termination types

* Secured routes can use any of the following three types of secure TLS
termination.

*Edge Termination*

* With edge termination, TLS termination occurs at the
router, prior to proxying traffic to its destination.
* TLS certificates are served by the front end of the router,
so they *must be configured into the route*, otherwise the
router's default certificate will be used for TLS termination.

ifdef::showscript[]
=== Transcript
endif::showscript[]

== Routes
:noaudio:

.Secured TLS termination types - Edge Termination

* A Secured Route definition using Edge Termination:

[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: route-edge-secured
spec:
  host: www.example.com
  to:
    kind: Service
    name: service-name
  tls:
    termination: edge            <1>
    key: |-                      <2>
      BEGIN PRIVATE KEY
      [...]
      END PRIVATE KEY
    certificate: |-              <3>
      BEGIN CERTIFICATE
      [...]
      END CERTIFICATE
    caCertificate: |-            <4>
      BEGIN CERTIFICATE
      [...]
      END
----

<1> The `*termination*` field is `edge` for edge termination.
<2> The `*certificate*` field is the contents of the PEM format certificate file.
<3> The `*key*` field is the contents of the PEM format key file.
<4> An optional CA certificate may be required to establish a certificate chain for validation.

NOTE: Because TLS is terminated at the router, connections from the router to
the endpoints over the internal network are not encrypted.

ifdef::showscript[]
=== Transcript
endif::showscript[]

== Routes
:noaudio:

.Secured TLS termination types - Passthrough Termination

*Passthrough Termination*

* With passthrough termination, encrypted traffic is sent straight to the
destination without the router providing TLS termination. Therefore no
key or certificate is required.

* The destination pod is responsible for serving certificates for the
traffic at the endpoint.

* This is currently the only method that can support requiring client
certificates (also known as two-way authentication).


ifdef::showscript[]
=== Transcript
endif::showscript[]

== Routes
:noaudio:

.Secured TLS termination types - Passthrough Termination

* A Secured Route definition using Passthrough Termination
+
[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: route-passthrough-secured
spec:
  host: www.example.com
  to:
    kind: Service
    name: service-name
  tls:
    termination: passthrough     <1>
----

<1> The `*termination*` field is set to `passthrough`. No other encryption fields are needed.


ifdef::showscript[]
=== Transcript
endif::showscript[]

== Routes
:noaudio:

.Secured TLS termination types Re-encryption Termination

*Re-encryption Termination*

* Re-encryption is a variation on edge termination where the router terminates
TLS with a certificate, then re-encrypts its connection to the endpoint which
may have a different certificate.
* Therefore the full path of the connection is encrypted, even over the internal
network. The router uses health checks to determine the authenticity of the host.


ifdef::showscript[]
=== Transcript
endif::showscript[]

== Routes
:noaudio:

.Secured TLS termination types Re-encryption Termination

* A Secured Route definition using Re-Encrypt Termination
+
[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: route-pt-secured
spec:
  host: www.example.com
  to:
    kind: Service
    name: service-name
  tls:
    termination: reencrypt        <1>
    key: [as in edge termination]
    certificate: [as in edge termination]
    caCertificate: [as in edge termination]
    destinationCaCertificate: |-  <2>
      BEGIN CERTIFICATE
      [...]
      END CERTIFICATE
----

<1> The `*termination*` field is set to `reencrypt`. Other fields are as in edge termination.
<2> The `*destinationCaCertificate*` field optionally specifies a CA
certificate to validate the endpoint certificate, securing the connection
from the router to the destination.


ifdef::showscript[]
=== Transcript
endif::showscript[]

== Routes
:noaudio:

.Routes with Hostnames

* In order for services to be exposed externally, an OpenShift route allows
you to associate a service with an externally-reachable hostname.
* This edge hostname is then used to route traffic to the service.

* A Route with a specified host:
+
[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: host-route
spec:
  host: www.example.com  <1>
  to:
    kind: Service
    name: service-name
----

<1> Specifies the externally-reachable hostname used to expose a service.

ifdef::showscript[]
=== Transcript
endif::showscript[]

== Routes
:noaudio:

.Routes without Hostnames

* If a hostname is *not* provided as part of the route specification, then
OpenShift will automatically generate one for you.
* The generated hostname is of the form `$routename[.$namespace].$suffix`.
* A Route definition without a host:

[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: no-route-hostname
spec:
  to:
    kind: Service
    name: service-name
----


ifdef::showscript[]
=== Transcript
endif::showscript[]

== Routes
:noaudio:

.Custom default routing subdomain

* A cluster administrator can customize the suffix or the default routing
subdomain for an environment using the OpenShift master configuration.
* The following example shows how you can set the configured suffix to
`v3.openshift.test`:

* OpenShift master configuration snippet (master-config.yaml):
+
[source,yaml]
----
routingConfig:
  subdomain: v3.openshift.test
----


* With the OpenShift master node(s) running the above configuration, the
generated hostname for our example of a host added to a namespace
+
----
my-namespace` would be: `no-route-hostname.my-namespace.v3.openshift.test
----


ifdef::showscript[]
=== Transcript
endif::showscript[]

== Routes
:noaudio:

.Routes Recap:

* *Routes* - Match FQDN-destined traffic requests to services and pods they represent
* *Services* - Do not route or load balance between pods
** Services only provide pod information (IP) to router
** You may consider a service as a list of IPs and ports of the pods that the service represents
* *Router container* (not *route*) - An `openshift3/ose-haproxy-router` container that is a preconfigured instance of HAProxy
** An instance of *Router* container watches a route's resource and updates with changes when required

ifdef::showscript[]

=== Transcript

*Routes* allow FQDN-destined traffic to ultimately reach the pods. The services do not route or load balance between the pods--they only provide the pod information (IP) to the router.

You can consider the service as a list of IPs and ports of the pods that the service represents.

In a simplification of the process, the `openshift3/ose-haproxy-router` container is a preconfigured instance of HAProxy.

The OpenShift Enterprise instance running in this container watches a route's resource on the OpenShift Enterprise master.


endif::showscript[]

== Routes
:noaudio:

.Routes Recap:

* Sample route JSON definition:
+
[source,json]
----
$ oc expose service hello-service --hostname=hello-openshift.cloudapps-$GUID.oslab.opentlc.com
NAME            HOST/PORT                                 PATH      SERVICE         LABELS
hello-service   hello-openshift-f4fc.oslab.opentlc.com             hello-service
----

* To display the *routes* in your current project
+
----

$ oc get routes
NAME                    HOST/PORT                                          SERVICE                   LABELS
hello-openshift-route   hello-openshift.cloudapps-GUID.oslab.opentlc.com   hello-openshift-service

----

ifdef::showscript[]

=== Transcript

The code sample gives an example of a route JSON definition. You can see it defines certain aspects of the route: its name, the fully qualified domain name, and the service to which the
route points. Note that the route actually routes directly to the pods, not to the service. The route gets the pod connection details from the service.

endif::showscript[]


== Using Persistent Volumes
:noaudio:

.Overview
* A `PersistentVolume` object is a storage resource in an OpenShift cluster.
* Storage is provisioned by an administrator by creating `PersistentVolume`
objects from sources such as:
** NFS mounts - Supported method
** GCE Persistent Disks (Google Compute)
** EBS Volumes (Amazon Elastic Block Stores)

NOTE: Persistent volume plug-ins other than the supported NFS plug-in, such as
AWS Elastic Block Stores (EBS), GCE Persistent Disks, GlusterFS, iSCSI, and
RADOS (Ceph), are currently in Technology Preview.


ifdef::showscript[]

=== Transcript


endif::showscript[]


== Using Persistent Volumes
:noaudio:

.Requesting Storage

* Storage can be made available to you by laying claims to the resource.
* You can make a request for storage resources using a `PersistentVolumeClaim`
object;
**  the claim is paired with a volume that generally matches your request.

ifdef::showscript[]

=== Transcript


endif::showscript[]

== Using Persistent Volumes
:noaudio:

.Requesting Storage - prerequisite
* For a user to be able to *claim* a volume (`PersistentVolumeClaim`), a
Persistent Volume (`PersistentVolume`) needs to be created.
** A *cluster admin* needs to define and "created" the *pv* in the project it
belongs to.

[source,yaml]
----
{
  "apiVersion": "v1",
  "kind": "PersistentVolume",
  "metadata": {
    "name": "pv0001"
  },
  "spec": {
    "capacity": {
        "storage": "5Gi"
    },
    "accessModes": [ "ReadWriteOnce" ],
    "nfs": {
        "path": "/exports/ose_shares/share154",
        "server": "172.17.0.2"
    },
    "persistentVolumeReclaimPolicy": "Recycle"
  }
}
----

ifdef::showscript[]

=== Transcript

endif::showscript[]



== Using Persistent Volumes
:noaudio:

.Requesting Storage
* After a *PersistentVolume* has been defined in your project:
** You can request storage by creating `PersistentVolumeClaim` objects in your
*projects*:

.Persistent Volume Claim Object Definition

[source,json]
----
{
    "apiVersion": "v1",
    "kind": "PersistentVolumeClaim",
    "metadata": {
        "name": "claim1"
    },
    "spec": {
        "accessModes": [ "ReadWriteOnce" ],
        "resources": {
            "requests": {
                "storage": "5Gi"
            }
        }
    }
}
----


ifdef::showscript[]

=== Transcript


endif::showscript[]


== Using Persistent Volumes
:noaudio:

.Volume and Claim Binding
* A `PersistentVolume` is a specific resource.
* A `PersistentVolumeClaim` is a request for a resource with specific
attributes, such as storage size.
* In between the two is a process that matches a claim to an available volume
and binds them together.
** This allows the claim to be used as a volume in a pod.
** OpenShift finds the volume backing the claim and mounts it into the pod.


ifdef::showscript[]

=== Transcript


endif::showscript[]


== Using Persistent Volumes
:noaudio:

.Volume and Claim Binding

* You can tell whether a claim or volume is bound by querying using the CLI:

----
$ oc get pvc
NAME        LABELS    STATUS    VOLUME
claim1      map[]     Bound     pv0001

$ oc get pv
NAME                LABELS              CAPACITY            ACCESSMODES         STATUS    CLAIM
pv0001              map[]               5368709120          RWO                 Bound     yournamespace / claim1
----

ifdef::showscript[]

=== Transcript


endif::showscript[]


== Using Persistent Volumes
:noaudio:

.Claims as Volumes in Pods

* A `PersistentVolumeClaim` is used by a pod as a volume.
* OpenShift finds the claim with the given name in the same namespace as the
pod, then uses the claim to find the corresponding volume to mount.

* Review the example Pod Definition with a Claim:
[source,json]
----
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "name": "mypod",
        "labels": {
            "name": "frontendhttp"
        }
    },
    "spec": {
        "containers": [{
            "name": "myfrontend",
            "image": "nginx",
            "ports": [{
                "containerPort": 80,
                "name": "http-server"
            }],
            "volumeMounts": [{
                "mountPath": "/var/www/html",
                "name": "pvol"
            }]
        }],
        "volumes": [{
            "name": "pvol",
            "persistentVolumeClaim": {
                "claimName": "claim1"
            }
        }]
    }
}
----



== Summary
:noaudio:

In This chapter we covered:

* OpenShift Resources
* Projects and Users
* Client tool authentication
* Resource Quota
* Service Accounts
* Routes
* Persistent Volumes

ifdef::showscript[]

=== Transcript


endif::showscript[]
