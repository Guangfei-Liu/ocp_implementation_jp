== Labs Overview

* Lab: Add a user to the "privileged" SCC
** Lab Scenario - Allow user to run a privileged container

* Lab: Create SCCs to manage permissions
** Lab Scenario - It's not always desired to add users directly to the very
permissive "privileged" SCC, In this section we will create a couple of SCCs to
control our users permissions and capabilities.

* Lab: Manage Project Self-Provisioning
** Lab Scenario - In this scenario we will disable Self-Provisioning for all
users then and allow specific users to Self-provision projects from a set template.

== Lab: Allow User to run privileged container
Lab Scenario - Add a user to the "privileged" SCC to allow the user to run a
privileged container

=== Authenticate to OpenShift Enterprise and Choose Your Project

. Connect to the OpenShift Enterprise master by following the same steps you used previously.
. Authenticate user `marina` to Openshift Enterprise
+
----

[root@master00 ~]# su - marina
[marina@master00 ~]$ guid=`hostname|cut -f2 -d-|cut -f1 -d.`
[marina@master00 ~]$ oc login -u marina --insecure-skip-tls-verify --server=https://master00-${guid}.oslab.opentlc.com:8443

----
+
You will See
+
----
Password: (Enter r3dh4t1!)
Login successful.
Welcome to OpenShift! See 'oc help' to get started.
----

. Create a project to work in:
+
----
[marina@master00 ~]$ oc new-project managesecurity-lab

----
=== Create the Privileged Pod Definition

. Run the following command to create the `hello-pod-priv.json` file:
. Note the setting : "privileged": true"
----

[marina@master00 ~]$ cat <<EOF > hello-pod-priv.json
{
  "kind": "Pod",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-openshift",
    "creationTimestamp": null,
    "labels": {
      "name": "hello-openshift"
    }
  },
  "spec": {
    "containers": [
      {
        "name": "hello-openshift",
        "image": "openshift/hello-openshift:v0.4.3",
        "ports": [
          {
            "containerPort": 8080,
            "protocol": "TCP"
          }
        ],
        "resources": {
          "limits": {
            "cpu": "10m",
            "memory": "16Mi"
          }
        },
        "terminationMessagePath": "/dev/termination-log",
        "imagePullPolicy": "IfNotPresent",
        "capabilities": {},
        "securityContext": {
          "capabilities": {},
          "privileged": true
        },
        "nodeSelector": {
          "region": "primary"
        }
      }
    ],
    "restartPolicy": "Always",
    "dnsPolicy": "ClusterFirst",
    "serviceAccount": ""
  },
  "status": {}
}

EOF

----

. Try (and fail) to create the pod from the `hello-pod-priv.json`definition file
+
----
[marina@master00-GUID ~]$ oc create -f hello-pod-priv.json
Error from server: Pod "hello-openshift" is forbidden: unable to validate against any security context constraint: [provider restricted: .spec.containers[0].securityContext.privileged: invalid value 'true': Privileged containers are not allowed]
----


=== Add User to Privileged SCC

. One way to  allow *marina* to deploy "Privileged" Containers is to add her to
the "privileged" *SCC* :
- as *root*, Display the available *SCCs*:
+
----
[root@master00 ~]# oc get scc
NAME         PRIV      CAPS      HOSTDIR   SELINUX     RUNASUSER
privileged   true      []        true      RunAsAny    RunAsAny
restricted   false     []        false     MustRunAs   MustRunAsRange
----

. Edit the *privileged scc*, to look similar to the example:
- Add _- marina_ at the last line of the file. (for clarity: "-<space>marina")
+
----
[root@master00 ~]# oc edit scc privileged
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
allowHostDirVolumePlugin: true
allowPrivilegedContainer: true
apiVersion: v1
groups:
- system:cluster-admins
- system:nodes
kind: SecurityContextConstraints
metadata:
  creationTimestamp: 2015-07-30T02:46:22Z
  name: privileged
  resourceVersion: "5104"
  selfLink: /api/v1/securitycontextconstraints/privileged
  uid: 29c38820-3665-11e5-9899-2cc260072896
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: RunAsAny
users:
- system:serviceaccount:openshift-infra:build-controller
- marina
----

=== Retry to deploy Pod

. Go back to the *marina* user, and test your new priviliges:
+
----
[marina@master00-GUID ~]$ oc create -f hello-pod-priv.json
----

* Did the pod deploy this time? (It should have).

. Lets clean up the environment and try another way of granting users permissions.
- As *marina*, Delete the Pod you created
+
----
[marina@master00-GUID ~]$ oc delete -f hello-pod-priv.json
----

. As *root*, remove *marina* from the "privileged" SCC. Remove _- marina_ from
the last line of the file:
+
----
[root@master00 ~]# oc edit scc privileged
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
allowHostDirVolumePlugin: true
allowPrivilegedContainer: true
apiVersion: v1
groups:
- system:cluster-admins
- system:nodes
kind: SecurityContextConstraints
metadata:
  creationTimestamp: 2015-07-30T02:46:22Z
  name: privileged
  resourceVersion: "5104"
  selfLink: /api/v1/securitycontextconstraints/privileged
  uid: 29c38820-3665-11e5-9899-2cc260072896
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: RunAsAny
users:
- system:serviceaccount:openshift-infra:build-controller

----

== Lab: Create SCCs to manage permissions
Lab Scenario - It's not always desired to add users directly to the very permissive "privileged"
SCC, In this section we will create a couple of SCCs to control our users
permissions and capabilities.

=== Create SCCs to allocation permissions and capabilities


. Create the *scc-ops* SCC:
- We are creating an SCC to allow specific users to run "Privileged" containers.
+
[source,yaml]
----
cat << EOF > scc-ops.yaml
kind: SecurityContextConstraints
apiVersion: v1
metadata:
  name: scc-ops
allowPrivilegedContainer: true
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: RunAsAny
users:
- marina

EOF

----

NOTE: This is different than the "privileged" built-in SCC, it is more restrictive, it
doesn't allow to mount local host directories with: _allowHostDirVolumePlugin_

. After saving the file, use the *oc create* command to create the scc
+
----
[root@master00 ~] oc create -f scc-ops.yaml

----


. Check your available SCCs:
+
----

[root@master00 ~]# oc get scc
NAME         PRIV      CAPS      HOSTDIR   SELINUX     RUNASUSER
privileged   true      []        true      RunAsAny    RunAsAny
restricted   false     []        false     MustRunAs   MustRunAsRange
scc-ops      true      []        false     RunAsAny    RunAsAny

----


. Create the *scc-dev* SCC:
- This SCC is for our developer team, it allows them create docker builds that
use *any user other than root*.
- We will take another approach to achieving this, we will *oc export* the
"restricted" built-in SCC and make changed to it.

+
[source,yaml]
----
[root@master00 ~] oc export scc restricted | tee scc-dev.yaml
apiVersion: v1
groups:
- system:authenticated
kind: SecurityContextConstraints
metadata:
  creationTimestamp: null
  name: restricted
runAsUser:
  type: MustRunAsRange
seLinuxContext:
  type: MustRunAs
----

. Edit the file to look like the following:
- Delete the _groups_ section.
- Change _RunAsUser_ Type value to "*MustRunAsNonRoot*".
- Change the SCC _name_ to "*scc-dev*".
- Add the _users_ section, and make sure user *andrew* is on the list
+
[source,yaml]
----
apiVersion: v1
kind: SecurityContextConstraints
metadata:
  creationTimestamp: null
  name: scc-dev
runAsUser:
  type: MustRunAsNonRoot
seLinuxContext:
  type: MustRunAs
users:
 - andrew
----

. After saving the file, use the *oc create* command to create the scc
+
----
[root@master00 ~] oc create -f scc-dev.yaml

----

. Check your available SCCs:
+
----
[root@master00-GUID ~]# oc get scc
NAME         PRIV      CAPS      HOSTDIR   SELINUX     RUNASUSER
privileged   true      []        true      RunAsAny    RunAsAny
restricted   false     []        false     MustRunAs   MustRunAsRange
scc-ops      true      []        false     RunAsAny    RunAsAny
scc-dev      false     []        false     MustRunAs   MustRunAsNonRoot
----

=== Test your new SCCs

. Go back to the *marina* user, and test your new privileges:
+
----
[marina@master00-GUID ~]$ oc create -f hello-pod-priv.json
----

* Did the pod deploy this time? (It should have).




== Lab: Manage Project Self-Provisioning
Lab Scenario - In this scenario we will disable Self-Provisioning for all
users then and allow specific users to Self-provision projects from a set template.

=== Disable Self-Provisioning

. First, Lets disable *Self-Provisioning* for the "`system:authenticated`" group
.. As root, use the *oc edit* command to edit the `clusterPolicybinding`
+
----
 [root@master00-GUID ~]$ oc edit clusterPolicybinding :default
----

. Your *self-provisioners* binding will have the "`system:authenticated`" group
listed and will look similar to the output below.
+
[source,yaml]
----
- name: self-provisioners
  roleBinding:
    groupNames:
    - system:authenticated
    metadata:
      creationTimestamp: 2015-08-10T06:40:30Z
      name: self-provisioners
      resourceVersion: "44"
      uid: b18cdc05-3f2a-11e5-a361-2cc260072896
    roleRef:
      name: self-provisioner
    userNames: []
----

. Remove the line with the  "`system:authenticated`" group, after your change,
the self-provisioners binding should look like this:
+
[source,yaml]
----
- name: self-provisioners
  roleBinding:
    groupNames:
    metadata:
      creationTimestamp: 2015-08-10T06:40:30Z
      name: self-provisioners
      resourceVersion: "44"
      uid: b18cdc05-3f2a-11e5-a361-2cc260072896
    roleRef:
      name: self-provisioner
    userNames: []
----

. To create a message for users trying to provision projects, edit the master's
config file `/etc/openshift/master/master-config.yaml`, and change the
*projectRequestMessage* key to your own message:
+
----
  projectRequestMessage: "Please create project using the portal http://portal.example.com/provision or Contact Mike"
----

. Restart OpenShift master daemon
+
----
[root@master00-GUID ~]$ systemctl restart openshift-master
----

=== Allow specific users Self-Provisioning from project template

. Lets create a user for our new hire *mike*, this user will be used to create
projects for all the users in the environment.
+
----
[root@master00-GUID ~]# useradd mike
[root@master00-GUID ~]# htpasswd -b /etc/openshift/openshift-passwd mike r3dh4t1!
----

. Lets add the "Self-Provisioner" role to Mike
----
[root@master00-GUID ~]# oadm policy add-cluster-role-to-user self-provisioner mike
----

. Log in as *marina* and see if you can create a project (you can't)
+
----
[root@master00-GUID ~]# su - marina
Last login: Wed Aug 12 02:19:02 EDT 2015 on pts/1
[marina@master00-GUID ~]$ oc new-project notgoingtowork
Error from server: Please create project using the portal http://portal.example.com/provision


----
. Log in as *mike*, login to openshift and see if you can create a project and allocate marina as the administrator.
+
----
[root@master00-GUID ~]# su - mike
Last login: Wed Aug 12 02:20:02 EDT 2015 on pts/1
[mike@master00-d9b2 ~]$ oc new-project thiswillwork
Now using project "thiswillwork" on server "https://localhost:8443"
----

NOTE: If you already logged in with a user, you might have to use the *oc login*
command to refresh the token.

. Have a look at the project that *mike* created
.. Note that the Node Selector isn't defined, we'll have to do something about that.
+
----
[mike@master00-GUID ~]$ oc describe project thiswillwork
Name:           thiswillwork
Created:        8 minutes ago
Labels:         <none>
Annotations:    openshift.io/description=
                openshift.io/display-name=
                openshift.io/sa.scc.mcs=s0:c8,c7
                openshift.io/sa.scc.uid-range=1000070000/10000
Display Name:   <none>
Description:    <none>
Status:         Active
Node Selector:  <none>

Quota:  <none>

Resource limits:        <none>

----

. We want to limit all users (including mike, but excluding cluster
  administrators) to be able to deploy projects only in the "primary" region.
. As root, Create a project template definition file:
- Note we have set the desired *node-selector*
+
[source,yaml]
----
[root@master00-GUID ~]# echo '
apiVersion: v1
kind: Template
metadata:
  name: project-request
  namespace: openshift
objects:
- apiVersion: v1
  displayName: ${PROJECT_DISPLAYNAME}
  kind: Project
  metadata:
    annotations:
      description: ${PROJECT_DESCRIPTION}
      displayName: ${PROJECT_DISPLAYNAME}
      openshift.io/node-selector: region=primary
    creationTimestamp: null
    name: ${PROJECT_NAME}
  spec: {}
  status: {}
- apiVersion: v1
  groupNames: []
  kind: RoleBinding
  metadata:
    creationTimestamp: null
    name: admins
    namespace: ${PROJECT_NAME}
  roleRef:
    name: admin
  userNames:
  - ${PROJECT_ADMIN_USER}
parameters:
- name: PROJECT_NAME
- name: PROJECT_DISPLAYNAME
- name: PROJECT_DESCRIPTION
- name: PROJECT_ADMIN_USER
' > project-request.yaml
----


. As root, load the template into the "openshift" project.
+
----
[root@master00-GUID ~]# oc create -f project-request.yaml --namespace=openshift
----

. To set the default template, edit the master's config file:
`/etc/openshift/master/master-config.yaml`, and change the
projectRequestTemplate key value to "*openshift/project-request*"
+
----
[root@master00-GUID ~]# vi /etc/openshift/master/master-config.yaml
or
[root@master00-GUID ~]# sed -i 's/projectRequestTemplate: ""/projectRequestTemplate: "openshift\/project-request"/g' /etc/openshift/master/master-config.yaml
----

. The line should look something like this:
+
----
  projectRequestTemplate: "openshift/project-request"
----

. Restart the master daemon:
+
----
[root@master00-GUID ~]# systemctl restart openshift-master
----


. As *mike*, try to create another project
+
----
[mike@master00-GUID ~]$ oc new-project thiswillworkbetter
Now using project "thiswillworkbetter" on server "https://localhost:8443".
----

. Lets look if the *Node Selector* was defined by the project template
+
----
[mike@master00-GUID ~]$ oc describe project thiswillworkbetter
Name:           thiswillworkbetter
Created:        8 seconds ago
Labels:         <none>
Annotations:    displayName=
                openshift.io/display-name=
                openshift.io/node-selector=region=primary
                openshift.io/sa.scc.mcs=s0:c10,c0
                openshift.io/sa.scc.uid-range=1000090000/10000
Display Name:   <none>
Description:    <none>
Status:         Active
Node Selector:  region=primary

Quota:  <none>

Resource limits:        <none>
----

. In order to make marina the *admin* on the project, we need to bind the *admin*
role to her.
+
----
[mike@master00-GUID ~]$ oc policy add-role-to-user admin marina -n thiswillworkbetter
----

. As *marina*, check that you have access to the "thiswillworkbetter" project
+
----

[root@master00-GUID ~]# su - marina
Last login: Wed Aug 12 02:29:14 EDT 2015 on pts/1
[marina@master00-GUID ~]$ oc get project
NAME                 DISPLAY NAME   STATUS
thiswillworkbetter                  Active


----
