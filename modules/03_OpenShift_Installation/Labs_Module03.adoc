:icons: images/icons
:toc2:

== Deploy OpenShift Enterprise
:numbered:

In this lab we will Deploy OpenShift Enterprise on a single host (in later labs we will add more nodes).

* Configure a DNS on our *oselab* server to serve our OpenShift environment.
* Configure SSH Keys
* Configure Repositories
* Configure Network Settings
* Install Docker on our host
* Configure and Install OpenShift Enterprise
* Test deployment.

== Lab Environment Architecture and Important Information

The lab environment consists of 4 VMs:

* `oselab-GUID.oslab.opentlc.com` (administration host)

* `master00-GUID.oslab.opentlc.com` (master host, contains Etcd and the management console)

* `infranode00-GUID.oslab.opentlc.com` (infranode host, Will run our infrastructure containers: Registry and Router)

* `node00-GUID.oslab.opentlc.com` (node host, Region: Primary, Zone: East. )

* `node01-GUID.oslab.opentlc.com` (node host, Region: Primary, Zone: West. )

[NOTE]
As a reminder you will only be allowed to SSH to the administration host from the outside of the lab environment, all other hosts have external SSH blocked.  Once on the administration host, you can SSH to the other hosts internally.  As described earlier, you will have to use your private SSH key and OPENTLC login to access the system (not root!).

Each student lab is assigned a global unique identifier (GUID) that consists of 4 characters.  This GUID is provided to you in the provisioning email that will be sent to you when you provision your lab environment.  *Anywhere you see GUID from this point on, you will replace it with your lab's GUID.*

*In each lab step take special care to make sure that you are running the step on the required host.  Each step should contain the name of the host to run the step on and the example code should contain the host name in the shell prompt.*

* Administration host example:
+
----

[root@oselab-GUID ~]# command

----

* Master host example:
+
----

[root@master00-GUID ~]# command

----


== Configure Wildcard DNS to Service the OpenShift Environment.

OpenShift requires a wildcard DNS A record.  The wildcard A record should point to the publicly available (external) IP address of the OpenShift router.  For this training, we will ensure that the router will end up on the OpenShift server that is running the master.  It is advisable to use a low TTL for this record in order for DNS client caches to expire quicker so that changes become available quicker.  The DNS server runs on the administration host.

. Connect to your administration host `oselab-GUID.oslab.opentlc.com` (your private key location may vary):
+
----

yourdesktop$ ssh -i ~/.ssh/id_rsa your-opentlc-login@oselab-GUID.oslab.opentlc.com

----

. Become the `root` user on the administration host:
+
----

-bash-4.2$ sudo bash

----

. Install the `bind` and `bind-utils` package on the administration host:
+
----

[root@oselab-GUID ~]# yum -y install bind bind-utils

----

. On the admistration host collect and define the environment's information, We will define the Public IP of our *InfraNode00* as the taget of our wildcard record.
+
----
[root@oselab-GUID ~]# guid=`hostname|cut -f2 -d-|cut -f1 -d.`
[root@oselab-GUID ~]# host infranode00-$guid.oslab.opentlc.com  ipa.opentlc.com |grep infranode | awk '{print $4}'
54.201.162.205
[root@oselab-GUID ~]# HostIP=`host infranode00-$guid.oslab.opentlc.com  ipa.opentlc.com |grep infranode | awk '{print $4}'`
[root@oselab-GUID ~]# domain="cloudapps-$guid.oslab.opentlc.com"

----

. On the administration host create the zone file with the wildcard DNS:
+
----

[root@oselab-GUID ~]# mkdir /var/named/zones
[root@oselab-GUID ~]# echo "\$ORIGIN  .
\$TTL 1  ;  1 seconds (for testing only)
${domain} IN SOA master.${domain}.  root.${domain}.  (
  2011112904  ;  serial
  60  ;  refresh (1 minute)
  15  ;  retry (15 seconds)
  1800  ;  expire (30 minutes)
  10  ; minimum (10 seconds)
)
  NS master.${domain}.
\$ORIGIN ${domain}.
test A ${HostIP}
* A ${HostIP}"  >  /var/named/zones/${domain}.db

----

. Configure `named.conf` on the administration host:
+
----

[root@oselab-GUID ~]# echo "// named.conf
options {
  listen-on port 53 { any; };
  directory \"/var/named\";
  dump-file \"/var/named/data/cache_dump.db\";
  statistics-file \"/var/named/data/named_stats.txt\";
  memstatistics-file \"/var/named/data/named_mem_stats.txt\";
  allow-query { any; };
  recursion yes;
  /* Path to ISC DLV key */
  bindkeys-file \"/etc/named.iscdlv.key\";
};
logging {
  channel default_debug {
    file \"data/named.run\";
    severity dynamic;
  };
};
zone \"${domain}\" IN {
  type master;
  file \"zones/${domain}.db\";
  allow-update { key ${domain} ; } ;
};" > /etc/named.conf

----

. On the administration host correct file permissions and start the DNS server:
+
----

[root@oselab-GUID ~]# chgrp named -R /var/named
[root@oselab-GUID ~]# chown named -R /var/named/zones
[root@oselab-GUID ~]# restorecon -R /var/named
[root@oselab-GUID ~]# chown root:named /etc/named.conf
[root@oselab-GUID ~]# restorecon /etc/named.conf

----

. Enable and start `named` on the administration host:
+
----

[root@oselab-GUID ~]# systemctl enable named
[root@oselab-GUID ~]# systemctl start named

----

. Configure FirewallD on the administation host to allow inbound DNS queries:
+
----

[root@oselab-GUID bin]# firewall-cmd --zone=public --add-service=dns --permanent
[root@oselab-GUID bin]# firewall-cmd --reload

----

=== Verify DNS configuration:

. A test DNS entry was created called `test.cloudapps-GUID.oslab.opentlc.com`.
. First try testing the DNS server running on the administration host:
+
----

[root@oselab-GUID ~]# host test.cloudapps-$guid.oslab.opentlc.com 127.0.0.1

----

. Second try testing with an external name server:
+
----

[root@oselab-GUID ~]# host test.cloudapps-$guid.oslab.opentlc.com 8.8.8.8

----
+
[NOTE]
The first time you query 8.8.8.8 you may notice lag and an error "connection timed out; trying next origin Host test.cloudapps-GUID.oslab.opentlc.com not found: 3(NXDOMAIN)" is normal.  if you do the test again, it will go faster and not error out.

. Lastly test DNS from your laptop/desktop, this might take a few minutes to be updated.  Be sure to replace GUID with the correct GUID.
+
----

yourhost$ nslookup test.cloudapps-$guid.oslab.opentlc.com

----

== Configure SSH Keys:

The OpenShift installer uses SSH to configure hosts.  In this lab we create and install an SSH key pair on the master host and add the public key to the `authorized_hosts` file.

. SSH to the master host from the admin host and create an SSH key pair for the `root` user.
+
----

[root@oselab00-GUID ~]# ssh master00-$guid
...[output omitted]...
[root@master00-GUID ~]# ssh-keygen -f /root/.ssh/id_rsa -N ''

----
+
[NOTE]
If a key exists, go ahead and allow `ssh-keygen` to overwrite it.

. Add the public ssh key to `/root/.ssh/authorized_keys` locally to the master host:
+
----

[root@master00-GUID ~]# cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys

----

. Configure `/etc/ssh/ssh_conf` to disable `StrictHostKeyChecking` on the master host:
+
----

[root@master00-GUID ~]# echo StrictHostKeyChecking no >> /etc/ssh/ssh_config

----
+
[NOTE]
Only do this for hosts that are used for development, testing, or demos!

. From the master host test the new SSH key by connecting to itself over the loopback interface without a keyboard prompt:
+
----

[root@master00-GUID ~]# ssh 127.0.0.1
...[output ommitted]...
[root@master00-GUID ~]# exit

----

. Copy the SSH key to the rest of the nodes in the environment
+
----

[root@master00-GUID ~]# guid=`hostname|cut -f2 -d-|cut -f1 -d.`
[root@master00-GUID ~]# for node in infranode00-$guid.oslab.opentlc.com node00-$guid.oslab.opentlc.com node01-$guid.oslab.opentlc.com; do ssh-copy-id root@$node ; done

----
+
[NOTE]
Remember the default root password is *r3dh4t1!*


== Configure the Repositories on the Master Host

OpenShift requires several software repositories:

* `rhel-7-server-rpms`

* `rhel-7-server-extras-rpms`

* `rhel-7-server-optional-rpms`

* `rhel-server-7-ose-rpms`

Normally you would get these repositories via `subscription-manager` but we have provided a mirror that we will configure in the following lab steps.

. If not already connected, SSH to your master host `master00-GUID.oslab.opentlc.com` from the admin host:
+
----

[yourlogin@oselab-GUID ~]$ ssh root@master00-$guid.oslab.opentlc.com

----

. It is highly recommended that you use a terminal multiplexing tool such as `tmux` or `screen` in case you lose connectivity to your environment.  This will keep your session at the place it was at the time of disconnection.  You are allowed to install the `tmux` or `screen` package using `yum` on the master host.  It is not installed by default.
+
[NOTE]
For more information on using `tmux` use `man tmux` after installing the package.
For more information on using `screen` use `man screen` after installing the package.


. On the master host set up the yum repository configuration file `/etc/yum.repos.d/open.repo` with the following repositories:
+
----
[root@master00-GUID ~]# cat << EOF > /etc/yum.repos.d/open.repo
[rhel-x86_64-server-7]
name=Red Hat Enterprise Linux 7
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-7
enabled=1
gpgcheck=0

[rhel-x86_64-server-rh-common-7]
name=Red Hat Enterprise Linux 7 Common
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-rh-common-7
enabled=1
gpgcheck=0

[rhel-x86_64-server-extras-7]
name=Red Hat Enterprise Linux 7 Extras
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-extras-7
enabled=1
gpgcheck=0

[rhel-x86_64-server-optional-7]
name=Red Hat Enterprise Linux 7 Optional
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-optional-7
enabled=1
gpgcheck=0

EOF

----
+
[NOTE]
We are using a local mirror of the repositories in our lab environment, as stated earlier you would normally use `subscription-manager`.

. Add the OpenShift repository mirror to the master host:
+
----

[root@master00-GUID ~]# cat << EOF >> /etc/yum.repos.d/open.repo
[rhel-7-server-ose-3.0-rpms]
name=Red Hat Enterprise Linux 7 OSE 3
baseurl=http://www.opentlc.com/repos/rhel-7-server-ose-3.0-rpms
enabled=1
gpgcheck=0

EOF

----

. List the available repositories on the master host:
+
-----

[root@master00-GUID ~]# yum repolist

-----
+
You should see the following:
+
----

Loaded plugins: product-id
...[output omitted]...
repo id                           repo name                               status
rhel-7-server-ose-3.0-rpms        Red Hat Enterprise Linux 7 OSE 3           25
rhel-x86_64-server-7              Red Hat Enterprise Linux 7              4,387
rhel-x86_64-server-extras-7       Red Hat Enterprise Linux 7 Extras          19
rhel-x86_64-server-optional-7     Red Hat Enterprise Linux 7 Optional     4,087
rhel-x86_64-server-rh-common-7    Red Hat Enterprise Linux 7 Common          19
...[output omitted]...

----

. The Nodes require to be configures as well, for the sake of simplicity we will copy the repo file to all the nodes directly from the the master
+
-----

[root@master00-GUID ~]# for node in infranode00-$guid.oslab.opentlc.com node00-$guid.oslab.opentlc.com node01-$guid.oslab.opentlc.com; do scp /etc/yum.repos.d/open.repo ${node}:/etc/yum.repos.d/open.repo ; done

-----


== Verify Network Configuration:

In this lab we will verify that the master host is configured correctly for internal and external DNS name resolution.

. Verify the hostname for the master host:
+
----

[root@master00-GUID ~]# hostname -f

----
+
.You should see the following:
----

master00-GUID.oslab.opentlc.com

----

. Take note of the master host's internal IP address:
+
----

[root@master00-GUID ~]# ip address show dev eth0|grep "inet "|awk '{print $2}'|cut -f1 -d/

----

. Make sure the master host's internal DNS entry matches the internal IP address:
+
----

[root@master00-GUID ~]# host `hostname -f`

----

. Take note of the master host's external IP address:
+
----

[root@master00-GUID ~]# curl http://www.opentlc.com/getip

----

. Make sure the master host's external DNS entry matches the external IP address:
+
----

[root@master00-GUID ~]# host `hostname -f` 8.8.8.8

----
+
NOTE: It might take some time for the global DNS servers to be updated. Try again after a short while if this doesn't work on the first try.

. Remove NetworkManager:
+
----
[root@master00-GUID ~]# yum -y remove NetworkManager*
----

. Do the same for the rest of the nodes
+
----

[root@master00-GUID ~]# for node in infranode00-$guid.oslab.opentlc.com node00-$guid.oslab.opentlc.com node01-$guid.oslab.opentlc.com; do ssh $node "yum -y  remove NetworkManager*"  ; done

----

. Install Misc tools and utilities on the master
+
----

[root@master00-GUID ~]# yum -y install wget git net-tools bind-utils iptables-services bridge-utils python-virtualenv gcc

----


== Install Docker

OpenShift uses Docker to store and manage container images.  In this lab we install Docker.

. Install the `docker` package on the master host
+
----

[root@master00-GUID ~]# yum -y install docker

----
+
NOTE: We will provide you with a command to do all the node configuration at once, *learn that you need to run these commands on the nodes* but know that we have a command waiting to install them all at once later on.
+
CAUTION: Make sure you `do run all the commands on the master host`.

. Do the same for the rest of the nodes
+
----

[root@master00-GUID ~]# for node in infranode00-$guid.oslab.opentlc.com node00-$guid.oslab.opentlc.com node01-$guid.oslab.opentlc.com; do ssh $node "yum -y install docker"  ; done

----


. Configure the *Docker* registry on all hosts:
+
----

[root@master00-GUID ~]# sed -i "s/OPTIONS.*/OPTIONS='--selinux-enabled --insecure-registry 0.0.0.0\/0'/" /etc/sysconfig/docker

----
. Do the same for the rest of the nodes
+
----

[root@master00-GUID ~]# for node in infranode00-$guid.oslab.opentlc.com node00-$guid.oslab.opentlc.com node01-$guid.oslab.opentlc.com; do scp  /etc/sysconfig/docker $node:/etc/sysconfig/docker ; done

----

== Configure Docker Storage

In this lab we configure the Docker storage pool.

. The default Docker storage configuration uses loopback devices and is not appropriate for production. Red Hat considers the dm.thinpooldev storage option to be the only appropriate configuration for production use.
. Stop the Docker daemon and remove the out of the box loopback docker storage from the host:
+
----

[root@master00-GUID ~]# rm -rf /var/lib/docker/*

----

. Do the same for the rest of the nodes
+
----
 
[root@master00-GUID ~]# for node in infranode00-$guid.oslab.opentlc.com node00-$guid.oslab.opentlc.com node01-$guid.oslab.opentlc.com; do ssh $node "rm -rf /var/lib/docker/*"  ; done
 
----

. In order to use `dm.thinpooldev` you must have space for an LVM thinpool available, the `docker-storage-setup` package will assist you in configuring LVM.  Run `docker-storage-setup` on the infranode host to create logical volumes for Docker:
+
----

[root@master00-GUID ~]# pvcreate /dev/vdb
[root@master00-GUID ~]# vgextend `vgs | grep rhel | awk '{print $1}'` /dev/vdb
[root@master00-GUID ~]# docker-storage-setup

----
+
. You should see the following:
+
----

  Rounding up size to full physical extent 32.00 MiB
  Logical volume "docker-poolmeta" created.
  Logical volume "docker-pool" created.
  WARNING: Converting logical volume rhel_host2cc260760b15/docker-pool and rhel_host2cc260760b15/docker-poolmeta to pool's data and metadata volumes.
  THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
  Converted rhel_host2cc260760b15/docker-pool to thin pool.
  Logical volume "docker-pool" changed.

----
+
[NOTE]
Be careful with `docker-storage-setup` as it will, by default, find any unused extents in the volume group that contains your root filesystem to create the pool.  You can also specify a specific volume group or block device.  This can be a destructive process to the specified VG or block device!  Consult the OpenShift documentation for more information.

. You can use the ssh command to do this from the master host quickly for all hosts
+
----

[root@master00-GUID ~]# for node in infranode00-$guid.oslab.opentlc.com node00-$guid.oslab.opentlc.com node01-$guid.oslab.opentlc.com
do
  ssh $node "pvcreate /dev/vdb ; vgextend `vgs | grep rhel | awk '{print $1}'` /dev/vdb; docker-storage-setup ; "
  ssh $node "systemctl enable docker; reboot "
done

----

. On the master host examine the newly created logical volume `docker-pool`:
+
----

[root@master00-GUID ~]# lvs /dev/rhel_host2cc260760b15/docker-pool

----
+
You should see the following:
+
----

  LV          VG                    Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  docker-pool rhel_host2cc260760b15 twi-a-t--- 5.98g             0.00   0.11

----

. On the master host, examine the docker storage configuration:
+
----

[root@master00-GUID ~]# cat /etc/sysconfig/docker-storage

----
+
You should see the following:
+
----

DOCKER_STORAGE_OPTIONS=-s devicemapper --storage-opt dm.fs=xfs --storage-opt dm.thinpooldev=/dev/mapper/rhel_host2cc260760b15-docker--pool

----

. Enable, start, and get status for the *Docker* service on the master host:
+
----

[root@master00-GUID ~]# systemctl enable docker

----

. Reboot the master host
+
-----

[root@master00-GUID ~]# reboot

-----

== Populate local Docker registry

. Log back into the master host after the reboot from previous lab is complete.

. Log into the infranode and check that the *Docker* service is started
+
----

[root@master00-GUID ~]# guid=`hostname|cut -f2 -d-|cut -f1 -d.`
[root@master00-GUID ~]# ssh infranode00-$guid
[root@infranode00-GUID ~]# systemctl status docker

----
+
You should see the following:
+
----

docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled)
   Active: active (running) since Wed 2015-06-10 15:31:11 EDT; 1s ago
...OUTPUT OMMITTED...

----
+
[NOTE]
Make sure the status shows *enabled* and *active (running)*.

. In order to save time later, we will pre-fetch the docker images to the *infranode00* host. This process will take about 10 minutes to complete:
+
----

[root@infranode00-GUID ~]# RHN="registry.access.redhat.com";PTH="openshift3"
[root@infranode00-GUID ~]# docker pull $RHN/$PTH/ose-haproxy-router:v3.0.0.1 ; \
docker pull $RHN/$PTH/ose-deployer:v3.0.0.1 ; \
docker pull $RHN/$PTH/ose-sti-builder:v3.0.0.1 ; \
docker pull $RHN/$PTH/ose-sti-image-builder:v3.0.0.1 ; \
docker pull $RHN/$PTH/ose-docker-builder:v3.0.0.1 ; \
docker pull $RHN/$PTH/ose-pod:v3.0.0.1 ; \
docker pull $RHN/$PTH/ose-docker-registry:v3.0.0.1 ; \
docker pull $RHN/$PTH/ose-keepalived-ipfailover:v3.0.0.1 ; \
docker pull $RHN/$PTH/ruby-20-rhel7 ; \
docker pull $RHN/$PTH/mysql-55-rhel7 ; \
docker pull $RHN/jboss-eap-6/eap-openshift ; \
docker pull openshift/hello-openshift:v0.4.3

----
+
[NOTE]
This will take about 10 minutes to complete.

. Examine docker pool info on the *infranode00* host:
+
----

[root@infranode00-GUID ~]# docker info

----
+
You should see something like this:
+
----

Containers: 0
Images: 70
Storage Driver: devicemapper
 Pool Name: rhel_host2cc260760b15-docker--pool
 Pool Blocksize: 524.3 kB
 Backing Filesystem: xfs
 Data file:
 Metadata file:
 Data Space Used: 3.5 GB
 Data Space Total: 6.417 GB
 Data Space Available: 2.918 GB
 Metadata Space Used: 1.081 MB
 Metadata Space Total: 33.55 MB
 Metadata Space Available: 32.47 MB
 Udev Sync Supported: true
 Library Version: 1.02.93-RHEL7 (2015-01-28)
Execution Driver: native-0.2
Kernel Version: 3.10.0-229.el7.x86_64
Operating System: Red Hat Enterprise Linux Server 7.1 (Maipo)
CPUs: 2
Total Memory: 1.797 GiB
Name: infranode00-GUID.oslab.opentlc.com
...

----

. On the *infranode00* host examine the `docker-pool` logical volume again:
+
----

[root@infranode00-GUID ~]# lvs /dev/rhel_host2cc260760b15/docker-pool

----
+
You should see something similar to the following:
+
----

  LV          VG                    Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  docker-pool rhel_host2cc260760b15 twi-aot--- 5.98g             54.53  3.22    

----

== Install OpenShift Enterprise


. on the *master00* host, download and unpack the installation utility on a host that has SSH access to your intended master and node hosts
+
----

[root@master00-GUID ~]# curl -o oo-install-ose.tgz https://install.openshift.com/portable/oo-install-ose.tgz
[root@master00-GUID ~]# tar -zxf oo-install-ose.tgz

----

. Optional Tip: Copy the master and node names to your paste buffer.
+
----
[root@master00-GUID ~]# for node in master00-$guid.oslab.opentlc.com infranode00-$guid.oslab.opentlc.com node00-$guid.oslab.opentlc.com node01-$guid.oslab.opentlc.com; do echo $node ; done
master00-GUID.oslab.opentlc.com
infranode00-GUID.oslab.opentlc.com
node00-GUID.oslab.opentlc.com
node01-GUID.oslab.opentlc.com

----

. Execute the installation utility to interactively configure one or more hosts
+
----
[root@master00-GUID ~]# ./oo-install-ose
----
+
[NOTE]
The steps in this section will be changing soon as there is a lot of work being done to add features to the text installer.

. Follow the instructions of the Installer
----
Welcome to the OpenShift Enterprise 3 installation.

Please confirm that following prerequisites have been met:

* All systems where OpenShift will be installed are running Red Hat Enterprise
  Linux 7.
* All systems are properly subscribed to the required OpenShift Enterprise 3
  repositories.
* All systems have run docker-storage-setup (part of the Red Hat docker RPM).
* All systems have working DNS that resolves not only from the perspective of
  the installer but also from within the cluster.

When the process completes you will have a default configuration for Masters
and Nodes.  For ongoing environment maintenance it's recommended that the
official Ansible playbooks be used.

For more information on installation prerequisites please see:
https://docs.openshift.com/enterprise/latest/admin_guide/install/prerequisites.html

Are you ready to continue?  y/Y to confirm, or n/N to abort [n]:
----

. Enter *y* and you should see:
+
----

This installation process will involve connecting to remote hosts via ssh.  Any
account may be used however if a non-root account is used it must have
passwordless sudo access.

User for ssh access [root]: root

----

. Answer *root*, and you should see:
+
----

***Master Configuration***

The OpenShift Master serves the API and web console.  It also coordinates the
jobs that have to run across the environment.  It can even run the datastore.
For wizard based installations the database will be embedded.  It's possible to
change this later using etcd from Red Hat Enterprise Linux 7.

Any Masters configured as part of this installation process will also be
configured as Nodes.  This is so that the Master will be able to proxy to Pods
from the API.  By default this Node will be unscheduleable but this can be changed
after installation with 'oadm manage-node'.

http://docs.openshift.com/enterprise/latest/architecture/infrastructure_components/kubernetes_infrastructure.html#master


Next we will launch an editor for entering masters.  The default editor in your
environment can be overridden exporting the VISUAL environment variable.

Press any key to continue ...


----


. Press any key and then press *i* to enter insert mode then enter the following host:
+
----

master00-GUID.oslab.opentlc.com

----

. Press *ESC* then enter *:wq* to exit `vi`, and you should see:
+
----
1) master00-GUID.oslab.opentlc.com
Please confirm the following masters.  y/Y to confirm, or n/N to edit [n]:
----

. Enter *y* to confirm the master hosts.
+
----

***Node Configuration***

The OpenShift Node provides the runtime environments for containers.  It will
host the required services to be managed by the Master.

By default all Masters will be configured as Nodes.

http://docs.openshift.org/latest/architecture/infrastructure_components/kubernetes_infrastructure.html#node


Next we will launch an editor for entering nodes.  The default editor in your
environment can be overridden exporting the VISUAL environment variable.

Press any key to continue ...

----

. Press any key to continue.

. Press *o* to add a line after master00-GUID and add the infranode and the two nodes, make sure to leave the master host in the list as it is also a node:
+
----

master00-GUID.oslab.opentlc.com
infranode00-GUID.oslab.opentlc.com
node00-GUID.oslab.opentlc.com
node01-GUID.oslab.opentlc.com

----

. Press *ESC* then enter *:wq* to exit `vi`, you should then see:
+
----
1) master00-GUID.oslab.opentlc.com
2) infranode00-GUID.oslab.opentlc.com
3) node00-GUID.oslab.opentlc.com
4) node01-GUID.oslab.opentlc.com


Please confirm the following masters.  y/Y to confirm, or n/N to edit [n]:
----

. Enter *y* to confirm the node hosts.
+
----
Gathering information from hosts...
You'll now be asked to edit a file that will be used to validate settings
gathered from the Masters and Nodes.  Since it's often the case that the
hostname for a system inside the cluster is different from the hostname that is
resolveable from commandline or web clients these settings cannot be validated
automatically.

For some cloud providers the installer is able to gather metadata exposed in
the instance so reasonable defaults will be provided.

Press any key to continue ...


----

. Press any key and verify that the information gathered is correct (order may vary):
+
----
infranode00-GUID.oslab.opentlc.com,192.168.0.101,192.168.0.101,infranode00-GUID.oslab.opentlc.com,infranode00-GUID.oslab.opentlc.com
master00-GUID.oslab.opentlc.com,192.168.0.100,192.168.0.100,master00-GUID.oslab.opentlc.com,master00-GUID.oslab.opentlc.com
node00-GUID.oslab.opentlc.com,192.168.0.200,192.168.0.200,node00-GUID.oslab.opentlc.com,node00-GUID.oslab.opentlc.com
node01-GUID.oslab.opentlc.com,192.168.0.201,192.168.0.201,node01-GUID.oslab.opentlc.com,node01-GUID.oslab.opentlc.com
----

. Enter *:wq* to exit `vi`, and you should see:
+
----
If changes are needed to the values recorded by the installer please update /root/.config/openshift/installer.cfg.yml.

Proceed? y/Y to confirm, or n/N to exit [y]:
----

. Enter *y* to start the install:
+
----
PLAY [Populate oo_masters_to_config host group] *******************************
PLAY [Populate oo_masters_to_config host group] *******************************

TASK: [add_host ] *************************************************************
ok: [localhost] => (item=192.168.0.100)

PLAY [Configure master instances] *********************************************

GATHERING FACTS ***************************************************************
ok: [192.168.0.100]

TASK: [os_firewall | Install firewalld packages] ******************************
skipping: [192.168.0.100]

TASK: [os_firewall | Check if iptables-services is installed] *****************

....
....
....

PLAY RECAP ********************************************************************
infranode00-GUID.oslab.opentlc.com : ok=40   changed=0    unreachable=0    failed=0
localhost                  : ok=5    changed=0    unreachable=0    failed=0
master00-GUID.oslab.opentlc.com : ok=94   changed=0    unreachable=0    failed=0
node00-GUID.oslab.opentlc.com : ok=40   changed=0    unreachable=0    failed=0
node01-GUID.oslab.opentlc.com : ok=40   changed=0    unreachable=0    failed=0


The installation was successful!

If this is your first time installing please take a look at the Administrator
Guide for advanced options related to routing, storage, authentication and much
more:

http://docs.openshift.com/enterprise/latest/admin_guide/overview.html

Press any key to continue ...
Removing temporary assets.
Please see /tmp/oo-install-ose-20150630-2050.log for full output.

The installation was successful!

----

. After the installer is complete press any key as requested and reboot the master host:
+
----
root@master00-GUID ~]# reboot
----

. Log back into the *master* and check the status of your host using the `oc get nodes` command:
+
----

root@master00-GUID ~]# oc get nodes
NAME                                 LABELS                                                                        STATUS
infranode00-GUID.oslab.opentlc.com   kubernetes.io/hostname=infranode00-GUID.oslab.opentlc.com                     Ready
master00-GUID.oslab.opentlc.com      kubernetes.io/hostname=master00-GUID.oslab.opentlc.com							    Ready,SchedulingDisabled
node00-GUID.oslab.opentlc.com        kubernetes.io/hostname=node00-GUID.oslab.opentlc.com                          Ready
node01-GUID.oslab.opentlc.com        kubernetes.io/hostname=node01-GUID.oslab.opentlc.com                          Ready



----

== Set Regions and Zones

The assignments of "regions" and "zones" at the node-level are handled by labels
on the nodes.

. Label the nodes
+
----
root@master00-GUID ~]# oc label node infranode00-$guid.oslab.opentlc.com region="infra" zone="infranodes"
root@master00-GUID ~]# oc label node node00-$guid.oslab.opentlc.com region="primary" zone="east"
root@master00-GUID ~]# oc label node node01-$guid.oslab.opentlc.com region="primary" zone="west"
----


. On the master host look at how the labels were implemented with `oc get nodes`:
+
----

[root@master00-GUID ~]# oc get nodes

----
+
You should see:
+
----

NAME                                 LABELS                                                                                   STATUS
infranode00-GUID.oslab.opentlc.com   kubernetes.io/hostname=infranode00-GUID.oslab.opentlc.com,region=infra,zone=infranodes   Ready
master00-GUID.oslab.opentlc.com      kubernetes.io/hostname=master00-GUID.oslab.opentlc.com,region=infra,zone=na              Ready,SchedulingDisabled
node00-GUID.oslab.opentlc.com        kubernetes.io/hostname=node00-GUID.oslab.opentlc.com,region=primary,zone=east            Ready
node01-GUID.oslab.opentlc.com        kubernetes.io/hostname=node01-GUID.oslab.opentlc.com,region=primary,zone=west            Ready

----

At this point we have a running OpenShift environment across three hosts, with
one master and three nodes, divided up into two regions -- "infrastructure"
and "primary".

From here we will start to deploy "applications" and other resources into
OpenShift.

### Useful OpenShift Logs

RHEL 7 uses `systemd` and `journal`. As such, looking at logs is not a matter of
`/var/log/messages` any longer. You will need to use `journalctl`.

Since we are running all of the components in higher loglevels, it is suggested
that you use your terminal emulator to set up windows for each process.

On the master host you should run each of the following in its own
window:

----

[root@master00-GUID ~]# journalctl -f -u openshift-master
[root@master00-GUID ~]# journalctl -f -u openshift-node

----

[NOTE]
You will want to do this on the other nodes, but you won't need the
`openshift-master` service. You may also wish to watch the Docker logs, too.

= The Registy and Router

In the scenario we are simulating in the lab, we are using *Infranode00* as the target for both the *registry* and the *default router*.

. If you wanted to make a node unschedulable, you could use this command:
.. This isn't needed in our environment as our master is already made unschedulable by the installer
+
----
[root@master00-GUID ~]# oadm manage-node master00-$guid.oslab.opentlc.com  --schedulable=false
master00-GUID.oslab.opentlc.com   kubernetes.io/hostname=master00-GUID.oslab.opentlc.com,region=infra,zone=na   Ready,SchedulingDisabled
----

. Checkout the output of *oc get nodes*
+
----
[root@master00-GUID ~]# oc get nodes
NAME                                 LABELS                                                                                   STATUS
infranode00-GUID.oslab.opentlc.com   kubernetes.io/hostname=infranode00-GUID.oslab.opentlc.com,region=infra,zone=infranodes   Ready
master00-GUID.oslab.opentlc.com      kubernetes.io/hostname=master00-GUID.oslab.opentlc.com,region=infra,zone=na              Ready,SchedulingDisabled
node00-GUID.oslab.opentlc.com        kubernetes.io/hostname=node00-GUID.oslab.opentlc.com,region=primary,zone=east            Ready
node01-GUID.oslab.opentlc.com        kubernetes.io/hostname=node01-GUID.oslab.opentlc.com,region=primary,zone=west            Ready

----

. Deploy the *Registry*
+
----
[root@master00-GUID ~]# oadm registry  --credentials=/etc/openshift/master/openshift-registry.kubeconfig  --images='registry.access.redhat.com/openshift3/ose-docker-registry:v3.0.0.1' --selector='region=infra'
----

. On a separate terminal connected to the master host, watch the output of the `oc get nodes` command:
+
----

[root@master00-GUID ~]# watch oc get pods

----

.. You can look at the status of your pod using the following commands, This can take a few minutes the first time around as the images are being pulled from the registry:
+
----

NAME                       READY     REASON    RESTARTS   AGE
docker-registry-1-deploy   0/1       Running   0          6s

... Wait a few seconds ...

NAME                      READY     REASON    RESTARTS   AGE
docker-registry-1-j6hdu   1/1       Running   0          59s

----

. Deploy the *Default Router*
+
----
[root@master00-GUID ~]# oadm router trainingrouter --stats-password='r3dh@t1!' --replicas=1 \
--config=/etc/openshift/master/admin.kubeconfig  \
--credentials='/etc/openshift/master/openshift-router.kubeconfig' \
--images='registry.access.redhat.com/openshift3/ose-haproxy-router:v3.0.0.1' \
--selector='region=infra'
----

. You should see the following output:
+
----
deploymentconfigs/trainingrouter
services/trainingrouter
----

.. In the seperate terminal watch the status of your pods:
+
----
[root@master00-GUID ~]# watch oc get pods
NAME                      READY     REASON    RESTARTS   AGE
...
trainingrouter-1-deploy   0/1       Pending   0          4s

.. Wait a few seconds ..

NAME                      READY     REASON    RESTARTS   AGE
...
trainingrouter-1-22mr1    0/1       Pending   0          2s
trainingrouter-1-deploy   1/1       Running   0          8s

.. Wait a few seconds ..

NAME                      READY     REASON    RESTARTS   AGE
...
trainingrouter-1-22mr1    0/1       Running   0          8s
trainingrouter-1-deploy   1/1       Running   0          14s

----

.. You would probably also have the Docker registry pods listed in the output above.

.. Press *CTRL+C* to exit the watch on `oc get pods`.

=== Populating OpenShift

OpenShift ships with *Image Streams* and *Templates*, they reside in: `/usr/share/openshift/examples/`.  The installer will import all of the ImageStreams and Templates for you from this directory.

. Take a look at the JSON files in `/usr/share/openshift/examples`

. The remaining steps in this lab are for reference only:

. If for some reason you had to re-create the core set of image streams, that use the Red Hat Enterprise Linux (RHEL) 7 based images you would use the following:
----

 oc create -f /usr/share/openshift/examples/image-streams/image-streams-rhel7.json -n openshift
----

. If you had to create the core set of database templates:
+
----
 oc create -f /usr/share/openshift/examples/db-templates -n openshift
----

. If you had to create the core QuickStart templates:
+
----

 oc create -f /usr/share/openshift/examples/quickstart-templates -n openshift

----

