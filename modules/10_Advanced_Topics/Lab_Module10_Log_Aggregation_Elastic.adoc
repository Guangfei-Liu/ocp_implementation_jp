== Lab: Log Aggregation Using Elasticsearch (BROKEN)

Elasticsearch is an open source distributed document database that indexes
documents and provides full-text search capabilities. By storing container logs
in Elasticsearch, users are able to search all content and filter appropriately.
This documentation shows how to run Kibana.
This option requires more configuration and more resources than the centralized
file system option, but makes logs more useful for troubleshooting and fault
finding.

Enabling aggregated logging to Elasticsearch involves:

* Creating an Elasticsearch cluster
* Creating logging pods
* Creating the Kibana service

=== Creating an Elasticsearch cluster

Logs are stored in an Elasticsearch cluster running on OpenShift. This cluster
is scalable using a replication controller, so you can scale the Elasticsearch
cluster up and down as required.

. Create the manifest for the Elasticsearch cluster:
+
[source,yaml]
----
cat << EOF > Elasticsearch.cluster.yaml
apiVersion: "v1"
kind: "List"
items:
-
  apiVersion: "v1"
  kind: "Service"
  metadata:
    labels:
      provider: "fabric8"
      component: "elasticsearch"
    name: "es-logging"
  spec:
    ports:
    -
      port: 9200
      targetPort: 9200
    selector:
      provider: "fabric8"
      component: "elasticsearch"
-
  apiVersion: "v1"
  kind: "Service"
  metadata:
    labels:
      provider: "fabric8"
      component: "elasticsearch"
    name: "es-logging-cluster"
  spec:
    portalIP: "None"
    ports:
    -
      port: 9300
      targetPort: 9300
    selector:
      provider: "fabric8"
      component: "elasticsearch"
-
  apiVersion: "v1"
  kind: "ReplicationController"
  metadata:
    labels:
      provider: "fabric8"
      component: "elasticsearch"
    name: "elasticsearch"
  spec:
    replicas: 1
    selector:
      provider: "fabric8"
      component: "elasticsearch"
    template:
      metadata:
        labels:
          provider: "fabric8"
          component: "elasticsearch"
      spec:
        containers:
          -
            env:
            -
              name: "KUBERNETES_TRUST_CERT"
              value: "true"
            -
              name: "SERVICE_DNS"
              value: "infranode00-${GUID}.oslab.opentlc.com"
            image: "fabric8/elasticsearch-k8s:1.5.2"
            securityContext:
              privileged: true
            name: "elasticsearch"
            nodeSelector:
              region: infra
            ports:
            -
              containerPort: 9200
              name: "http"
            -
              containerPort: 9300
              name: "transport"

EOF

----

. Use the manifest to create the Elasticsearch cluster node:
+
----
$ oc create -f Elasticsearch.cluster.yaml
----

NOTE: This starts a single Elasticsearch instance. If you need to create a
larger cluster, you can scale the Elasticsearch replication controller using:
$ oc scale --replicas=3 rc elasticsearch

=== Creating Logging Pods

To read the container logs, a static pod must be deployed on each node.
To do this, you must first ensure that the node is configured to read local pod
manifest configuration files.
This is enabled by configuring the podManifestConfig in the node-config.yaml
file on each node, changing the configuration path and check interval
appropriately:

. Connect to the *infranode00* server and run the following commands:
. To create the logging pod, create a file with the following contents in the
directory specified by podManifestConfig.path below:
+
----
[root@infranode00-GUID ~]# mkdir -p  /var/lib/openshift/openshift.local.manifests
[root@infranode00-GUID ~]#

 oc delete -f /var/lib/openshift/openshift.local.manifests/fluentd-elasticsearch.yaml
cat << EOF > /var/lib/openshift/openshift.local.manifests/fluentd-elasticsearch.yaml
apiVersion: v1
kind: Pod
metadata:
  name: fluentd-elasticsearch
  label:
    component: "elasticsearch"
spec:
  label:
    component: "elasticsearch"
  containers:
  - name: fluentd-elasticsearch

    image: fabric8/fluentd-kubernetes:1.0
    env:
    -
      name: "HOST"
      value: "infranode00-${GUID}.oslab.opentlc.com"
    -
      name: "host"
      value: "infranode00-${GUID}.oslab.opentlc.com"
    securityContext:
      privileged: true
    resources:
      limits:
        cpu: 100m
    volumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: varlibdockercontainers
      mountPath: /var/lib/docker/containers
      readOnly: true
  volumes:
  - name: varlog
    hostPath:
      path: /var/log
  - name: varlibdockercontainers
    hostPath:
      path: /var/lib/docker/containers
EOF
oc create -f /var/lib/openshift/openshift.local.manifests/fluentd-elasticsearch.yaml
 oc describe pod fluentd-elasticsearch



----

. Configure the node to run the all manifests in our local manifest directory
+
----
[root@infranode00-GUID ~]# cp /etc/openshift/node/node-config.yaml /etc/openshift/node/node-config.yaml.original
[root@infranode00-GUID ~]# sed -i 's%podManifestConfig:%podManifestConfig:\n  path: /var/lib/openshift/openshift.local.manifests\n  fileCheckIntervalSeconds: 10%g' /etc/openshift/node/node-config.yaml
[root@infranode00-GUID ~]# systemctl restart openshift-node

----

. Do the same for the rest of the nodes:
+
----
[root@infranode00-GUID ~]# for node in node00-$guid.oslab.opentlc.com node01-$guid.oslab.opentlc.com; do
ssh $node "
mkdir -p  /var/lib/openshift/openshift.local.manifests
cp /etc/openshift/node/node-config.yaml /etc/openshift/node/node-config.yaml.original
sed -i 's%podManifestConfig:%podManifestConfig:\n  path: /var/lib/openshift/openshift.local.manifests\n  fileCheckIntervalSeconds: 10%g' /etc/openshift/node/node-config.yaml
systemctl restart openshift-node

scp /var/lib/openshift/openshift.local.manifests/fluentd-elasticsearch.yaml ${node}:/var/lib/openshift/openshift.local.manifests/fluentd-elasticsearch.yaml ;
done


----





This starts a pod on the node and posts the container logs to Elasticsearch.

To validate it is working, you can query Elasticsearch and check that the data is correctly being persisted. First, identify one of the Elasticsearch pods:

$ oc get pods -l component=elasticsearch

Then query Elasticsearch, replacing the pod ID with one returned from the above command:

$ oc exec -p <pod_id> -c elasticsearch -- curl -s localhost:9200/_cat/indices?v

You should see output similar to the following:

health status index               pri rep docs.count docs.deleted store.size pri.store.size
yellow open   logstash-2015.06.05   5   1        540            0      251kb          251kb

If the value for docs.count is more than 0, then log records are being correctly sent to Elasticsearch.
Creating the Kibana Service

To create the Kibana service, save the following specification to your file system:

apiVersion: "v1"
kind: "List"
items:
-
  apiVersion: "v1"
  kind: "Service"
  metadata:
    name: "kibana"
  spec:
    ports:
      -
        port: 80
        targetPort: "kibana-port"
    selector:
      provider: fabric8
      component: "kibana"
-
  apiVersion: "v1"
  kind: "ReplicationController"
  metadata:
    name: "kibana"
    labels:
      provider: fabric8
      component: "kibana"
  spec:
    replicas: 1
    selector:
      component: "kibana"
    template:
      metadata:
        name: "kibana"
        labels:
          provider: fabric8
          component: "kibana"
      spec:
        containers:
          -
            name: "kibana"
            image: "fabric8/kibana4:4.0.2"
            ports:
              -
                name: "kibana-port"
                containerPort: 5601
            env:
              -
                name: "ELASTICSEARCH_URL"
                value: "http://es-logging:9200"

Create the Kibana replication controller and service:

$ oc create -f path/to/kibana.yaml
