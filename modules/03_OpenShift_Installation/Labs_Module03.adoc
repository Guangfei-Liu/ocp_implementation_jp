:toc2:
:icons: images/icons

== OpenShift Installation Lab


=== Overview

This lab consists of four sections:

.Prepare to Deploy OpenShift Container Platform

In this section you prepare hosts for installing OpenShift Container Platform, configure Domain Name System (DNS) and Network File System (NFS) servers on the administration host, configure the settings, and install Docker.

.Install OpenShift Container Platform

In this section you install OpenShift Container Platform with the quick installer.

.Configure and Set Up OpenShift

In this section you configure OpenShift Container Platform. You label nodes, configure authentication, and deploy the registry and default router containers on the infranode node host.

.Set Up Persistent Storage

In this section you prepare the OpenShift cluster to use NFS storage as a persistent volume provider.

[NOTE]
The default `root` password is `r3dh4t1!`.


=== Notes

IMPORTANT: Read these notes. They help you successfully navigate the lab.

* You run many commands in this lab remotely from the `bastion` or `master1` host. The instructions and command-line prompts always specify the host from which to run the commands.

* Many steps require you to run a command on a host (usually `master1`) and then run the same command with a `for` loop on the rest of the nodes. When that occurs, examine the output of those commands to ensure that they completed correctly.

* This lab is long and extremely important. All future labs depend on the success of your deployment.

* Depending on your workstation's screen size, resolution, and browser, some commands may lose their intended formatting, appearing with extra line breaks. Even though this lab endeavors to avoid this pitfall, the formatting still "breaks" a command sometimes. Pay attention to the command you paste in and correct the formatting if necessary.

=== Known Issues

[cols="1,5,5",options="header"]
|=======================================================================
|Issue | Description | Workaround
|Failure of Docker storage.
|The Docker daemon cannot access the storage disks following the `docker-storage-setup` storage configuration. This problem sometimes occurs when the hosting platform reuses disks by accident.
|Run `lvremove`, `vgremove`, and `pvremove` to free the `/dev/vdb` disks and then run `fdisk` to delete the `/dev/vdb1` partition. Subsequently, run `docker-storage-setup` again and restart the Docker daemon.
|Failure of `infranode1` containers.
|The router and registry (containers on `infranode1`) stop responding or do not deploy--sometimes after an automatic shutdown of the virtual machine, sometimes at random.
|Reboot `infranode1` and redeploy the containers.
|The master daemon does not start after you configure authentication.
|This problem is most likely due to YAML's space sensitivity.
|Revert to the original configuration (a copy is available from before) and try again. YAML is space-sensitive and might require several tries to work.
|=======================================================================

:numbered:

== Prepare to Deploy OpenShift Container Platform

In this section you deploy OpenShift Container Platform on a master and two nodes, as follows:

* Configure the Secure Shell (SSH) keys.
* Configure the repositories.
* Configure the DNS on the `bastion` server for your OpenShift Container Platform environment.
* Configure the network settings.
* Install Docker on all hosts.

[NOTE]
*Reminder: The administration host is the only host that you can access with SSH outside the lab environment.* External SSH for all other hosts is blocked. From the administration host, you can access the other hosts internally through SSH. As described earlier, you must access the system (not as `root`) with your private SSH key and OPENTLC login.

Each lab is assigned a global unique identifier (GUID) with four characters, which you receive by email when provisioning your lab environment. *From this point on, replace _GUID_ with your lab's four-character GUID.*

.Your Environment

* The lab environment consists of the following five virtual machines (VMs):
+
[cols="3,7",options="header"]
|===
| Virtual Machine | Description

| `bastion.example.com`| Administration host; acts as the DNS server and NFS server and host from which you install the environment.

| `master1.example.com`| Master host; contains `etcd` and the management console.

| `infranode1.example.com`| Infranode host; a regular node for running only the infrastructure containers (registry and router).

| `node1.example.com`| Node host (region: primary, zone: east).

| `node2.example.com`| Node host (region: primary, zone: west).
|===

* In the lab exercises in this section, use the `bastion` host as your DNS and NFS server. Run remote commands on the OpenShift environment on the provisioning and staging host.

* `bastion` is *not* an OpenShift cluster member or part of the OpenShift environment. That host mimics your client's infrastructure or your laptop or desktop that is connected to the client's local area network (LAN).


.Important Details

* Run most, *but not all*, of your commands from the `bastion` host.
* When executing instructions on nodes or hosts:
- As a rule, run the commands on a specific server and examine the output.
- Execute the commands on the rest of the nodes or hosts with a `for` loop
 to save time and effort.
- In some cases, in the interest of saving time, run commands directly on the nodes or hosts instead of using the `for` loop.
* The `$guid/$GUID` environment variables are already defined on all hosts.
- For the GUID variable in links or file definitions, replace _GUID_ with its value.
- Administration host example:
+
----
[root@bastion ~]# command
----
- Master host example:
+
----
[root@master1 ~]# command
----

IMPORTANT: In each step, ensure that you are running the step on the required host. Each step contains the host name. The example code contains the host name in the shell prompt.

[TIP]
====
Red Hat highly recommends that you use a terminal multiplexing tool, such as *tmux* or *screen*, which keeps your place in the session if you are disconnected from your environment. You can install their packages after setting up the `rhel` repositories.

If you use tmux, type *Ctrl+B* (to enter "scroll mode") + page up or down to scroll, and use the *Esc* key to exit scroll mode.

You can detach from tmux : `Ctrl+B  D` or simply close you terminal. To attach again an existing tmux session, run the command `tmux attach` once you're connected to the bastion host.
====

=== Connect to Environment

When you connect to `bastion-GUID.oslab.opentlc.com` for the first time, you will have to accept the server SSH fingerprint. Reply 'yes': it will be added to your `known_hosts` and not asked next time you connect.

. Connect to your administration host `bastion-GUID.oslab.opentlc.com`. Note that your private key location may vary.
+
----
yourdesktop$ ssh -i ~/.ssh/id_rsa your-opentlc-login@bastion-GUID.oslab.opentlc.com
----

* Example of a successful connection:
+
----
-bash-4.2$
[gucore@work ~]$ ssh gucore-redhat.com@bastion-9e91.oslab.opentlc.com
The authenticity of host 'bastion-9e91.oslab.opentlc.com (31.220.66.121)' can't be established.
ECDSA key fingerprint is SHA256:fR4vFVswyvpj/Jevfin3+X0Fkehbfx4HBjw46AeIS14.
ECDSA key fingerprint is MD5:bb:25:92:ee:c9:ba:23:71:b7:c1:f7:2d:89:6d:b0:66.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'bastion-9e91.oslab.opentlc.com,31.220.66.121' (ECDSA) to the list of known hosts.
Creating home directory for gucore-redhat.com.
[gucore-redhat.com@bastion-9e91 ~]$ 
----

. Run `sudo` to become the `root` user on the administration host:
+
----
[gucore-redhat.com@bastion-9e91 ~]$  sudo -i
[root@bastion-9e91 ~]#
----

=== Configure SSH Keys

The OpenShift Container Platform installer configures hosts with SSH. In this section you create and install an SSH key pair on the `bastion` host and add the public key to the `authorized_hosts` file on the OpenShift hosts.

. Create an SSH key pair for the `root` user and overwrite the existing key:
+
----
[root@bastion ~]# ssh-keygen -f /root/.ssh/id_rsa -N ''
----
+
NOTE: In a different environment, you can adopt a nonroot user with `sudo`
 capabilities. For example, in Amazon Web Services (AWS), you adopt the `ec2-user` user.

. On the `bastion` host, add the public SSH key locally to `/root/.ssh/authorized_keys`:
+
----
[root@bastion ~]# cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys
----

. Configure `/etc/ssh/ssh_config` to disable `StrictHostKeyChecking` on the
 `bastion` and master hosts:
+
----
[root@bastion ~]# echo StrictHostKeyChecking no >> /etc/ssh/ssh_config
[root@bastion ~]# ssh master1.example.com "echo StrictHostKeyChecking no >> /etc/ssh/ssh_config"
----
* This configuration avoids your having to disable strict host-checking and to reply yes when running remote commands on unknown hosts. 
* You will run many commands from both the `bastion` and `master1` hosts.


. On the `bastion` host, test the new SSH key by connecting it to itself over
 the loopback interface without a keyboard prompt:
+
----
[root@bastion ~]# ssh 127.0.0.1
...[output omitted]...
[root@bastion ~]# exit
----

. Copy the SSH key to the rest of the nodes in the environment. When prompted, specify the root password for each of the nodes.
+
----
[root@bastion ~]# for node in   master1.example.com \
                                    infranode1.example.com \
                                    node1.example.com \
                                    node2.example.com; \
                                    do \
                                    ssh-copy-id root@$node ; \
                                    done
----
+
[NOTE]
The default `root` password is `r3dh4t1!`.

=== Configure Repositories on All Hosts

OpenShift Container Platform requires four software repositories:

* `rhel-7-server-rpms`

* `rhel-7-server-extras-rpms`

* `rhel-7-server-optional-rpms`

* `rhel-7-server-ose-3.x-rpms`

Normally, you obtain these repositories through `subscription-manager`. For the sake of expediency, a mirror is available for you. Configure it as follows:

. On the `bastion` host, set up the `yum` repository configuration file
 `/etc/yum.repos.d/open.repo` with the following repositories:
+
----
[root@bastion ~]# cat << EOF > /etc/yum.repos.d/open.repo
[rhel-x86_64-server-7]
name=Red Hat Enterprise Linux 7
baseurl=http://www.opentlc.com/repos/ose/3.3/rhel-7-server-rpms
enabled=1
gpgcheck=0

[rhel-x86_64-server-extras-7]
name=Red Hat Enterprise Linux 7 Extras
baseurl=http://www.opentlc.com/repos/ose/3.3/rhel-7-server-extras-rpms
enabled=1
gpgcheck=0

[rhel-x86_64-server-optional-7]
name=Red Hat Enterprise Linux 7 Optional
baseurl=http://www.opentlc.com/repos/ose/3.3/rhel-7-server-optional-rpms
enabled=1
gpgcheck=0

# This repo is added for the OPENTLC environment not OSE
[rhel-x86_64-server-rh-common-7]
name=Red Hat Enterprise Linux 7 Common
baseurl=http://www.opentlc.com/repos/ose/3.3/rhel-7-server-rh-common-rpms
enabled=1
gpgcheck=0

EOF
----

. Add the OpenShift Container Platform repository mirror to the `bastion` host:
+
----
[root@bastion ~]# cat << EOF >> /etc/yum.repos.d/open.repo
[rhel-7-server-ose-3.3-rpms]
name=Red Hat Enterprise Linux 7 OSE 3.3
baseurl=http://www.opentlc.com/repos/ose/3.3/rhel-7-server-ose-3.3-rpms
enabled=1
gpgcheck=0

EOF
----

. List the repositories on the `bastion` host:
+
-----
[root@bastion ~]# yum clean all ; yum repolist
-----

* The output is as follows:
+
----
Loaded plugins: product-id
...[output omitted]...
repo id                                        repo name                                           status
rhel-7-server-ose-3.3-rpms                     Red Hat Enterprise Linux 7 OSE 3                      323
rhel-x86_64-server-7                           Red Hat Enterprise Linux 7                          4,391
rhel-x86_64-server-extras-7                    Red Hat Enterprise Linux 7 Extras                      45
rhel-x86_64-server-optional-7                  Red Hat Enterprise Linux 7 Optional                 4,220
rhel-x86_64-server-rh-common-7                 Red Hat Enterprise Linux 7 Common                      19
repolist: 8,998

...[output omitted]...
----

. Configure the master nodes by copying the `open.repo` file to all of the nodes
 directly from the `bastion` host:
+
-----
[root@bastion ~]# for node in master1.example.com \
                                    infranode1.example.com \
                                    node1.example.com \
                                    node2.example.com; \
                                    do \
                                      echo Copying open repos to $node ; \
                                      scp /etc/yum.repos.d/open.repo ${node}:/etc/yum.repos.d/open.repo ;
                                      ssh ${node} yum clean all
                                      ssh ${node} yum repolist
                                   done
-----


=== Configure Wildcard DNS Entry on `bastion`

OpenShift Container Platform requires a wildcard DNS A record, which must point to the publicly available IP address of a node or nodes that are hosting the OpenShift default router container.


NOTE: In the OpenShift environment, the OpenShift default router is deployed on the `infranode1` host.


. Install the `bind` and `bind-utils` packages on the administration host:
+
----
[root@bastion ~]# yum -y install bind bind-utils
----

. Verify that you have correctly configured the `$GUID` and `$guid` environment variables:
+
----
[root@bastion ~]# echo GUID is $GUID and guid is $guid
----

* The output is similar to this:
+
----
GUID is c0fe and guid is c0fe
----

* If the environment variables `$GUID` and `$guid` *are not set*, run the following commands:
+
----
[root@bastion ~]# export GUID=`hostname|cut -f2 -d-|cut -f1 -d.`
[root@bastion ~]# export guid=`hostname|cut -f2 -d-|cut -f1 -d.`

----

. On the administration host, `bastion`, do the following:

.. Collect and define the environment's information. 
.. Define the public IP address of `infranode1` as the target of the wildcard record.

* The following commands use the `host` command against the server `ipa.opentlc.com` to get the public IP address, and so should be run on the same line:
+
----
[root@bastion ~]# host infranode1-$GUID.oslab.opentlc.com ipa.opentlc.com |grep infranode | awk '{print $4}'
[root@bastion ~]# HostIP=`host infranode1-$GUID.oslab.opentlc.com  ipa.opentlc.com |grep infranode | awk '{print $4}'`
[root@bastion ~]# domain="cloudapps-$GUID.oslab.opentlc.com"
[root@bastion ~]# echo $HostIP $domain
----

. Create the zone file with the wildcard DNS:
+
----
[root@bastion ~]# mkdir /var/named/zones
[root@bastion ~]# echo "\$ORIGIN  .
\$TTL 1  ;  1 seconds (for testing only)
${domain} IN SOA master.${domain}.  root.${domain}.  (
  2011112904  ;  serial
  60  ;  refresh (1 minute)
  15  ;  retry (15 seconds)
  1800  ;  expire (30 minutes)
  10  ; minimum (10 seconds)
)
  NS master.${domain}.
\$ORIGIN ${domain}.
test A ${HostIP}
* A ${HostIP}"  >  /var/named/zones/${domain}.db
[root@bastion ~]# cat /var/named/zones/${domain}.db
----

. Configure `named.conf`:
+
----
[root@bastion ~]# echo "// named.conf
options {
  listen-on port 53 { any; };
  directory \"/var/named\";
  dump-file \"/var/named/data/cache_dump.db\";
  statistics-file \"/var/named/data/named_stats.txt\";
  memstatistics-file \"/var/named/data/named_mem_stats.txt\";
  allow-query { any; };
  recursion yes;
  /* Path to ISC DLV key */
  bindkeys-file \"/etc/named.iscdlv.key\";
  forwarders {
   192.168.0.1;
  };
  allow-recursion { 192.168.0.0/16; };
};
logging {
  channel default_debug {
    file \"data/named.run\";
    severity dynamic;
  };
};
zone \"${domain}\" IN {
  type master;
  file \"zones/${domain}.db\";
  allow-update { key ${domain} ; } ;
};" > /etc/named.conf
[root@bastion ~]# cat /etc/named.conf
----

. Correct the file permissions and start the DNS server:
+
----
[root@bastion ~]#  chgrp named -R /var/named ; \
 chown named -Rv /var/named/zones ; \
 restorecon -Rv /var/named ; \
 chown -v root:named /etc/named.conf ; \
 restorecon -v /etc/named.conf ;
----

. Enable and start `named`:
+
----
[root@bastion ~]# systemctl enable named && \
 systemctl start named
----

. Configure `iptables` to allow inbound DNS queries:
+
----
[root@bastion bin]# iptables -I INPUT 1 -p tcp --dport 53 -s 0.0.0.0/0 -j ACCEPT ; \
iptables -I INPUT 1 -p udp --dport 53 -s 0.0.0.0/0 -j ACCEPT ; \
iptables-save > /etc/sysconfig/iptables
----

=== Verify DNS Configuration

A test DNS entry called `test.cloudapps-GUID.oslab.opentlc.com` is available.

. Test the DNS server on the administration host:
+
----
[root@bastion ~]# host test.cloudapps-$GUID.oslab.opentlc.com 127.0.0.1
----

. Test with an external name server:
+
----
[root@bastion ~]# host test.cloudapps-$GUID.oslab.opentlc.com 8.8.8.8
----

* The first time you query `8.8.8.8`, you may notice some lag and see the error message `Connection timed out; trying next origin Host test.cloudapps-GUID.oslab.opentlc.com not found: 3(NXDOMAIN).` This is normal. Rerunning the test results in faster performance and no errors.

. Test DNS from your laptop or desktop. Be sure to replace _GUID_ with the correct value. The update may take a few minutes.
+
----
Desktop$ nslookup test.cloudapps-$GUID.oslab.opentlc.com
----


=== Install and Configure Ansible on `bastion`

The advanced installation method is based on Ansible playbooks, so you must be able to directly invoke Ansible.

. Install Ansible from `yum`:
+
----
[root@bastion ~]# yum -y install ansible
----

. Create a simple inventory file with groups used by Ansible:
+
----
[root@bastion ~]# cat << EOF > /etc/ansible/hosts
[masters]
master1.example.com

[nodes]
master1.example.com
infranode1.example.com
node1.example.com
node2.example.com
EOF
[root@bastion ~]# cat /etc/ansible/hosts
----

. Test the Ansible configuration:
+
----
[root@bastion ~]# ansible nodes -m ping
master1.example.com | success >> {
    "changed": false,
    "ping": "pong"
}

infranode1.example.com | success >> {
    "changed": false,
    "ping": "pong"
}

node1.example.com | success >> {
    "changed": false,
    "ping": "pong"
}

node2.example.com | success >> {
    "changed": false,
    "ping": "pong"
}
----

=== Install Packages

Although NetworkManager could be removed in earlier versions of OpenShift, it has been recommended since version 3.2 and required in version 3.3.

. On the `bastion` host, run the following `for` loop to ensure that `NetworkManager` is installed on the master and all nodes:
+
----
[root@bastion ~]# for node in   master1.example.com \
                               infranode1.example.com \
                               node1.example.com \
                               node2.example.com; \
                               do \
                               echo installing NetworkManager on $node ; \
                                 ssh $node "yum -y install NetworkManager"
                               done
----
TIP: You can also use this Ansible command: `ansible nodes -a "yum -y install NetworkManager"` or `ansible nodes -m yum -a "name=NetworkManager"`


. Install the following tools and utilities on the `bastion` host:
+
----
[root@bastion ~]# yum -y install wget git net-tools bind-utils iptables-services bridge-utils
----

. Install `bash-completion` on both the `bastion` and `master` hosts:
+
----
[root@bastion ~]# yum -y install bash-completion
[root@bastion ~]# ssh master1.example.com yum -y install bash-completion
----
+

NOTE: `bash-completion` is not available for use until the `bash` shell is restarted.

. Run `yum update` on the master and all nodes:
+
----
[root@bastion ~]# for node in master1.example.com \
                                    infranode1.example.com \
                                    node1.example.com \
                                    node2.example.com; \
                                    do \
                                    echo Running yum update on $node ; \
                                    ssh $node "yum -y update " ; \
                                    done

----
+
TIP: You can also use this Ansible command: `ansible all -a "yum -y update"`.

=== Install Docker

OpenShift Container Platform stores and manages container images on Docker. Install Docker as follows:

* Install the `docker` package on the master and all nodes:
+
----
[root@bastion ~]# for node in master1.example.com \
                             infranode1.example.com \
                             node1.example.com \
                             node2.example.com; \
                             do \
                             echo Installing docker on $node ; \
                             ssh $node "yum -y install docker" ;
                             done
----

TIP: You can also use this Ansible command: `ansible nodes -m yum -a "name=docker"`.

=== Configure Docker Storage

Next, configure the Docker storage pool.

NOTE: The default configuration of loopback devices for the Docker storage does not support production. Red Hat considers the `dm.thinpooldev` storage option to be the only appropriate configuration for a production environment.

. Stop the Docker daemon and delete any files from `/var/lib/docker`:
+
----
[root@bastion ~]# for node in master1.example.com \
                             infranode1.example.com \
                             node1.example.com \
                             node2.example.com; \
                             do
                             echo Cleaning up Docker on $node ; \
                             ssh $node "systemctl stop docker ; rm -rf /var/lib/docker/*"  ;
                             done
----
+
TIP: You can also use this Ansible command: `ansible nodes -m shell -a "systemctl stop docker ; rm -rf /var/lib/docker/*"`.

. Specify the `/dev/vdb` hard drive as the Docker volume group for `docker-storage setup`:
+
----
[root@bastion ~]# ssh master1.example.com
[root@master1 ~]# cat <<EOF > /etc/sysconfig/docker-storage-setup
DEVS=/dev/vdb
VG=docker-vg
EOF

----

. Run `docker-storage-setup` on the `master1` host to create logical volumes
 for Docker:
+
----
[root@master1 ~]# docker-storage-setup
----

* The output is as follows:
+
----

Checking that no-one is using this disk right now ...
OK

Disk /dev/vdb: 20805 cylinders, 16 heads, 63 sectors/track
sfdisk:  /dev/vdb: unrecognized partition table type

Old situation:
sfdisk: No partitions found

New situation:
Units: sectors of 512 bytes, counting from 0

   Device Boot    Start       End   #sectors  Id  System
/dev/vdb1          2048  20971519   20969472  8e  Linux LVM
/dev/vdb2             0         -          0   0  Empty
/dev/vdb3             0         -          0   0  Empty
/dev/vdb4             0         -          0   0  Empty
Warning: partition 1 does not start at a cylinder boundary
Warning: partition 1 does not end at a cylinder boundary
Warning: no primary partition is marked bootable (active)
This does not matter for LILO, but the DOS MBR will not boot this disk.
Successfully wrote the new partition table

Re-reading the partition table ...

If you created or changed a DOS partition, /dev/foo7, say, then use dd(1)
to zero the first 512 bytes:  dd if=/dev/zero of=/dev/foo7 bs=512 count=1
(See fdisk(8).)
  Physical volume "/dev/vdb1" successfully created
  Volume group "docker-vg" successfully created
  Rounding up size to full physical extent 12.00 MiB
  Logical volume "docker-poolmeta" created.
  Logical volume "docker-pool" created.
  WARNING: Converting logical volume docker-vg/docker-pool and docker-vg/docker-poolmeta to pool's data and metadata volumes.
  THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
  Converted docker-vg/docker-pool to thin pool.
  Logical volume "docker-pool" changed.

----
+
[CAUTION]
In a production environment, exercise caution when running `docker-storage-setup` because that command, by default, locates unused extents in the volume group (VG) that contain the root file system to create the pool. You can specify a VG or block device, but that can be a destructive process for the specified VG or block device. See the OpenShift documentation for details.

. On the master host, examine the newly created `docker-pool` logical volume:
+
----
[root@master1 ~]#  lvs
----

* The output is as follows:
+
----
LV          VG                    Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
docker-pool docker-vg             twi-a-t---  3.99g             0.00   0.29
root        rhel_host2cc260760b15 -wi-ao---- 17.51g
swap        rhel_host2cc260760b15 -wi-ao----  2.00g
----

. On the master host, examine the configuration of `docker storage`:
+
----
[root@master1 ~]# cat /etc/sysconfig/docker-storage
----

* The output is as follows:
+
----
DOCKER_STORAGE_OPTIONS=--storage-driver devicemapper --storage-opt dm.fs=xfs --storage-opt dm.thinpooldev=/dev/mapper/docker--vg-docker--pool
----

. Enable the Docker service on the master host:
+
----
[root@master1 ~]# systemctl enable docker
----

. Run the following `for` loop to configure Docker storage on the other nodes, enable Docker, and restart the node:
+
----
[root@master1 ~]# for node in infranode1.example.com \
                                    node1.example.com \
                                    node2.example.com; \
                                    do
                                      echo Configuring Docker Storage and rebooting $node
                                      scp /etc/sysconfig/docker-storage-setup ${node}:/etc/sysconfig/docker-storage-setup
                                      ssh $node "
                                            docker-storage-setup ;
                                            systemctl enable docker
                                            systemctl start docker"
                                    done
----


* `Broken Pipeline` messages in the output are normal and not an indication
 of errors.
 
* If you have problems with Docker's storage setup, 
see the "Known Issues" section.


[TIP]
=====
You can also use this Ansible command: `bastion`:
`ansible nodes -m copy -a 'dest=/etc/sysconfig/docker-storage-setup content="DEVS=/dev/vdb\nVG=docker-vg"' ;
ansible nodes -m shell -a "docker-storage-setup; systemctl enable docker; systemctl start docker"`
=====


=== Populate Local Docker Registry

. Verify that the Docker service has started on all nodes:
+
----
[root@bastion ~]# for node in   master1.example.com \
                                    infranode1.example.com \
                                    node1.example.com \
                                    node2.example.com; \
                                    do
                                      echo Checking docker status on $node
                                      ssh $node "
                                            systemctl status docker | grep Active"
                                    done
----
TIP: You can also use this Ansible command:
 `ansible nodes -m shell -a "systemctl status docker | grep Active"`.


. In the output verify that the status is `active (running)`:
+
----
Checking docker status on master1.example.com
   Active: active (running) since Thu 2015-11-26 01:03:14 EST; 2min 24s ago
Checking docker status on infranode1.example.com
   Active: active (running) since Thu 2015-11-26 01:02:15 EST; 3min 24s ago
Checking docker status on node1.example.com
   Active: active (running) since Thu 2015-11-26 01:02:17 EST; 3min 23s ago
Checking docker status on node2.example.com
   Active: active (running) since Thu 2015-11-26 01:02:20 EST; 3min 21s ago

----

. On the `bastion` host, pull down the Docker images to `node1` and `node2` in the primary region:
+
----
[root@bastion ~]# REGISTRY="registry.access.redhat.com";PTH="openshift3"
[root@bastion ~]# OSE_VERSION=$(yum info atomic-openshift | grep Version | awk '{print $3}')
[root@bastion ~]# for node in  node1.example.com \
                                   node2.example.com; \
do
ssh $node "
docker pull $REGISTRY/$PTH/ose-deployer:v$OSE_VERSION ; \
docker pull $REGISTRY/$PTH/ose-sti-builder:v$OSE_VERSION ; \
docker pull $REGISTRY/$PTH/ose-pod:v$OSE_VERSION ; \
docker pull $REGISTRY/$PTH/ose-keepalived-ipfailover:v$OSE_VERSION ; \
docker pull $REGISTRY/$PTH/ruby-20-rhel7 ; \
docker pull $REGISTRY/$PTH/mysql-55-rhel7 ; \
docker pull openshift/hello-openshift:v1.2.1 ;
"
done
----
* You are downloading these images to save time later. Unless otherwise configured, if a node does not have a local image, it downloads it.

* This process takes about 10 minutes to complete on *each node*. For the sake of efficiency, do not wait for the process to complete. Just connect to each node, run `pull`, and continue with the other tasks.
+
====
TIP: You can also use this Ansible command: `REGISTRY="registry.access.redhat.com";PTH="openshift3"; OSE_VERSION=$(yum info atomic-openshift | grep Version | awk '{print $3}');  ansible 'nodes:!masters:!infranode1.example.com' -m shell -a "
docker pull $REGISTRY/$PTH/ose-deployer:v$OSE_VERSION ;
docker pull $REGISTRY/$PTH/ose-sti-builder:v$OSE_VERSION ;
docker pull $REGISTRY/$PTH/ose-pod:v$OSE_VERSION ;
docker pull $REGISTRY/$PTH/ose-keepalived-ipfailover:v$OSE_VERSION ;
docker pull $REGISTRY/$PTH/ruby-20-rhel7 ;
docker pull $REGISTRY/$PTH/mysql-55-rhel7 ;
docker pull openshift/hello-openshift:v1.2.1 ;"`
====



. On `bastion`, pull only the basic images and the registry and router images to the `infranode1` host:
+
----
[root@bastion ~]# REGISTRY="registry.access.redhat.com";PTH="openshift3"
[root@bastion ~]# OSE_VERSION=$(yum info atomic-openshift | grep Version | awk '{print $3}')
[root@bastion ~]# node=infranode1.example.com
[root@bastion ~]# ssh $node "
docker pull $REGISTRY/$PTH/ose-haproxy-router:v$OSE_VERSION  ; \
docker pull $REGISTRY/$PTH/ose-deployer:vOSE_VERSION ; \
docker pull $REGISTRY/$PTH/ose-pod:v$OSE_VERSION ; \
docker pull $REGISTRY/$PTH/ose-docker-registry:v$OSE_VERSION ;
"
----
+
TIP: You can also use the Ansible command: `REGISTRY="registry.access.redhat.com"; OSE_VERSION=$(yum info atomic-openshift | grep Version | awk '{print $3}'); PTH="openshift3"; ansible infranode1.example.com -m shell -a "
docker pull $REGISTRY/$PTH/ose-haproxy-router:v$OSE_VERSION  ;
docker pull $REGISTRY/$PTH/ose-deployer:v$OSE_VERSION ;
docker pull $REGISTRY/$PTH/ose-pod:v$OSE_VERSION ;
docker pull $REGISTRY/$PTH/ose-docker-registry:v$OSE_VERSION ;"`

* You are not pulling any images on the master host because it is not meant
to run any containers.

. Examine the information in the Docker pool on the `node1` and `node2` hosts:
+
----
[root@bastion ~]# ssh node1.example.com docker info
----

* The `node1` output is as follows:
+
----
Containers: 0
Images: 15
Storage Driver: devicemapper
Pool Name: docker--vg-docker--pool
Pool Blocksize: 524.3 kB
Backing Filesystem: xfs
Data file:
Metadata file:
Data Space Used: 1.481 GB
Data Space Total: 10.72 GB
Data Space Available: 9.24 GB
Metadata Space Used: 323.6 kB
Metadata Space Total: 29.36 MB
Metadata Space Available: 29.04 MB
Udev Sync Supported: true
Deferred Removal Enabled: false
Library Version: 1.02.93-RHEL7 (2015-01-28)
Execution Driver: native-0.2
Logging Driver: json-file
Kernel Version: 3.10.0-229.el7.x86_64
Operating System: Red Hat Enterprise Linux Server 7.1 (Maipo)
CPUs: 2
Total Memory: 1.797 GiB
Name: node1.example.com
ID: RXVI:JKOO:3U4X:LHDE:QXPN:FSQC:TTBL:UCWP:MCEH:2KU6:GWSD:IRIN
...
----

. On the `node1` and `node2` hosts, examine the `docker-pool` logical volume again:
+
----
[root@bastion ~]# ssh node1.example.com "lvs"
----

* The `node1` output is similar to below. 
* The `docker-pool` LV now contains data.

+
----
LV          VG                    Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
docker-pool docker-vg             twi-a-t---  9.98g             13.81  1.10
root        rhel_host2cc260760b15 -wi-ao---- 17.51g
swap        rhel_host2cc260760b15 -wi-ao----  2.00g
----

== Install OpenShift Container Platform

In this section you download and install the installer and then verify the environment.

=== Download Ansible Playbook

In this exercise you run the Ansible playbook from the `bastion` host, which, in a real-world scenario, could be a laptop or a staging or provisioning server. No packages are deployed directly from `bastion` to the OpenShift nodes or master.

* On the `bastion` host, install the OpenShift utility package:
+
----
[root@bastion ~]# yum -y install atomic-openshift-utils
----

=== Create Inventory File

The `/etc/ansible/hosts` file is Ansibleâ€™s inventory file for the playbook to use during the installation. The inventory file describes the configuration for your OpenShift Container Platform cluster.

* Write the inventory file:
+
----
[root@bastion ~]# cat << EOF > /etc/ansible/hosts
[OSEv3:children]
masters
nodes
nfs

[OSEv3:vars]
ansible_user=root

# enable ntp on masters to ensure proper failover
openshift_clock_enabled=true

deployment_type=openshift-enterprise
openshift_release=$OSE_VERSION

openshift_master_cluster_method=native
openshift_master_cluster_hostname=master1.example.com
openshift_master_cluster_public_hostname=master1-${GUID}.oslab.opentlc.com

os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'

openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]
#openshift_master_htpasswd_users={'andrew': '$apr1$cHkRDw5u$eU/ENgeCdo/ADmHF7SZhP/', 'marina': '$apr1$cHkRDw5u$eU/ENgeCdo/ADmHF7SZhP/'

# default project node selector
osm_default_node_selector='region=primary'
openshift_hosted_router_selector='region=infra'
openshift_hosted_router_replicas=1
#openshift_hosted_router_certificate={"certfile": "/path/to/router.crt", "keyfile": "/path/to/router.key", "cafile": "/path/to/router-ca.crt"}
openshift_hosted_registry_selector='region=infra'
openshift_hosted_registry_replicas=1

openshift_master_default_subdomain=cloudapps-${GUID}.oslab.opentlc.com

#openshift_use_dnsmasq=False
#openshift_node_dnsmasq_additional_config_file=/home/bob/ose-dnsmasq.conf

openshift_hosted_registry_storage_kind=nfs
openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
openshift_hosted_registry_storage_host=bastion.example.com
openshift_hosted_registry_storage_nfs_directory=/exports
openshift_hosted_registry_storage_volume_name=registry
openshift_hosted_registry_storage_volume_size=5Gi

[nfs]
bastion.example.com

[masters]
master1.example.com openshift_hostname=master1.example.com openshift_public_hostname=master1-${GUID}.oslab.opentlc.com

[nodes]
master1.example.com openshift_hostname=master1.example.com openshift_public_hostname=master1-${GUID}.oslab.opentlc.com openshift_node_labels="{'region': 'infra'}"
infranode1.example.com openshift_hostname=infranode1.example.com openshift_public_hostname=infranode1-${GUID}.oslab.opentlc.com openshift_node_labels="{'region': 'infra', 'zone': 'infranodes'}"
node1.example.com openshift_hostname=node1.example.com openshift_public_hostname=node1-${GUID}.oslab.opentlc.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_hostname=node2.example.com openshift_public_hostname=node2-${GUID}.oslab.opentlc.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
EOF
----



* `openshift_hostname` should resolve to the internal IP from the instances
   themselves.
* `openshift_public_hostname` should resolve to the external IP from hosts outside of the cloud.
* `openshift_master_default_subdomain` sets a default subdomain for the route.
* `osm_default_node_selector` sets a default node selector.


=== Run Ansible Playbook

. After configuring Ansible by defining an inventory file in `/etc/ansible/hosts`, run the installation using the following playbook:
+
----
[root@bastion ~]# ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml
----

. Watch the Ansible playbook run:
+
----
[Omitted long output]

PLAY RECAP ********************************************************************
infranode1.example.com     : ok=105  changed=29   unreachable=0    failed=0
localhost                  : ok=21   changed=0    unreachable=0    failed=0
master1.example.com        : ok=396  changed=73   unreachable=0    failed=0
node1.example.com          : ok=105  changed=29   unreachable=0    failed=0
node2.example.com          : ok=105  changed=29   unreachable=0    failed=0
----

=== Verify the Environment

. Connect to the `master1` host:
+
----
[root@bastion ~]# ssh master1.example.com
----

. Run `oc get nodes` to check the status of the hosts:
+
----

[root@master1 ~]# oc get nodes
NAME                     STATUS                     AGE
infranode1.example.com   Ready                      1m
master1.example.com      Ready,SchedulingDisabled   1m
node1.example.com        Ready                      1m
node2.example.com        Ready                      1m
----
* If you see an error message that a connection to the master host cannot be established, wait a few more seconds for the master daemon to start.

. Use your browser to connect to the OpenShift web console at `https://master1-GUID.oslab.opentlc.com:8443` and accept the untrusted certificate.

* You cannot log in yet because you have not set up authentication.

== Configure and Set Up OpenShift Container Platform

In this section, you establish regions and zones, configure OpenShift
 Container Platform, set up authentication, add development users, and configure
  `htpasswd` authentication. Then you deploy the registry and router
   and populate OpenShift Container Platform.

=== Check Regions and Zones


* On the `master1` host, run `oc get nodes --show-labels` to learn how the labels were implemented:
+
----

[root@master1 ~]# oc get nodes --show-labels

----

** The output is as follows:
+
----
NAME                     STATUS                     AGE       LABELS
infranode1.example.com   Ready                      6m       kubernetes.io/hostname=infranode1.example.com,region=infra,zone=infranodes
master1.example.com      Ready,SchedulingDisabled   6m       kubernetes.io/hostname=master1.example.com,region=infra
node1.example.com        Ready                      6m       kubernetes.io/hostname=node1.example.com,region=primary,zone=east
node2.example.com        Ready                      6m       kubernetes.io/hostname=node2.example.com,region=primary,zone=west

----

* You can add or overwrite labels with this command: `oc label node infranode1.example.com region="infra" zone="infranodes"`.

* You now have a running OpenShift Container Platform environment across three hosts with
 one master and three nodes, divided into two regions: infra and primary.

=== OpenShift Container Platform Configuration Tips

==== Configuring the `default` Namespace to Use the `infra` Region

. Edit the `default` namespace with the following command:
+
----
[root@master1 ~]# oc annotate namespace default openshift.io/node-selector='region=infra' --overwrite
----

. Check that your changes were updated in the `default` namespace:
+
----
[root@master1 ~]# oc get namespace default -o yaml
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    openshift.io/node-selector: region=infra
    openshift.io/sa.initialized-roles: "true"
    openshift.io/sa.scc.mcs: s0:c1,c0
    openshift.io/sa.scc.supplemental-groups: 1000000000/10000
    openshift.io/sa.scc.uid-range: 1000000000/10000
  creationTimestamp: 2016-07-27T14:40:21Z
  name: default
  resourceVersion: "18645"
  selfLink: /api/v1/namespaces/default
  uid: 0b862f84-5408-11e6-b694-2cc2600a5748
spec:
  finalizers:
  - kubernetes
  - openshift.io/origin
status:
  phase: Active
----

==== Setting Up Processes for Logs (Reference Only)
* Because the `systemd` and `journal` commands are for browsing logs in Red Hat Enterprise Linux 7, do not browse them with `/var/log/messages`. Run `journalctl` instead.

* Because Red Hat Enterprise Linux 7 runs all components in higher log levels, Red Hat recommends that you set up windows for each process in your terminal emulator. That is, on the master host, run each of the following command lines in its own window:
+
----
[root@master1 ~]# journalctl -f -u atomic-openshift-master
[root@master1 ~]# journalctl -f -u atomic-openshift-node
----

[NOTE]
To run the above commands on the other nodes, you do not need the `atomic-openshift-master` service. You may also want to watch the Docker logs.

=== Configure Authentication

* Verify that the `oauthConfig` section in  `/etc/origin/master/master-config.yaml` file reads as follows:
+
----
[root@master1 ~]# grep oauthConfig -A22 /etc/origin/master/master-config.yaml
oauthConfig:
  assetPublicURL: https://master1-GUID.oslab.opentlc.com:8443/console/
  grantConfig:
    method: auto
  identityProviders:
  - challenge: true
    login: true
    mappingMethod: claim
    name: htpasswd_auth
    provider:
      apiVersion: v1
      file: /etc/origin/master/htpasswd
      kind: HTPasswdPasswordIdentityProvider
  masterCA: ca.crt
  masterPublicURL: https://master1-GUID.oslab.opentlc.com:8443
  masterURL: https://master1.example.com:8443
  sessionConfig:
    sessionMaxAgeSeconds: 3600
    sessionName: ssn
    sessionSecretsFile: /etc/origin/master/session-secrets.yaml
  tokenConfig:
    accessTokenMaxAgeSeconds: 86400
    authorizeTokenMaxAgeSeconds: 500
----

+
NOTE: `GUID` should be your value.

=== Add Development Users

Real-world developers are likely to use the OpenShift Container Platform tools (`oc` and the web console) on their own machines. In this course, you create accounts for two nonprivileged OpenShift Container Platform users, `andrew` and `marina`, on the master.

* On the master host, add two Linux accounts:
+
----
[root@master1 ~]# useradd andrew
[root@master1 ~]# useradd marina
----

NOTE: Feel free to create those users on any machine in which the `oc` command is available. The master's API port (8443) is available to the public network.

=== Configure `htpasswd` Authentication

OpenShift Container Platform 3 supports several authentication mechanisms. The simplest use case for testing is `htpasswd`-based authentication.

As a preliminary requirement, you need the `htpasswd` binary in the `httpd-tools` package. Do the following:

. Install `httpd-tools` on the master host:
+
----
[root@master1 ~]# yum -y install httpd-tools
----

. Create a password for users `andrew` and `marina` on the master host:
+
----
[root@master1 ~]# htpasswd -cb /etc/origin/master/htpasswd andrew r3dh4t1!
[root@master1 ~]# htpasswd -b /etc/origin/master/htpasswd marina r3dh4t1!
----

. Verify that you can authenticate as `andrew` in the OpenShift web console:
.. Connect to `https://master1-GUID.oslab.opentlc.com:8443/`.
.. Log in as `andrew` with the password `r3dh4t1!`.


=== Registry and Router

In this lab scenario, `infranode1` is the target for both the registry and the default router.

==== Deploy Registry (Reference Only)

. Deploy `registry`:
+
----
[root@master1 ~]# oadm registry --config=/etc/origin/master/admin.kubeconfig --service-account=registry
----
+
NOTE: To pin down the registry for a specific region, use the `--selector` flag. However, you can skip this step because you already set the default namespace to be the default node selector.

. Check the status of your pod with the following commands:
+
----
 [root@master1 ~]# oc get pods
 NAME                       READY     STATUS    RESTARTS   AGE
 docker-registry-1-deploy   1/1       Pending   0          11s

... Wait a few seconds ...
 [root@master1 ~]# oc get pods

 NAME                       READY     STATUS    RESTARTS   AGE
 docker-registry-1-deploy   1/1       Running   0          31s
 docker-registry-1-diqlc    0/1       Pending   0          4s

... Wait a few seconds ...
 [root@master1 ~]# oc get pods
 NAME                      READY     STATUS    RESTARTS   AGE
 docker-registry-1-diqlc   1/1       Running   0          14s
----

* This process may take a few minutes the first time because the images are pulled from the registry.

. Run `oc status`:
+
----
[root@master1 master]# oc status
 In project default on server https://master1-GUID.oslab.opentlc.com:8443

 svc/docker-registry - 172.30.41.32:5000
   dc/docker-registry deploys docker.io/openshift3/ose-docker-registry:v3.3.x.x
     #1 deployed 5 minutes ago - 1 pod

 svc/kubernetes - 172.30.0.1 ports 443, 53, 53
----
* To see more, use `oc describe <resource>/<name>`.
* To see a list of other objects, use `oc get all`.
+
TIP: You can use the `curl` command to test the ability of the registry to communicate with its service port--for example: `curl -v 172.30.41.32:5000/healthz`.
+
. To test the registry for connectivity, run these commands:
+
----
 [root@master1 ~]# echo `oc get service docker-registry --template '{{.spec.portalIP}}:{{index .spec.ports 0 "port"}}/healthz'`
 172.30.42.118:5000/healthz
 [root@master1 ~]# curl -v `oc get service docker-registry --template '{{.spec.portalIP}}:{{index .spec.ports 0 "port"}}/healthz'`
----

* The output looks like this:
+
----
* About to connect() to 172.30.42.118 port 5000 (#0)
*   Trying 172.30.42.118...
* Connected to 172.30.42.118 (172.30.42.118) port 5000 (#0)
> GET /healthz HTTP/1.1
> User-Agent: curl/7.29.0
> Host: 172.30.42.118:5000
> Accept: */*
>
< HTTP/1.1 200 OK
< Content-Type: application/json; charset=utf-8
< Docker-Distribution-Api-Version: registry/2.0
< Date: Thu, 26 Nov 2015 06:56:11 GMT
< Content-Length: 3
<
{}
* Connection #0 to host 172.30.42.118 left intact
----

==== Redeploy Default Router

To use a custom default certificate, delete the old router, create a
 certificate, and create a new router.

. Delete the old router:
+
----
oc delete dc/router svc/router
----

. Create a certification authority (CA) certificate for the default router:
+
----
[root@master1 ~]# CA=/etc/origin/master
[root@master1 ~]# oadm ca create-server-cert --signer-cert=$CA/ca.crt \
       --signer-key=$CA/ca.key --signer-serial=$CA/ca.serial.txt \
       --hostnames='*.cloudapps-$guid.oslab.opentlc.com' \
       --cert=cloudapps.crt --key=cloudapps.key
----

. Combine `cloudapps.crt` and `cloudapps.key` with `CA` into a single Privacy Enhanced Mail (PEM) format file, which the router needs in the next step:
+
----
[root@master1 ~]# cat cloudapps.crt cloudapps.key $CA/ca.crt > /etc/origin/master/cloudapps.router.pem
----

. Deploy the default router:
+
----
[root@master1 ~]# oadm router trainingrouter --replicas=1 \
    --default-cert=${CA}/cloudapps.router.pem \
    --service-account=router --stats-password='r3dh@t1!'
----

* The output is as follows:
+
----
password for stats user admin has been set to r3dh@t1!
DeploymentConfig "trainingrouter" created
Service "trainingrouter" created

----
. On a separate terminal, watch the status of your pods:
+
----
[root@master1 ~]# oc get pods -w
NAME                      READY     STATUS    RESTARTS   AGE
docker-registry-1-diqlc   1/1       Running   0          11m
trainingrouter-1-r00xl    1/1       Running   0          23s


----

* The Docker registry pods are likely also listed in the above output.

* Type *Ctrl+C* to exit the watch on `oc get pods`.


=== Populate OpenShift Container Platform (Reference Only)

OpenShift Container Platform ships with _image streams_ and _templates_, which reside in `/usr/share/openshift/examples/`.  The installer imports all of the image streams and templates for you from that directory.

* To browse the JSON files, navigate to `/usr/share/openshift/examples`.

[IMPORTANT]
The commands below are for reference only. Run them only if you would like to perform the task in question.

* To create or delete the core set of image streams whose images are based on Red Hat Enterprise Linux 7:
+
----

oc create|delete -f /usr/share/openshift/examples/image-streams/image-streams-rhel7.json -n openshift
----

* To create or delete the core set of database templates:
+
----
oc create|delete or remove -f /usr/share/openshift/examples/db-templates -n openshift
----

* To create or delete the core QuickStart templates:
+
----
oc create|delete -f /usr/share/openshift/examples/quickstart-templates -n openshift
----


== Set Up Persistent Storage (Reference Only)

Having a database for development is useful, but what if you want the data you store to persist after redeploying the database pod? Pods are ephemeral and, by default, so is their storage. For shared or persistent storage, you must be able to mandate that pods use external volumes.

For the purpose of this course, you learn how to have `bastion` act as your NFS server to export NFS mounts as `PersistentVolume` targets.

=== Create Definition Files for Volumes

. Connect to the `master1` host:
+
----
[root@bastion ~]# ssh master1.example.com
----

. Create 25 instances of `PersistentVolume` with a size of 5GB each (`pv1` to `pv25`):
+
----

[root@master1 ~]# export volsize="5Gi"
[root@master1 ~]# for volume in pv{1..25} ; do
cat << EOF > /root/pvs/${volume}
{
  "apiVersion": "v1",
  "kind": "PersistentVolume",
  "metadata": {
    "name": "${volume}"
  },
  "spec": {
    "capacity": {
        "storage": "${volsize}"
    },
    "accessModes": [ "ReadWriteOnce" ],
    "nfs": {
        "path": "/var/export/pvs/${volume}",
        "server": "192.168.0.3"
    },
    "persistentVolumeReclaimPolicy": "Recycle"
  }
}
EOF
echo "Created def file for ${volume}";
done;
----

. Create 25 more instances of `PersistentVolume` with a size of 10GB each (`pv26` to `pv50`):
+
----

[root@master1 ~]# export volsize="10Gi"
[root@master1 ~]# for volume in pv{26..50} ; do
cat << EOF > /root/pvs/${volume}
{
  "apiVersion": "v1",
  "kind": "PersistentVolume",
  "metadata": {
    "name": "${volume}"
  },
  "spec": {
    "capacity": {
        "storage": "${volsize}"
    },
    "accessModes": [ "ReadWriteOnce" ],
    "nfs": {
        "path": "/var/export/pvs/${volume}",
        "server": "192.168.0.3"
    },
    "persistentVolumeReclaimPolicy": "Recycle"
  }
}
EOF
echo "Created def file for ${volume}";
done;
----

. Create 50 more instances of `PersistentVolume` with a size of 1GB each (`pv51` to `pv100`):
+
----

[root@master1 ~]# export volsize="1Gi"
[root@master1 ~]# for volume in pv{51..100} ; do
cat << EOF > /root/pvs/${volume}
{
  "apiVersion": "v1",
  "kind": "PersistentVolume",
  "metadata": {
    "name": "${volume}"
  },
  "spec": {
    "capacity": {
        "storage": "${volsize}"
    },
    "accessModes": [ "ReadWriteOnce" ],
    "nfs": {
        "path": "/var/export/pvs/${volume}",
        "server": "192.168.0.3"
    },
    "persistentVolumeReclaimPolicy": "Recycle"
  }
}
EOF
echo "Created def file for ${volume}";
done;
----

. Allocate three of the 5GB volumes to the default project: `pv21`, `pv22`, and `pv23`:
+
----
[root@master1 ~]# cd /root/pvs
[root@master1 ~]# cat pv21 pv22 pv23 | oc create -f - -n default
----

. Run `oc get pv` to ensure that your `pvs` volumes have been added and are available:
+
----
[root@master1 pvs]# oc get pv
NAME               LABELS    CAPACITY      ACCESSMODES   STATUS      CLAIM                    REASON
pv21               <none>    5368709120    RWO           Available
pv22               <none>    5368709120    RWO           Available
pv23               <none>    5368709120    RWO           Available
----


Although you performed this process manually here, the process can be easily automated to create volumes on request.

The infrastructure for persistent volumes is complete. You learn how to use them in the next lab.
