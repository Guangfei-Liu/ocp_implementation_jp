:scrollbar:
:data-uri:
:icons: images/icons
:toc2:		

== Deploy OpenShift Enterprise
:numbered:	

In this lab we will Deploy OpenShift Enterprise on a single host (in later labs we will add more nodes).

* Configure Repositories

* Configure Network Settings

* Install Docker on our host 

* Configure SSH Keys

* Install Ansible Installer 

* Configure and Install OpenShift Enterprise

* Configure a DNS on our *oselab* server to serve our OpenShift environment.

* Test deployment.

== Beta Course Warning

Take note that all of the steps in the lab may change when OpenShift 3.0 goes GA.  This course was built on a beta build.

== Lab Environment Architecture and Important Information

The lab environment consists of 4 VMs:

* `oselab-GUID.oslab.opentlc.com` (administration host)

* `master00-GUID.oslab.opentlc.com` (master host, includes node role as well)

* `node00-GUID.oslab.opentlc.com` (node host)

* `node01-GUID.oslab.opentlc.com` (node host)

[NOTE]
As a reminder you will only be allowed to SSH to the administration host from the outside of the lab environment, all other hosts have external SSH blocked.  Once on the administration host, you can SSH to the other hosts internally.  As described earlier, you will have to use your private SSH key and OPENTLC login to access the system (not root!).

Each student lab is assigned a global unique identifier (GUID) that consists of 4 characters.  This GUID is provided to you in the provisioning email that will be sent to you when you provision your lab environment.  *Anywhere you see GUID from this point on, you will replace it with your lab's GUID.*

*In each lab step take special care to make sure that you are running the step on the required host.  Each step should contain the name of the host to run the step on and the example code should contain the host name in the shell prompt.*

* Administration host example:
+
----

[root@oselab-GUID ~]# command

----

* Master host example:
+
----

[root@master00-GUID ~]# command

----


== Configure the Repositories on the Master Host

OpenShift requires several software repositories:

* `rhel-7-server-rpms`

* `rhel-7-server-extras-rpms`

* `rhel-7-server-optional-rpms`

* `rhel-server-7-ose-beta-rpms`

Normally you would get these repositories via `subscription-manager` but we have provided a mirror that we will configure in the following lab steps.

. If not already connected, connect to your administration host `oselab-GUID.oslab.opentlc.com` using your OPENTLC login and private SSH key:
+
----

yourdesktop$ ssh -i ~/.ssh/mykey your-opentlc-login@oselab-GUID.oslab.opentlc.com

----

. From the administration host SSH to the master host as the root user:
+
----

[yourlogin@oselab-GUID ~]$ ssh root@master00-GUID.oslab.opentlc.com

----
+
[NOTE]
When prompted for a password use *r3dh4t1!*
+
----

root@master00-GUID.oslab.opentlc.com's password: ******** (r3dh4t1!) 

----

. It is highly recommended that you use a terminal multiplexing tool such as `tmux` or `screen` in case you lose connectivity to your environment.  This will keep your session at the place it was at the time of disconnection.  You are allowed to install the `tmux` or `screen` package using `yum` on the master host.  It is not installed by default.
+
[NOTE]
For more information on using `tmux` use `man tmux` after installing the package.
For more information on using `screen` use `man screen` after installing the package.


. On the master host set up the yum repository configuration file `/etc/yum.repos.d/open.repo` with the following repositories:
+
----
[root@master00-GUID ~]# cat << EOF > /etc/yum.repos.d/open.repo
[rhel-x86_64-server-7]
name=Red Hat Enterprise Linux 7
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-7
enabled=1
gpgcheck=0

[rhel-x86_64-server-rh-common-7]
name=Red Hat Enterprise Linux 7 Common
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-rh-common-7
enabled=1
gpgcheck=0

[rhel-x86_64-server-extras-7]
name=Red Hat Enterprise Linux 7 Extras
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-extras-7
enabled=1
gpgcheck=0

[rhel-x86_64-server-optional-7]
name=Red Hat Enterprise Linux 7 Optional
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-optional-7
enabled=1
gpgcheck=0

EOF

----
+
[NOTE]
We are using a local mirror of the repositories in our lab environment, as stated earlier you would normally use `subscription-manager`.

. Add the OpenShift repository mirror to the master host:
+
----

[root@master00-GUID ~]# cat << EOF >> /etc/yum.repos.d/open.repo
[rhel-server-7-ose-beta-rpms]
name=Red Hat Enterprise Linux 7 OSE 3
baseurl=http://www.opentlc.com/repos/ose3_beta4
enabled=1
gpgcheck=0

EOF

----

. List the available repositories on the master host:
+
-----

[root@master00-GUID ~]# yum repolist 

-----
+
You should see the following:
+
----

repo id                           repo name                               status
rhel-server-7-ose-beta-rpms       Red Hat Enterprise Linux 7 OSE 3           16
rhel-x86_64-server-7              Red Hat Enterprise Linux 7              4,387
rhel-x86_64-server-extras-7       Red Hat Enterprise Linux 7 Extras          19
rhel-x86_64-server-optional-7     Red Hat Enterprise Linux 7 Optional     4,087
rhel-x86_64-server-rh-common-7    Red Hat Enterprise Linux 7 Common          19

----

== Verify Network Configuration:

In this lab we will verify that the master host is configured correctly for internal and external DNS name resolution.

. Verify the hostname for the master host:
+
----

[root@master00-GUID ~]# hostname -f 

----
+
.You should see the following:
----

master00-GUID.oslab.opentlc.com

----

. Take note of the master host's internal IP address:
+
----

[root@master00-GUID ~]# ip address show dev eth0|grep "inet "|awk '{print $2}'|cut -f1 -d/

----

. Make sure the master host's internal DNS entry matches the internal IP address:
+
----

[root@master00-GUID ~]# host `hostname -f` 

----

. Take note of the master host's external IP address:
+
----

[root@master00-GUID ~]# curl http://www.opentlc.com/getip

----

. Make sure the master host's external DNS entry matches the external IP address:
+
----

[root@master00-GUID ~]# host `hostname -f` 8.8.8.8

----

== Install Docker

OpenShift uses Docker to store and manage container images.  In this lab we install Docker and provide it's required storage pool.

. Install the `docker` package on the master host
+ 
----

[root@master00-GUID ~]# yum -y install docker

----
+
[NOTE]
The default Docker storage configuration uses loopback devices and is not appropriate for production. Red Hat considers the dm.thinpooldev storage option to be the only appropriate configuration for production use.

. Remove the out of the box loopback docker storage from the master host:
+
----

[root@master00-GUID ~]# rm -rf /var/lib/docker/*

----

. In order to use `dm.thinpooldev` you must have space for an LVM thinpool available, the `docker-storage-setup` package will assist you in configuring LVM.  Run `docker-storage-setup` on the master host to create logical volumes for Docker:
+
----

[root@master00-GUID ~]# docker-storage-setup

----
+
You should see the following:
+
----

  Rounding up size to full physical extent 32.00 MiB
  Logical volume "docker-poolmeta" created.
  Logical volume "docker-pool" created.
  WARNING: Converting logical volume rhel_host2cc260760b15/docker-pool and rhel_host2cc260760b15/docker-poolmeta to pool's data and metadata volumes.
  THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
  Converted rhel_host2cc260760b15/docker-pool to thin pool.
  Logical volume "docker-pool" changed.
  
----
+
[NOTE]
Be careful with `docker-storage-setup` as it will, by default, find any unused extents in the volume group that contains your root filesystem to create the pool.  You can also specify a specific volume group or block device.  This can be a destructive process to the specified VG or block device!  Consult the OpenShift documentation for more information.

. On the master host examine the newly created logical volume `docker-pool`:
+
----

[root@master00-GUID ~]# lvs /dev/rhel_host2cc260760b15/docker-pool

----
+
You should see the following:
+
----

  LV          VG                    Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  docker-pool rhel_host2cc260760b15 twi-a-t--- 5.98g             0.00   0.11

----

. On the master host, examine the docker storage configuration:
+
----

[root@master00-GUID ~]# cat /etc/sysconfig/docker-storage

----
+
You should see the following:
+
----

DOCKER_STORAGE_OPTIONS=-s devicemapper --storage-opt dm.fs=xfs --storage-opt dm.thinpooldev=/dev/mapper/rhel_host2cc260760b15-docker--pool

----

. Configure the *Docker* registry on the master host:
+
----

[root@master00-GUID ~]# sed -i "s/OPTIONS.*/OPTIONS='--selinux-enabled --insecure-registry 0.0.0.0\/0'/" \
    /etc/sysconfig/docker

----

. Enable, start, and get status for the *Docker* service on the master host:
+
----

[root@master00-GUID ~]# systemctl enable docker
[root@master00-GUID ~]# systemctl start docker
[root@master00-GUID ~]# systemctl status docker

----
+
You should see the following:
+
----

docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled)
   Active: active (running) since Wed 2015-06-10 15:31:11 EDT; 1s ago
...OUTPUT OMMITTED...

----
+
[NOTE]
Make sure the status shows *enabled* and *active (running)*.

. In order to save time later, we will pre-fetch the docker images to the master host. This process will take about 10 minutes to complete:
+
----

[root@master00-0a0c ~]# \
docker pull registry.access.redhat.com/openshift3_beta/ose-haproxy-router:v0.5.2.2 ; \
docker pull registry.access.redhat.com/openshift3_beta/ose-deployer:v0.5.2.2 ; \
docker pull registry.access.redhat.com/openshift3_beta/ose-sti-builder:v0.5.2.2 ; \
docker pull registry.access.redhat.com/openshift3_beta/ose-sti-image-builder:v0.5.2.2 ; \
docker pull registry.access.redhat.com/openshift3_beta/ose-docker-builder:v0.5.2.2 ; \
docker pull registry.access.redhat.com/openshift3_beta/ose-pod:v0.5.2.2 ; \
docker pull registry.access.redhat.com/openshift3_beta/ose-docker-registry:v0.5.2.2 ; \
docker pull registry.access.redhat.com/openshift3_beta/sti-basicauthurl:latest ; \
docker pull registry.access.redhat.com/openshift3_beta/ose-keepalived-ipfailover:v0.5.2.2
 ; \
docker pull registry.access.redhat.com/openshift3_beta/ruby-20-rhel7 ; \
docker pull registry.access.redhat.com/openshift3_beta/mysql-55-rhel7 ; \
docker pull registry.access.redhat.com/jboss-eap-6/eap-openshift ; \
docker pull openshift/hello-openshift:v0.4.3

----

. Examine docker pool info on the master host:
+
----

[root@master00-0a0c ~]# docker info
----
+
You should something like this:
+
----

Containers: 0
Images: 63
Storage Driver: devicemapper
 Pool Name: rhel_host2cc260760b15-docker--pool
 Pool Blocksize: 524.3 kB
 Backing Filesystem: xfs
 Data file:
 Metadata file:
 Data Space Used: 2.308 GB
 Data Space Total: 6.417 GB
 Data Space Available: 4.109 GB
 Metadata Space Used: 778.2 kB
 Metadata Space Total: 33.55 MB
 Metadata Space Available: 32.78 MB
 Udev Sync Supported: true
 Library Version: 1.02.93-RHEL7 (2015-01-28)
Execution Driver: native-0.2
Kernel Version: 3.10.0-229.el7.x86_64
Operating System: Red Hat Enterprise Linux Server 7.1 (Maipo)
CPUs: 2
Total Memory: 1.797 GiB
Name: master00-GUID.oslab.opentlc.com
...

----

. On the master host examine the `docker-pool` logical volume again:
+
----

[root@master00-GUID ~]# lvs /dev/rhel_host2cc260760b15/docker-pool

----
+
You should see something similar to the following:
+
----

  LV          VG                    Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  docker-pool rhel_host2cc260760b15 twi-aot--- 5.98g             35.96  2.32
  
----

== Configure SSH Keys:

The OpenShift installer uses SSH to configure hosts.  In this lab we create and install an SSH key pair on the master host and add the public key to the `authorized_hosts` file.

. On the master host, create an SSH key pair for the `root` user.
+
----

[root@master00-GUID ~]# ssh-keygen -f /root/.ssh/id_rsa -N '' 

----

. Add the public ssh key to `/root/.ssh/authorized_keys` locally to the master host:
+
----

[root@master00-GUID ~]# cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys 

----

. Configure `/etc/ssh/ssh_conf` to disable `StrictHostKeyChecking` on the master host:
+
----

[root@master00-GUID ~]# echo StrictHostKeyChecking no >> /etc/ssh/ssh_config

----
+
[NOTE]
Only do this for hosts that are used for development, testing, or demos!

. From the master host test the new SSH key by connecting to itself over the loopback interface without a keyboard prompt:
+
----

[root@master00-GUID ~]# ssh 127.0.0.1
...[output ommitted]...
[root@master00-GUID ~]# exit

----

== Install Ansible Installer

Currently the Ansible installer is only available via the `Extra Packages for Enterprise Linux` or `EPEL` repository.  This lab will configure the `EPEL` repositroy and install the `ansible` package.

. Add the `EPEL` repository to the master host and disable it:
+
----

[root@master00-GUID ~]# yum -y install http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-5.noarch.rpm
[root@master00-GUID ~]# sed -i -e "s/^enabled=1/enabled=0/" /etc/yum.repos.d/epel.repo

----
+
[NOTE]
At this time, the `ansible` package is only available from `EPEL`.  We disable the `EPEL` repository so that other packages from the repository are not accidentally installed.

. Install the `ansible` package on the master host:
+
----

[root@master00-GUID ~]# yum -y --enablerepo=epel install ansible

----
+
[NOTE]
*The steps in this section will drastically change when the product goes GA.*
Eventually there will be an interactive text-based CLI installer that leverages Ansible under the covers. For now, we have to invoke Ansible manually.  We will do this in the next lab.

=== Configure and Install OpenShift Enterprise 3.0

In this lab we will use Ansible to install OpenShift on the master host.  This will configure the master host with the master and node roles.

. Download the OpenShift 3 Ansible "playbook" to the master host in root's home directory:
+
---- 

[root@master00-GUID ~]# cd;git clone https://github.com/detiber/openshift-ansible.git -b v3-beta4 

----

. The OpenShift playbook uses `/etc/ansible/hosts` to determine which hosts to configure and what roles to apply.  Configure `/etc/ansible/hosts` on the master host:
+
----

[root@master00-GUID ~]# export GUID=`hostname|cut -f2 -d-|cut -f1 -d.`
[root@master00-GUID ~]# cat << EOF > /etc/ansible/hosts
# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user=root

# To deploy origin, change deployment_type to origin
deployment_type=enterprise

# enable htpasswd authentication
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/openshift/openshift-passwd'}]

# host group for masters
[masters]
master00-$GUID.oslab.opentlc.com

# host group for nodes, includes region info
[nodes]
master00-$GUID.oslab.opentlc.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
#node00-$GUID.oslab.opentlc.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
#node01-$GUID.oslab.opentlc.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"

EOF

----
+
[NOTE]
Note that the nodes are commented out, they will be enabled later.  Also note the region and zone settings.  These are explained a little more in the next section.

. Run the Ansible installer `ansible-playbook` on the master host:
+
---- 

[root@master00-GUID ~]# ansible-playbook -vvv /root/openshift-ansible/playbooks/byo/config.yml

----
+
[NOTE]
Running the Ansible installer will take a few minutes to run.  This is a good time for a break.

. The output of the `ansible-playbook` command shoold show *failed=0* for all hosts:
+
----

...OUTPUT OMMITTED...
PLAY RECAP ********************************************************************
localhost                  : ok=5    changed=0    unreachable=0    failed=0
master00-GUID.oslab.opentlc.com : ok=X   changed=Y    unreachable=0    failed=0

----

. After the installer is complete, check the status of your host using the `osc get nodes` command on the master host:
+
----

[root@master00-GUID ~]#  osc get nodes
NAME                              LABELS                                                                             STATUS
master00-GUID.oslab.opentlc.com   kubernetes.io/hostname=master00-GUID.oslab.opentlc.com,region=infra,zone=default   Ready

----

== About Regions and Zones

In OpenShift 2, we introduced the specific concepts of "regions" and "zones" to enable organizations to provide some topologies for application resiliency. Apps would be spread throughout the zones in a region and, depending on the way you configured OpenShift, you could make different regions accessible to users.

For OpenShift 3, Kubernetes doesn't actually care about your topology. In other words, OpenShift is "topology agnostic". In fact, OpenShift 3 provides advanced controls for implementing whatever topologies you can dream up, leveraging filtering and affinity rules to ensure that parts of applications (pods) are either grouped together or spread apart.

For the purposes of a simple example, we'll be sticking with the "regions" and "zones" theme. But, as you go through these examples, think about what other complex topologies you could implement. Perhaps "secure" and "insecure" hosts, or other topologies.

=== Configure Wildcard DNS to Service the OpenShift Environment.

OpenShift requires a wildcard DNS A record.  The wildcard A record should point to the publicly available (external) IP address of the OpenShift router.  For this training, we will ensure that the router will end up on the OpenShift server that is running the master.  It is advisable to use a low TTL for this record in order for DNS client caches to expire quicker so that changes become available quicker.  The DNS server runs on the administration host.

. Connect to your administration host `oselab-GUID.oslab.opentlc.com` (your private key location may vary):
+
----

yourdesktop$ ssh -i ~/.ssh/id_rsa your-opentlc-login@oselab-GUID.oslab.opentlc.com

----

. Become the `root` user on the administration host:
+
----

-bash-4.2$ sudo -i

----

. Install the `bind` and `bind-utils` package on the administration host:
+
----

[root@oselab-GUID ~]# yum -y install bind bind-utils

----

. On the admistration host collect and define the environment's information:
+
----

[root@oselab-GUID ~]# guid=`hostname|cut -f2 -d-|cut -f1 -d.`
[root@oselab-GUID ~]# masterIP=`host master00-$guid.oslab.opentlc.com ipa.opentlc.com | grep $guid | awk '{ print $4 }'`
[root@oselab-GUID ~]# domain="cloudapps-$guid.oslab.opentlc.com"

----

. On the administration host create the zone file with the wildcard DNS:
+
----

[root@oselab-GUID ~]# mkdir /var/named/zones
[root@oselab-GUID ~]# echo "\$ORIGIN  .
\$TTL 1  ;  1 seconds (for testing only)
${domain} IN SOA master.${domain}.  root.${domain}.  (
  2011112904  ;  serial
  60  ;  refresh (1 minute)
  15  ;  retry (15 seconds)
  1800  ;  expire (30 minutes)
  10  ; minimum (10 seconds)
)
  NS master.${domain}.
\$ORIGIN ${domain}.
test A ${masterIP}
* A ${masterIP}"  >  /var/named/zones/${domain}.db

----

. Configure `named.conf` on the administration host:
+
----

[root@oselab-GUID ~]# echo "// named.conf
options {
  listen-on port 53 { any; };
  directory \"/var/named\";
  dump-file \"/var/named/data/cache_dump.db\";
  statistics-file \"/var/named/data/named_stats.txt\";
  memstatistics-file \"/var/named/data/named_mem_stats.txt\";
  allow-query { any; };
  recursion yes;
  /* Path to ISC DLV key */
  bindkeys-file \"/etc/named.iscdlv.key\";
};
logging {
  channel default_debug {
    file \"data/named.run\";
    severity dynamic;
  }; 
};
zone \"${domain}\" IN {
  type master;
  file \"zones/${domain}.db\";
  allow-update { key ${domain} ; } ;
};" > /etc/named.conf

----

. On the administration host correct file permissions and start the DNS server:
+
----

[root@oselab-GUID ~]# chgrp named -R /var/named
[root@oselab-GUID ~]# chown named -R /var/named/zones
[root@oselab-GUID ~]# restorecon -R /var/named
[root@oselab-GUID ~]# chown root:named /etc/named.conf
[root@oselab-GUID ~]# restorecon /etc/named.conf

----

. Enable and start `named` on the administration host:
+
----

[root@oselab-GUID ~]# systemctl enable named
[root@oselab-GUID ~]# systemctl start named

----

. Configure FirewallD on the administation host to allow inbound DNS queries:
+
----

[root@oselab-GUID bin]# firewall-cmd --zone=public --add-service=dns --permanent
[root@oselab-GUID bin]# firewall-cmd --reload

----

== Verify DNS configuration:

A test DNS entry was created called `test.cloud-appsGUID.oslab.opentlc.com`.  This lab will test internal and external resolution of that DNS entry.

. First try testing the DNS server running on the administration host:
+
----

[root@oselab-GUID ~]# guid=`hostname|cut -f2 -d-|cut -f1 -d.`
[root@oselab-GUID ~]# host test.cloudapps-$guid.oslab.opentlc.com 127.0.0.1

----

. Second try testing with an external name server:
+
----

[root@oselab-GUID ~]# host test.cloudapps-$guid.oslab.opentlc.com 8.8.8.8

----
+
[NOTE]
The first time you query 8.8.8.8 you may notice lag and an error:
`;; connection timed out; trying next origin
Host test.cloudapps-GIOD.oslab.opentlc.com not found: 3(NXDOMAIN)`
This is normal.  if you do the test again, it will go faster and not error out.

. Lastly test DNS from your laptop/desktop, this might take a few minutes to be updated.  Be sure to replace GUID with the correct GUID.
+
----

yourhost$ nslookup test.cloudapps-GUID.oslab.opentlc.com

----

== Add Nodes to the Environment
:numbered:	

In the previous lab we have installed OpenShift Enterprise 3.0 On a single host. 
In this lab we will demonstrate how easy it is to add nodes to the OpenShift 3.0 environment. 

. From the master host use `ssh-copy-id` to copy the public SSH key to each of the nodes:
+
----

[root@master00-GUID ~]# GUID=`hostname|cut -f2 -d-|cut -f1 -d.`
[root@master00-GUID ~]# ssh-copy-id node00-$GUID.oslab.opentlc.com
[root@master00-GUID ~]# ssh-copy-id node01-$GUID.oslab.opentlc.com

----
+
[NOTE]
If prompted for a password use *r3dh4t1!*
The SSH keys may already be in place in this lab environment, but it is good practice to make sure this is done.

. From the master host test password-less SSH connections to both of the nodes:
+
----

[root@master00-GUID ~]# ssh node00-$GUID.oslab.opentlc.com "hostname -f"

----
+
----

node00-$GUID.oslab.opentlc.com

----
+
----

[root@master00-GUID ~]# ssh node01-$GUID.oslab.opentlc.com "hostname -f"

----
+
----

node01-$GUID.oslab.opentlc.com

----

=== Configure the Repositories on the Node Hosts

OpenShift nodes require the same repositories as the master:

* `rhel-7-server-rpms`

* `rhel-7-server-extras-rpms`

* `rhel-7-server-optional-rpms`

* `rhel-server-7-ose-beta-rpms`

Normally you would get these repositories via `subscription-manager` but we have provided a mirror that we will configure in the following lab steps.

. On *each node* set up the yum repository configuration file `/etc/yum.repos.d/open.repo` with the following command.  In each instance of this step where it says node0X replace it with *node00* and *node01*:
+
----
[root@master00-GUID ~]# ssh node0X-$GUID.oslab.opentlc.com
[root@node0X-GUID ~]# cat << EOF > /etc/yum.repos.d/open.repo
[rhel-x86_64-server-7]
name=Red Hat Enterprise Linux 7
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-7
enabled=1
gpgcheck=0

[rhel-x86_64-server-rh-common-7]
name=Red Hat Enterprise Linux 7 Common
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-rh-common-7
enabled=1
gpgcheck=0

[rhel-x86_64-server-extras-7]
name=Red Hat Enterprise Linux 7 Extras
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-extras-7
enabled=1
gpgcheck=0

[rhel-x86_64-server-optional-7]
name=Red Hat Enterprise Linux 7 Optional
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-optional-7
enabled=1
gpgcheck=0

[rhel-server-7-ose-beta-rpms]
name=Red Hat Enterprise Linux 7 OSE 3
baseurl=http://www.opentlc.com/repos/ose3_beta4
enabled=1
gpgcheck=0

EOF

----

. List the available repositories on the node host:
+
-----

[root@node0X-GUID ~]# yum repolist 

-----
+
You should see the following:
+
----

repo id                           repo name                               status
rhel-server-7-ose-beta-rpms       Red Hat Enterprise Linux 7 OSE 3           16
rhel-x86_64-server-7              Red Hat Enterprise Linux 7              4,387
rhel-x86_64-server-extras-7       Red Hat Enterprise Linux 7 Extras          19
rhel-x86_64-server-optional-7     Red Hat Enterprise Linux 7 Optional     4,087
rhel-x86_64-server-rh-common-7    Red Hat Enterprise Linux 7 Common          19

----

. Exit the first node and *repeat these steps for the second node*.  When you get here on the second node, just exit:
+
----

[root@node0X-GUID ~]# exit 

----

=== Reconfigure and Re-Run the Ansible Playbook to Add Nodes

. Edit the Ansible host configuration file `/etc/ansible/hosts` on the master host and note only the master host is uncommented under `[nodes]`:
+
----

[root@master00-GUID ~]# vi /etc/ansible/hosts
...
[nodes]
master00-$GUID.oslab.opentlc.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
#node00-$GUID.oslab.opentlc.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
#node01-$GUID.oslab.opentlc.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"

---- 

. Uncommment the two nodes in the Ansible host configuration file on the master host, do not change or remove anything else from the file.  Write the file when done editing: 
+
----

[nodes]
master00-$GUID.oslab.opentlc.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
node00-$GUID.oslab.opentlc.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node01-$GUID.oslab.opentlc.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"

---- 

. Run the Ansible installer on the master host again using the `ansible-playbook` command:
+
----

[root@master00-GUID ~]# ansible-playbook ~/openshift-ansible/playbooks/byo/config.yml

----
+
[NOTE]
This will take a while to complete.  This is a good time for a break.

. The output of the `ansible-playbook` command shoold show failed=0 for all hosts:
+
----

...OUTPUT OMMITTED...
PLAY RECAP ********************************************************************
localhost                  : ok=5    changed=0    unreachable=0    failed=0
master00-GUID.oslab.opentlc.com : ok=83   changed=5    unreachable=0    failed=0
node00-GUID.oslab.opentlc.com : ok=40   changed=18   unreachable=0    failed=0
node01-GUID.oslab.opentlc.com : ok=40   changed=18   unreachable=0    failed=0

----

. After the installer is complete, check the status of your nodes using the `osc get nodes` command on the master host:
+
----

[root@master00-GUID ~]# osc get nodes
NAME                              LABELS        STATUS
master00-GUID.oslab.opentlc.com   Schedulable   <none>    Ready
node00-GUID.oslab.opentlc.com     Schedulable   <none>    NotReady
node01-GUID.oslab.opentlc.com     Schedulable   <none>    NotReady

---- 

### Scheduler and Defaults

The "scheduler" is essentially the OpenShift master. Any time a pod needs to be
created (instantiated) somewhere, the master needs to figure out where to do
this. This is called "scheduling". The default configuration for the scheduler
looks like the following JSON (although this is embedded in the OpenShift code
and you won't find this in a file):

    {
      "predicates" : [
        {"name" : "PodFitsResources"},
        {"name" : "MatchNodeSelector"},
        {"name" : "HostName"},
        {"name" : "PodFitsPorts"},
        {"name" : "NoDiskConflict"}
      ],"priorities" : [
        {"name" : "LeastRequestedPriority", "weight" : 1},
        {"name" : "ServiceSpreadingPriority", "weight" : 1}
      ]
    }

When the scheduler tries to make a decision about pod placement, first it goes
through "predicates", which essentially filter out the possible nodes we can
choose. Note that, depending on your predicate configuration, you might end up
with no possible nodes to choose. This is totally OK (although generally not
desired).

These default options are documented in the JSON code above.  Here is a quick overview of what it means:

* *PodFitsResources* - Place pod on a node that has enough resources for it

* *PodFitsPorts* - Place pod on a node that doesn't have a port conflict

* *NoDiskConflict* - Place pod on a node that doesn't have a storage conflict

And some more obscure ones:

* *MatchNodeSelector* - Place pod on a node whose `NodeSelector` matches

* *HostName* - Place pod on a node whose hostname matches the `Host` attribute value

The next thing is, of the available nodes after the filters are applied, how do
we select the "best" one. This is where "priorities" come in. Long story short,
the various priority functions each get a score, multiplied by the weight, and
the node with the highest score is selected to host the pod.

The defaults are:

* Choose the node that is "least requested" (the least busy)

* Spread services around - minimize the number of pods in the same service on the same node

[NOTE]
For an extremely detailed explanation about what these various
configuration flags are doing, check out: http://docs.openshift.org/latest/admin_guide/scheduler.html

In a small environment, the defaults are pretty sane. Let's look at one of the
important predicates (filters) before we move on to "regions" and "zones".

### The NodeSelector
`NodeSelector` is a part of the Pod data model. And, if we think back to our pod
definition, there was a "label", which is just a key:value pair. In the case of
a `NodeSelector`, our labels (key:value pairs) are used to help us try to find
nodes that match, assuming that:

* The scheduler is configured to *MatchNodeSelector*

* The end user creating the pod knows which labels are out there

This use case is pretty simplistic.  It doesn't really allow for a
topology, and there's not a lot of logic behind it. For instance, if you specify a
*NodeSelector* label when using *MatchNodeSelector* and there are no matching nodes,
my workload will *never* get scheduled.

We can make this more intelligent by using "regions" and "zones".

### Customizing the Scheduler Configuration

The Ansible installer is configured to understand "regions" and "zones" as a
matter of convenience. However, for the master (scheduler) to actually do
something with them requires changing from the default configuration.

. On the master host take a look at `/etc/openshift/master/master-config.yaml` and find the line with `schedulerConfigFile`.
+
----

[root@master00-GUID ~]# less /etc/openshift/master/master-config.yaml

----
+
You should see:
+
----

    schedulerConfigFile: "/etc/openshift/master/scheduler.json"

----

. On the master host take a look at `/etc/openshift/master/scheduler.json`:
+
----

[root@master00-GUID ~]# less /etc/openshift/master/scheduler.json

----
+
You should see:
+
----

    {
      "predicates" : [
        {"name" : "PodFitsResources"},
        {"name" : "PodFitsPorts"},
        {"name" : "NoDiskConflict"},
        {"name" : "Region", "argument" : {"serviceAffinity" : { "labels" : ["region"]}}}
      ],"priorities" : [
        {"name" : "LeastRequestedPriority", "weight" : 1},
        {"name" : "ServiceSpreadingPriority", "weight" : 1},
        {"name" : "Zone", "weight" : 2, "argument" : {"serviceAntiAffinity" : { "label" : "zone" }}}
      ]
    }

----

To quickly review the JSON code above:

* Filter out nodes that don't fit the resources, don't have the ports, or have disk conflicts

* If the pod specifies a label with the key "region", filter nodes by the value.

So, if we have the following nodes and the following labels:

* Node 1 -- "region":"infra"

* Node 2 -- "region":"primary"

* Node 3 -- "region":"primary"

If we try to schedule a pod that has a `NodeSelector` of "region":"primary",
then only Node 1 and Node 2 would be considered.

That takes care of the "region" part.  What about the "zone" part?

Our priorities tell us to:

* Score the least-busy node higher

* Score any nodes who don't already have a pod in this service higher

* Score any nodes whose zone label's value **does not** match higher

Why do we score a zone that **doesn't** match higher? Note that the definition
for the Zone priority is a `serviceAntiAffinity` -- anti affinity. In this case,
our anti affinity rule helps to ensure that we try to get nodes from *different*
zones to take our pod.

If we consider that our "primary" region might be a certain datacenter, and that
each "zone" in that datacenter might be on its own power system with its own
dedicated networking, this would ensure that, within the datacenter, pods of an
application would be spread across power/network segments.

The documentation link provided earlier has some more complicated examples. The topoligical
possibilities are endless!

### Node Labels

The assignments of "regions" and "zones" at the node-level are handled by labels
on the nodes. 

. On the master host look at how the labels were implemented with `osc get nodes`:
+
----

[root@master00-GUID ~]# osc get nodes

----
+
You should see:
+
----

NAME                              LABELS                                                                             STATUS
master00-GUID.oslab.opentlc.com   kubernetes.io/hostname=master00-GUID.oslab.opentlc.com,region=infra,zone=default   Ready
node00-GUID.oslab.opentlc.com     kubernetes.io/hostname=node00-GUID.oslab.opentlc.com,region=primary,zone=east      Ready
node01-GUID.oslab.opentlc.com     kubernetes.io/hostname=node01-GUID.oslab.opentlc.com,region=primary,zone=west      Ready

----

At this point we have a running OpenShift environment across three hosts, with
one master and three nodes, divided up into two regions -- "infrastructure"
and "primary".

From here we will start to deploy "applications" and other resources into
OpenShift.

### Useful OpenShift Logs

RHEL 7 uses `systemd` and `journal`. As such, looking at logs is not a matter of
`/var/log/messages` any longer. You will need to use `journalctl`.

Since we are running all of the components in higher loglevels, it is suggested
that you use your terminal emulator to set up windows for each process.

On the master host you should run each of the following in its own
window:

----

[root@master00-GUID ~]# journalctl -f -u openshift-master
[root@master00-GUID ~]# journalctl -f -u openshift-node

----

[NOTE]
You will want to do this on the other nodes, but you won't need the
`openshift-master` service. You may also wish to watch the Docker logs, too.
