:icons: images/icons
:toc2:

:numbered:

=== Connect to Lab

. If not already connected, connect to your administration host:
+
----

yourdesktop$ ssh -i ~/.ssh/id_rsa your-opentlc-login@oselab-GUID.oslab.opentlc.com

----

. SSH to the master host:
+
----
[root@oselab~]# ssh root@master00-GUID.oslab.opentlc.com
----
+
[NOTE]
If prompted for a password use *r3dh4t1!*
+
----

root@master00-GUID.oslab.opentlc.com's password: ******** (r3dh4t1!)

----

== Lab: Rollback/Activate and Code Lifecycle

Not every coder is perfect, and sometimes you want to rollback to a previous
incarnation of your application. Sometimes you then want to go forward to a
newer version, too.

The next few labs require that you have a Github account. We will take Marina's
"wiring" application and modify its front-end and then rebuild. We'll roll-back
to the original version, and then go forward to our re-built version.

=== Fork the Repository

. Using the git web interface, fork the following git repo:
link:https://github.com/openshift/ruby-hello-world[https://github.com/openshift/ruby-hello-world]

. Fork this into your own account by clicking the *Fork* Button at
the upper right corner of the Github web UI.

=== Create your application

NOTE: Remember that a `BuildConfig`(uration) tells OpenShift how to do a build.

. As *root*, Create a project for Marina to work with:
+
----
[root@master00 ~]# oadm new-project lifecycle --display-name="Lifecycle Lab" \
    --description="This is the project we use to learn about Lifecycle management" \
    --admin=marina --node-selector='region=primary'
----

. Switch to Marina and use the *"lifecycle"* project.
+
----
[marina@master00 ~]$ guid=`hostname|cut -f2 -d-|cut -f1 -d.`
[marina@master00 ~]$ oc login -u marina --insecure-skip-tls-verify --server=https://master00-${guid}.oslab.opentlc.com:8443
[marina@master00 ~]$ oc project lifecycle
----

. Lets create an app based on the link:https://github.com/openshift/ruby-hello-world[https://github.com/openshift/ruby-hello-world.git] repo
+
----
[marina@master00~]$ oc new-app https://github.com/openshift/ruby-hello-world.git
----
+
NOTE: You could have, of course, started the new-app with your own repo, we are only picking the "wrong" one as part of the learning exercise.

. Since we know that we want to talk to a database eventually, let's take a moment to add the environment variables for it. Conveniently, there is an env subcommand to oc. As marina, we can use it like so:
+
----
[marina@master00~]$ oc env dc/ruby-hello-world MYSQL_USER=root \
  MYSQL_PASSWORD=redhat MYSQL_DATABASE=mydb
----

. While we wait for the build to finish, lets *expose* our service to the world so we can test this from our local browser:
+

----
[marina@master00~]$ oc expose service ruby-hello-world \
  --name="ruby-hello-world" \
  --hostname=ruby-hello-world.lifecycle.cloudapps-${guid}.oslab.opentlc.com

----


. Take a look at the current `BuildConfig` for our application:
+
----

[marina@master00~]$ oc get buildconfig ruby-hello-world -o yaml

----

. Expect output similar to the following
+
----

apiVersion: v1
kind: BuildConfig
metadata:
  creationTimestamp: 2015-07-11T03:44:43Z
  name: ruby-hello-world
  namespace: lifecycle
  resourceVersion: "10546"
  selfLink: /osapi/v1beta3/namespaces/lifecycle/buildconfigs/ruby-hello-world
  uid: 2ad8d8bc-277f-11e5-a5f8-2cc260072896
spec:
  output:
    to:
      kind: ImageStreamTag
      name: ruby-hello-world:latest
  resources: {}
  source:
    git:
      uri: https://github.com/openshift/ruby-hello-world.git
    type: Git
  strategy:
    dockerStrategy:
      from:
        kind: ImageStreamTag
        name: ruby-20-centos7:latest
    type: Docker
  triggers:
  - github:
      secret: jV5Ipwr7__4ae_sZG2Jm
    type: GitHub
  - generic:
      secret: ALNUyArydLb22JqdXYIb
    type: Generic
  - imageChange:
      lastTriggeredImageID: openshift/ruby-20-centos7:latest
    type: ImageChange
status:
  lastVersion: 1

----
+

. As you can see, the current configuration points at the `openshift/ruby-hello-world` repository. Since you've forked this repo, let's go ahead and re-point our configuration.

. Use `oc edit` to re-point the configuration:
+
----

[marina@master00~]$  oc edit bc ruby-hello-world

----

. Change the "uri" reference to match the name of your Github
repository. Assuming your github user is `marina`, you would point it
to `https://github.com/marina/ruby-hello-world.git`. Save and exit
the editor.

WARNING: Please do not use `marina` as your user name, this needs to be you actual
GitHub user. for example, `https://github.com/openshift/ruby-hello-world.git`

. If you again run `oc get buildconfig ruby-hello-world -o yaml` you should see
that the `uri` has been updated.

. Run `oc get builds` to see if the new build has started:
+
----
[marina@master00~]$ oc get builds
----

. If a build has not started yet, you can start it yourself and follow the build-log:
+
----
[marina@master00~]$ oc get bc
NAME               TYPE      SOURCE
ruby-hello-world   Docker    https://github.com/marina/ruby-hello-world.git

[marina@master00~]$ oc start-build ruby-hello-world
ruby-hello-world-7

[marina@master00~]$ watch oc get builds
NAME                 TYPE      STATUS     POD
ruby-hello-world-5   Docker    Complete   ruby-hello-world-5-build
ruby-hello-world-6   Docker    Complete   ruby-hello-world-6-build
ruby-hello-world-7   Docker    Running    ruby-hello-world-7-build
...
CTRL+C

[marina@master00~]$ oc build-logs ruby-hello-world-X # Replace X with proper number from oc start-build output
I0709 23:41:08.493756       1 docker.go:69] Starting Docker build from justanother1/ruby-hello-world-7 BuildConfig ...
I0709 23:41:08.508448       1 tar.go:133] Adding to tar: /tmp/docker-build062004796/.gitignore as .gitignore
I0709 23:41:08.509588       1 tar.go:133] Adding to tar: /tmp/docker-build062004796/.sti/bin/README as .sti/bin/README
I0709 23:41:08.509953       1 tar.go:133] Adding to tar: /tmp/docker-build062004796/.sti/environment as .sti/environment
I0709 23:41:08.510183       1 tar.go:133] Adding to tar: /tmp/docker-build062004796/Dockerfile as Dockerfile
I0709 23:41:08.510548       1 tar.go:133] Adding to tar: /tmp/docker-build062004796/Gemfile as Gemfile
.......
Cropped Output
.......
----

. Create a file called *mysql-template.json*:
+
----
[marina@master00 ~]$ cat << EOF > mysql-template.json
{
  "kind": "Template",
  "apiVersion": "v1beta3",
  "metadata": {
    "name": "mysql-ephemeral",
    "creationTimestamp": null,
    "annotations": {
      "description": "MySQL database service, without persistent storage. WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing",
      "iconClass": "icon-mysql-database",
      "tags": "database,mysql"
    }
  },
  "objects": [
    {
      "kind": "Service",
      "apiVersion": "v1beta3",
      "metadata": {
        "name": "${DATABASE_SERVICE_NAME}",
        "creationTimestamp": null
      },
      "spec": {
        "ports": [
          {
            "name": "mysql",
            "protocol": "TCP",
            "port": 3306,
            "targetPort": 3306,
            "nodePort": 0
          }
        ],
        "selector": {
          "name": "${DATABASE_SERVICE_NAME}"
        },
        "portalIP": "",
        "type": "ClusterIP",
        "sessionAffinity": "None"
      },
      "status": {
        "loadBalancer": {}
      }
    },
    {
      "kind": "DeploymentConfig",
      "apiVersion": "v1beta3",
      "metadata": {
        "name": "${DATABASE_SERVICE_NAME}",
        "creationTimestamp": null
      },
      "spec": {
        "strategy": {
          "type": "Recreate",
          "resources": {}
        },
        "triggers": [
          {
            "type": "ImageChange",
            "imageChangeParams": {
              "automatic": true,
              "containerNames": [
                "mysql"
              ],
              "from": {
                "kind": "ImageStreamTag",
                "name": "mysql:latest",
                "namespace": "openshift"
              },
              "lastTriggeredImage": ""
            }
          },
          {
            "type": "ConfigChange"
          }
        ],
        "replicas": 1,
        "selector": {
          "name": "${DATABASE_SERVICE_NAME}"
        },
        "template": {
          "metadata": {
            "creationTimestamp": null,
            "labels": {
              "name": "${DATABASE_SERVICE_NAME}"
            }
          },
          "spec": {
            "containers": [
              {
                "name": "mysql",
                "image": "mysql",
                "ports": [
                  {
                    "containerPort": 3306,
                    "protocol": "TCP"
                  }
                ],
                "env": [
                  {
                    "name": "MYSQL_USER",
                    "value": "${MYSQL_USER}"
                  },
                  {
                    "name": "MYSQL_PASSWORD",
                    "value": "${MYSQL_PASSWORD}"
                  },
                  {
                    "name": "MYSQL_DATABASE",
                    "value": "${MYSQL_DATABASE}"
                  }
                ],
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "imagePullPolicy": "IfNotPresent",
                "capabilities": {},
                "securityContext": {
                  "capabilities": {},
                  "privileged": false
                }
              }
            ],
            "restartPolicy": "Always",
            "dnsPolicy": "ClusterFirst"
          }
        }
      },
      "status": {}
    }
  ],
  "parameters": [
    {
      "name": "DATABASE_SERVICE_NAME",
      "description": "Database service name",
      "value": "mysql"
    },
    {
      "name": "MYSQL_USER",
      "description": "Username for MySQL user that will be used for accessing the database",
      "generate": "expression",
      "from": "user[A-Z0-9]{3}"
    },
    {
      "name": "MYSQL_PASSWORD",
      "description": "Password for the MySQL user",
      "generate": "expression",
      "from": "[a-zA-Z0-9]{16}"
    },
    {
      "name": "MYSQL_DATABASE",
      "description": "Database name",
      "value": "sampledb"
    }
  ],
  "labels": {
    "template": "mysql-ephemeral-template"
  }
}
EOF

----

. Lets start the *database* service, This time we will do it a little differently:
.. Notice how we are providing the values and processing the *mysql-template.json* file.
.. The *oc process* command output can be saved into a file or "piped" into the *oc create* command
+
----
[marina@master00~]$ oc process -f mysql-template.json \
  --value="MYSQL_USER=root,MYSQL_PASSWORD=redhat,MYSQL_DATABASE=mydb" | \
  tee mysql-processed.json
[marina@master00~]$ oc create -f  mysql-processed.json
----
. *Or* we can process the template, add our variables and "pipe" the output directly to the *oc create* command
.. Note that we are providing the *MYSQL_* attibutes, but also choosing the name of the *service* to be created.
+
----
[marina@master00~]$ oc process -f mysql-template.json --value="MYSQL_USER=root,MYSQL_PASSWORD=redhat,MYSQL_DATABASE=mydb,DATABASE_SERVICE_NAME=database" | oc create -f -
----

. Check that your values were processed correctly
+
----
[marina@master00~]$ oc env dc/mysql --list
# deploymentconfigs mysql, container mysql
MYSQL_USER=root
MYSQL_PASSWORD=redhat
MYSQL_DATABASE=mydb
----
+
[INFO]
Your frontend needs to be "redeployed" so it checked for the DB again.

== Lab: Using Webhooks

=== Create a Webhook

Webhooks are a way to integrate external systems into your OpenShift
environment so that they can fire off OpenShift builds. Generally
speaking, one would make code changes, update the code repository, and
then some process would hit OpenShift's webhook URL in order to start
a build with the new code.

Your GitHub account has the capability to configure a webhook to request
whenever a commit is pushed to a specific branch;

. To find the webhook URL, you can visit the web console, click into the
project, click on *Browse* and then on *Builds*. You'll see two webhook
URLs.

. Copy the *Generic* one. It should look like:
+
----
https://master00-GUID.oslab.opentlc.com:8443/osapi/v1beta3/namespaces/lifecycle/buildconfigs/ruby-hello-world/webhooks/ALNUyArydLb22JqdXYIb/generic
----

. Get the *secret* password from the *BuildConfig*
+
----
[marina@master00~]$ oc get bc ruby-hello-world -o yaml
----

. It will look similar to this output, use the "secret" value in your configuration in git.
+
----
.... Cropped Output ....
  triggers:
  - github:
      secret: xTah2lioO2Bz9JZT9dPf
    type: GitHub
  - generic:
      secret: B5h3ARS88HD7S3LOcbRZ
    type: Generic
.... Cropped Output ....
----

. Complete the configuration on Github.

=== Test your WebHook

We want to make a change to the code, then, commit and push the change into the git repository.
+
NOTE: If you know how, you can do this "the normal way" but cloning your repo locally, making changes and pushing them to the repo.
+
. Github's web interface will let you make edits to files. Go to your forked
repository (eg: https://github.com/marina/ruby-hello-world), and find the file `main.erb` in the `views` folder.

. Change the following HTML:
+
----

    <div class="page-header" align=center>
      <h1> Welcome to an OpenShift v3 Demo App! </h1>
    </div>

----
+
To read (with the typo):
+
----

    <div class="page-header" align=center>
      <h1> This is my crustom demo! </h1>
    </div>

----

. When finished changing your code, commit the change to the repo.
. Now check if a build has been triggered.
. You can also check the web interface (logged in as `marina`) and see
that the build is running. Once it is complete, point your web browser
at the application: link:http://ruby-hello-world.lifecycle.cloudapps-GUID.oslab.opentlc.com/[http://ruby-hello-world.lifecycle.cloudapps-GUID.oslab.opentlc.com/]

+
You should see your big fat typo.
+
[NOTE]
Remember that it can take a minute for your service endpoint to get
updated. You might get a `503` error if you try to access the application before
this happens.
+
Since we failed to properly test our application, and our ugly typo has made it
into production, a nastygram from corporate marketing has told us that we need
to revert to the previous version, ASAP.

. If you log into the web console as `marina` and find the `Deployments` section of
the `Browse` menu, you'll see that there are two deployments of our frontend: 1
and 2.

. You can also see this information from the cli by doing:
+
----

[marina@master00~]$ oc get replicationcontroller

----
+
The semantics of this are that a `DeploymentConfig` ensures a
`ReplicationController` is created to manage the deployment of the built `Image`
from the `ImageStream`.

=== Rollback

You can rollback a deployment using the CLI.
. Check which builds you have available
+
----
[marina@master00~] oc get builds

----
. Choose a deployment and, check out what a rollback to`ruby-hello-world-#` would look like:
+
----

[marina@master00~]$ oc rollback ruby-hello-world-2 --dry-run

----

. Since it looks OK, let's go ahead and do it:
+
----

[marina@master00~]$ oc rollback ruby-hello-world-2

----
+
If you look at the `Browse` tab of your project, you'll see that in the `Pods`
section there is a `frontend-3...` pod now. After a few moments, revisit the
application in your web browser, and you should see the old "Welcome..." text.

=== Roll Back/Forward

. Corporate marketing called again. They think the typo makes us look hip and
cool. Let's now roll forward (activate) the typo-enabled application:
+
----

[marina@master00~]$ oc rollback ruby-hello-world-3

----

== Lab: Customized Build and Run Processes

OpenShift v3 supports customization of both the build and run processes.
Generally speaking, this involves modifying the various S2I scripts from the
builder image. When OpenShift builds your code, it checks to see if any of the
scripts in the `.sti/bin` folder of your repository override/supercede the
builder image's scripts. If so, it will execute the repository script instead.

More information on the scripts, their execution during the process, and
customization can be found here:

    http://docs.openshift.org/latest/creating_images/sti.html#sti-scripts

=== Add a Script

. Go to your Github repository for your application from the previous lab.

. In the Github web GUI

. Navigate to the `.sti/bin` folder.

. Click the "+" button at the top (to the right of `bin` in the breadcrumbs).

. Name your file `assemble`.

. On the master host command line find the script called `custom-assemble.sh` in the `resources` folder and paste the contents of `custom-assemble.sh` into the text area.

. Provide a nifty commit message.

. Click the "commit" button.
+
[NOTE]
If you know how to Git(hub), you can do this via your shell.

. Once the file is added, we can now do another build. The "custom" assemble
script will log some extra data.


=== Watch the Build Logs

. Using the skills you have learned, watch the build logs for this build. If you
miss them, remember that you can find the Docker container that ran the build
and look at its Docker logs.

. Did You See It?
+
----

2015-03-11T14:57:00.022957957Z I0311 10:57:00.022913       1 sti.go:357]
---> CUSTOM S2I ASSEMBLE COMPLETE

----
+
But where's the output from the custom `run` script? The `assemble` script is
run inside of your builder pod. That's what you see by using `build-logs` - the
output of the assemble script. The
`run` script actually is what is executed to "start" your application's pod. In
other words, the `run` script is what starts the Ruby process for an image that
was built based on the `ruby-20-rhel7` S2I builder.

. To look inside the builder pod, as `marina`:
+
----

[marina@master00~]$ oc logs `oc get pod | grep -e "[0-9]-build" | tail -1 | awk {'print $1'}` | grep CUSTOM

----
+
You should see something similar to:
+
----

2015-04-27T22:23:24.110630393Z ---> CUSTOM S2I ASSEMBLE COMPLETE

----

== Lab: Lifecycle Pre and Post Deployment Hooks (Optional)

Like in OpenShift 2, we have the capability of "hooks" - performing actions both
before and after the **deployment**. In other words, once an S2I build is
complete, the resulting Docker image is pushed into the registry. Once the push
is complete, OpenShift detects an `ImageChange` and, if so configured, triggers
a **deployment**.

The *pre*-deployment hook is executed just *before* the new image is deployed.

The *post*-deployment hook is executed just *after* the new image is deployed.

How is this accomplished? OpenShift will actually spin-up an *extra* instance of
your built image, execute your hook script(s), and then shut the instance down.
Neat, huh?

Since we already have our `wiring` app pointing at our forked code repository,
let's go ahead and add a database migration file. In the `beta4` folder you will
find a file called `1_sample_table.rb`. Add this file to the `db/migrate` folder
of the `ruby-hello-world` repository that you forked. If you don't add this file
to the right folder, the rest of the steps will fail.

=== Examining Deployment Hooks

Take a look at the following JSON:

    "strategy": {
        "type": "Recreate",
        "resource": {},
        "recreateParams": {
            "pre": {
                "failurePolicy": "Abort",
                "execNewPod": {
                    "command": [
                        "/bin/true"
                    ],
                    "env": [
                        {
                            "name": "CUSTOM_VAR1",
                            "value": "custom_value1"
                        }
                    ],
                    "containerName": "ruby-helloworld"
                }
            },
            "post": {
                "failurePolicy": "Ignore",
                "execNewPod": {
                    "command": [
                        "/bin/false"
                    ],
                    "env": [
                        {
                            "name": "CUSTOM_VAR2",
                            "value": "custom_value2"
                        }
                    ],
                    "containerName": "ruby-helloworld"
                }
            }
        }
    },

You can see that both a *pre* and *post* deployment hook are defined. They don't
actually do anything useful. But they are good examples.

The pre-deployment hook executes "/bin/true" whose exit code is always 0 --
success. If for some reason this failed (non-zero exit), our policy would be to
`Abort` -- consider the entire deployment a failure and stop.

The post-deployment hook executes "/bin/false" whose exit code is always 1 --
failure. The policy is to `Ignore`, or do nothing. For non-essential tasks that
might rely on an external service, this might be a good policy.

More information on these strategies, the various policies, and other
information can be found in the documentation:

    http://docs.openshift.org/latest/dev_guide/deployments.html

=== Modifying the Hooks

. Since we are talking about **deployments**, let's look at our
`DeploymentConfig`s. As the `marina` user in the `wiring` project:
+
----

[marina@master00~]$ oc get dc

----
+
You should see something like:
+
----

NAME               TRIGGERS                    LATEST VERSION
database           Change, ConfigChange        1
ruby-hello-world   ConfigChange, ImageChange   6


----

. Since we are trying to associate a Rails database migration hook with our
application, we are ultimately talking about a deployment of the frontend (ruby-hello-world). If
you edit the ruby-hello-world's `DeploymentConfig` as `marina`:
+
----

[marina@master00~]$ oc edit dc ruby-hello-world -o json

----

. Yes, the default for `oc edit` is to use YAML. For this exercise, JSON will be
easier as it is indentation-insensitive. Find the section that looks like the
following before continuing:
+
----

    "spec": {
        "strategy": {
            "type": "Recreate",
            "resources": {}
        },

----

A Rails migration is commonly performed when we have added/modified the database
as part of our code change. In the case of a pre- or post-deployment hook, it
would make sense to:

* Attempt to migrate the database

* Abort the new deployment if the migration fails

Otherwise we could end up with our new code deployed but our database schema
would not match. This could be a *Real Bad Thing (TM)*.

In the case of the `ruby-20` builder image, we are actually using RHEL7 and the
Red Hat Software Collections (SCL) to get our Ruby 2.0 support. So, the command
we want to run looks like:

    /usr/bin/scl enable ruby200 ror40 'cd /opt/openshift/src ; bundle exec rake db:migrate'

This command:

* executes inside an SCL "shell"

* enables the Ruby 2.0.0 and Ruby On Rails 4.0 environments

* changes to the `/opt/openshift/src` directory (where our applications' code is
    located)

* executes `bundle exec rake db:migrate`

If you're not familiar with Ruby, Rails, or Bundler, that's OK.

The `command` directive inside the hook's definition tells us which command to
actually execute. It is required that this is an array of individual strings.
Represented in JSON, our desired command above represented as a string array
looks like:

    "command": [
        "/usr/bin/scl",
        "enable",
        "ruby200",
        "ror40",
        "cd /opt/openshift/src ; bundle exec rake db:migrate"
    ]

This is great, but actually manipulating the database requires that we talk
**to** the database. Talking to the database requires a user and a password.
Smartly, our hook pods inherit the same environment variables as the main
deployed pods, so we'll have access to the same datbase information.

Looking at the original hook example in the previous section, and our command
reference above, in the end, you will have something that looks like:

    "strategy": {
        "type": "Recreate",
        "resources": {},
        "recreateParams": {
            "pre": {
                "failurePolicy": "Abort",
                "execNewPod": {
                    "command": [
                        "/usr/bin/scl",
                        "enable",
                        "ruby200",
                        "ror40",
                        "cd /opt/openshift/src ; bundle exec rake db:migrate"
                    ],
                    "containerName": "ruby-helloworld"
                }
            },
        }
    },

Remember, indentation isn't critical in JSON, but closing brackets and braces
are. When you are done editing the deployment config, save and quit your editor.

=== Quickly Clean Up

When we did our previous builds and rollbacks and etc, we ended up with a lot of
stale pods that are not running (`Succeeded`). Currently we do not auto-delete
these pods because we have no log store -- once they are deleted, you can't view
their logs any longer.

. For now, we can clean up by doing the following as `marina`:
+
----

[marina@master00~]$ oc get pod | grep -E "[0-9]-build" |\
   awk {'print $1'} | xargs -r oc delete pod

----

This will get rid of all of our old build and lifecycle pods. The lifecycle pods
are the pre- and post-deployment hook pods, and the sti-build pods are the pods
in which our previous builds occurred.

=== Build Again

Now that we have modified the deployment configuration and cleaned up a bit, we
need to trigger another deployment. While killing the frontend pod would trigger
another deployment, our current Docker image doesn't have the database migration
file in it. Nothing really useful would happen.

In order to get the database migration file into the Docker image, we actually
need to do another build. Remember, the S2I process starts with the builder
image, fetches the source code, executes the (customized) assemble script, and
then pushes the resulting Docker image into the registry. **Then** the
deployment happens.

. As `marina`:
+
----

[marina@master00~]$ oc start-build ruby-hello-world

----
+
Or go into the web console and click the "Start Build" button in the Builds
area.

=== Verify the Migration

. About a minute after the build completes, you should see something like the following output
of `oc get pod` as `marina`:
+
----

[marina@master00~]$ oc get pod

----
+
----

POD                                IP          CONTAINER(S)               IMAGE(S)                                                                                                                HOST                                    LABELS                                                                                                                  STATUS       CREATED         MESSAGE
database-2-rj72q                   10.1.0.15                                                                                                                                                      master00-GUID.oslab.opentlc.com/192.168.133.2   deployment=database-2,deploymentconfig=database,name=database                                                           Running      About an hour
                                               ruby-helloworld-database   registry.access.redhat.com/openshift3_beta/mysql-55-rhel7                                                                                                                                                                                                                               Running      About an hour
deployment-frontend-7-hook-4i8ch                                                                                                                                                                  node00-GUID.oslab.opentlc.com/192.168.133.3    <none>                                                                                                                  Succeeded    41 seconds
                                               lifecycle                  172.30.118.110:5000/wiring/origin-ruby-sample@sha256:2984cfcae1dd42c257bd2f79284293df8992726ae24b43470e6ffd08affc3dfd                                                                                                                                                                   Terminated   36 seconds      exit code 0
frontend-7-nnnxz                   10.1.1.24                                                                                                                                                      node00-GUID.oslab.opentlc.com/192.168.133.3    deployment=frontend-7,deploymentconfig=frontend,name=frontend                                                           Running      29 seconds
                                               ruby-helloworld            172.30.118.110:5000/wiring/origin-ruby-sample@sha256:2984cfcae1dd42c257bd2f79284293df8992726ae24b43470e6ffd08affc3dfd                                                                                                                                                                   Running      26 seconds
ruby-sample-build-7-build                                                                                                                                                                         master00-GUID.oslab.opentlc.com/192.168.133.2   build=ruby-sample-build-7,buildconfig=ruby-sample-build,name=ruby-sample-build,template=application-template-stibuild   Succeeded    2 minutes
                                               sti-build                  openshift3_beta/ose-sti-builder:v0.5.2.2                                                                                                                                                                                                                                                Terminated   2 minutes       exit code 0

----
+
You'll see that there is a single `hook`/`lifecycle` pod -- this corresponds
with the pod that ran our pre-deployment hook.

. Inspect this pod's logs:
+
----

[marina@master00~]$ oc logs deployment-frontend-7-hook-4i8ch

----
+
The output should show something like:
+
----

== 1 SampleTable: migrating ===================================================
-- create_table(:sample_table)
   -> 0.1075s
== 1 SampleTable: migrated (0.1078s) ==========================================

----
+
If you have no output, you may have forgotten to actually put the migration file
in your repo. Without that file, the migration does nothing, which produces no
output.
+
You can even talk directly to the database on its service IP/port
using the `mysql` client and the environment variables (you would need the
`mysql` package installed on your master, for example).

. As `marina`, find your database:
+
----

[marina@master00~]$ oc get service
NAME       LABELS    SELECTOR        IP(S)            PORT(S)
database   <none>    name=database   172.30.108.133   5434/TCP
frontend   <none>    name=frontend   172.30.229.16    5432/TCP

----
+
Take note of the database IP and PORT.

. Get database connection info:
+
----

[marina@master00~]$ oc get dc database -o yaml | grep -A1 MYSQL

----
+
----

  - name: MYSQL_USER
    value: [username]
  - named: MYSQL_PASSWORD
    value: [password]
  - name: MYSQL_DATABASE
    value: [database]

----

. As root on the master host install the mysql client:
+
----

[root@master00~]# yum -y install mariadb

----

. Then use the `mysql` client to connect to this service using the connection information discovered in the earlier steps and dump the table that we created:
+
----

[marina@master00~]$ mysql -u[username] \
      -p[password] \
      -h[db_IP] \
      -P[db_PORT] \
      -e'show tables; describe sample_table;' \
      [database]

----
+
----

+-------------------+
| Tables_in_root    |
+-------------------+
| sample_table      |
| key_pairs         |
| schema_migrations |
+-------------------+
+-------+--------------+------+-----+---------+----------------+
| Field | Type         | Null | Key | Default | Extra          |
+-------+--------------+------+-----+---------+----------------+
| id    | int(11)      | NO   | PRI | NULL    | auto_increment |
| name  | varchar(255) | NO   |     | NULL    |                |
+-------+--------------+------+-----+---------+----------------+

----
