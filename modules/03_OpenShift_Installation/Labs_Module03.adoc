:scrollbar:
:data-uri:
:toc2:
:icons: images/icons

== Prepare to Deploy OpenShift Enterprise
:numbered:

In this lab you deploy OpenShift Enterprise on a single host (in later labs you add more nodes).

* Configure a DNS on the `oselab` server to serve your OpenShift Enterprise environment
* Configure SSH keys
* Configure repositories
* Configure network settings
* Install Docker on the host
* Configure and install OpenShift Enterprise


== Lab Environment Architecture 

The lab environment consists of 4 VMs:

* `oselab-GUID.oslab.opentlc.com` (administration host)

* `master00-GUID.oslab.opentlc.com` (master host, contains Etcd and the management console)

* `infranode00-GUID.oslab.opentlc.com` (infranode host, runs the infrastructure containers: Registry and Router)

* `node00-GUID.oslab.opentlc.com` (node host, Region: primary, Zone: east)

* `node01-GUID.oslab.opentlc.com` (node host, Region: primary, Zone: west)

[NOTE]
A reminder: You are only allowed to use SSH to access the administration host from outside the lab environment. All other hosts have external SSH blocked.  Once on the administration host, you can use SSH to access the other hosts internally.  As described earlier, you must use your private SSH key and OPENTLC login to access the system (not `root`).

Each student lab is assigned a global unique identifier (GUID) that consists of 4 characters.  This GUID is provided in the email that is sent to you when you provision your lab environment. _Whenever you see GUID from this point on, replace it with your lab's GUID._

IMPORTANT: In each step, be especially careful to make sure that you are running the step on the required host.  Each step contains the name of the host to use, and the example code contains the host name in the shell prompt.

* Administration host example:
+
----
[root@oselab-GUID ~]# command
----

* Master host example:
+
----
[root@master00-GUID ~]# command
----


=== Configure Wildcard DNS

OpenShift Enterprise requires a wildcard DNS A record.  The wildcard A record should point to the publicly available (external) IP address of the OpenShift Enterprise router.  For this training, ensure that the router ends up on the OpenShift Enterprise server that is running the master.  Your instructor recommends that you use a low TTL for this record in order for DNS client caches to expire more quickly so that changes become available sooner. The DNS server runs on the administration host.

. Connect to your administration host `oselab-GUID.oslab.opentlc.com` (your private key location may vary).
+
----
yourdesktop$ ssh -i ~/.ssh/id_rsa your-opentlc-login@oselab-GUID.oslab.opentlc.com
----

. Become the `root` user on the administration host.
+
----
-bash-4.2$ sudo bash
----

. Install the `bind` and `bind-utils` package on the administration host.
+
----
[root@oselab-GUID ~]# yum -y install bind bind-utils
----

. On the administration host, collect and define the environment's information. You define the public IP of `InfraNode00` as the target of the wildcard record.
+
----
[root@oselab-GUID ~]# GUID=`hostname|cut -f2 -d-|cut -f1 -d.`
[root@oselab-GUID ~]# host infranode00-$GUID.oslab.opentlc.com  ipa.opentlc.com |grep infranode | awk '{print $4}'
[root@oselab-GUID ~]# HostIP=`host infranode00-$GUID.oslab.opentlc.com  ipa.opentlc.com |grep infranode | awk '{print $4}'`
[root@oselab-GUID ~]# domain="cloudapps-$GUID.oslab.opentlc.com"
----

. On the administration host, create the zone file with the wildcard DNS.
+
----
[root@oselab-GUID ~]# mkdir /var/named/zones
[root@oselab-GUID ~]# echo "\$ORIGIN  .
\$TTL 1  ;  1 seconds (for testing only)
${domain} IN SOA master.${domain}.  root.${domain}.  (
  2011112904  ;  serial
  60  ;  refresh (1 minute)
  15  ;  retry (15 seconds)
  1800  ;  expire (30 minutes)
  10  ; minimum (10 seconds)
)
  NS master.${domain}.
\$ORIGIN ${domain}.
test A ${HostIP}
* A ${HostIP}"  >  /var/named/zones/${domain}.db
----

. Configure `named.conf` on the administration host.
+
----
[root@oselab-GUID ~]# echo "// named.conf
options {
  listen-on port 53 { any; };
  directory \"/var/named\";
  dump-file \"/var/named/data/cache_dump.db\";
  statistics-file \"/var/named/data/named_stats.txt\";
  memstatistics-file \"/var/named/data/named_mem_stats.txt\";
  allow-query { any; };
  recursion yes;
  /* Path to ISC DLV key */
  bindkeys-file \"/etc/named.iscdlv.key\";
};
logging {
  channel default_debug {
    file \"data/named.run\";
    severity dynamic;
  };
};
zone \"${domain}\" IN {
  type master;
  file \"zones/${domain}.db\";
  allow-update { key ${domain} ; } ;
};" > /etc/named.conf
----

. On the administration host, correct file permissions and start the DNS server.
+
----
[root@oselab-GUID ~]# chgrp named -R /var/named
[root@oselab-GUID ~]# chown named -R /var/named/zones
[root@oselab-GUID ~]# restorecon -R /var/named
[root@oselab-GUID ~]# chown root:named /etc/named.conf
[root@oselab-GUID ~]# restorecon /etc/named.conf
----

. Enable and start `named` on the administration host.
+
----
[root@oselab-GUID ~]# systemctl enable named
[root@oselab-GUID ~]# systemctl start named
----

. Configure `firewalld` on the administration host to allow inbound DNS queries.
+
----
[root@oselab-GUID bin]# firewall-cmd --zone=public --add-service=dns --permanent
[root@oselab-GUID bin]# firewall-cmd --reload
----

=== Verify DNS Configuration

A test DNS entry was created called `test.cloudapps-GUID.oslab.opentlc.com`.

. First, test the DNS server running on the administration host.
+
----
[root@oselab-GUID ~]# host test.cloudapps-$GUID.oslab.opentlc.com 127.0.0.1
----

. Second, test with an external name server.
+
----
[root@oselab-GUID ~]# host test.cloudapps-$GUID.oslab.opentlc.com 8.8.8.8
----
+
[NOTE]
The first time you query 8.8.8.8 you may notice lag and an error "connection timed out; trying next origin Host test.cloudapps-GUID.oslab.opentlc.com not found: 3(NXDOMAIN)". This is normal.  If you run the test again, it will go faster and not error out.

. Test DNS from your laptop/desktop. It might take a few minutes to update. Be sure to replace GUID with the correct GUID.
+
----
Desktop$ nslookup test.cloudapps-$GUID.oslab.opentlc.com
----

=== Configure SSH Keys

The OpenShift Enterprise installer uses SSH to configure hosts.  In this lab you create and install an SSH key pair on the master host and add the public key to the `authorized_hosts` file.

. Use SSH to access the master host from the admin host and create an SSH key pair for the `root` user.
+
----
[root@oselab-GUID ~]# ssh master00-$GUID
...[output omitted]...
[root@master00-GUID ~]# ssh-keygen -f /root/.ssh/id_rsa -N ''
----
+
[NOTE]
If a key exists, allow `ssh-keygen` to overwrite it.

. On the master host, locally add the public SSH key to `/root/.ssh/authorized_keys`.
+
----
[root@master00-GUID ~]# cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys
----

. Configure `/etc/ssh/ssh_conf` to disable `StrictHostKeyChecking` on the master host.
+
----
[root@master00-GUID ~]# echo StrictHostKeyChecking no >> /etc/ssh/ssh_config
----
+
[NOTE]
Only do this for hosts that are used for development, testing, or demos!

. From the master host, test the new SSH key by connecting it to itself over the loopback interface without a keyboard prompt.
+
----
[root@master00-GUID ~]# ssh 127.0.0.1
...[output omitted]...
[root@master00-GUID ~]# exit
----

. Copy the SSH key to the rest of the nodes in the environment.
+
----
[root@master00-GUID ~]# GUID=`hostname|cut -f2 -d-|cut -f1 -d.`
[root@master00-GUID ~]# for node in infranode00-$GUID.oslab.opentlc.com node00-$GUID.oslab.opentlc.com node01-$GUID.oslab.opentlc.com; do ssh-copy-id root@$node ; done
----
+
[NOTE]
Remember: The default `root` password is `r3dh4t1!`.


=== Configure the Repositories on the Master Host

OpenShift Enterprise requires several software repositories:

* `rhel-7-server-rpms`

* `rhel-7-server-extras-rpms`

* `rhel-7-server-optional-rpms`

* `rhel-server-7-ose-rpms`

Normally you obtain these repositories via `subscription-manager` but your instructor has provided a mirror that you configure in the following steps:

. If not already connected, use SSH to access your master host `master00-GUID.oslab.opentlc.com` from the admin host.
+
----
[yourlogin@oselab-GUID ~]$ ssh root@master00-$GUID.oslab.opentlc.com
----

. Your instructor highly recommends that you install a terminal multiplexing tool such as `tmux` or `screen` in case you lose connectivity to your environment.  This keeps your place in your session if you are disconnected.  You are allowed to install the `tmux` or `screen` package using `yum` on the master host. Neither one is installed by default.
+
[NOTE]
====
For more information on using `tmux`, use `man tmux` after installing the package. For more information on using `screen`, use `man screen` after installing the package.
====

. On the master host, set up the `yum` repository configuration file `/etc/yum.repos.d/open.repo` with the following repositories:
+
----
[root@master00-GUID ~]# cat << EOF > /etc/yum.repos.d/open.repo
[rhel-x86_64-server-7]
name=Red Hat Enterprise Linux 7
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-7
enabled=1
gpgcheck=0

[rhel-x86_64-server-extras-7]
name=Red Hat Enterprise Linux 7 Extras
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-extras-7
enabled=1
gpgcheck=0

[rhel-x86_64-server-optional-7]
name=Red Hat Enterprise Linux 7 Optional
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-optional-7
enabled=1
gpgcheck=0

# This repo is added for the OPENTLC environment not OSE
[rhel-x86_64-server-rh-common-7]
name=Red Hat Enterprise Linux 7 Common
baseurl=http://www.opentlc.com/repos/rhel-x86_64-server-rh-common-7
enabled=1
gpgcheck=0


EOF
----

. Add the OpenShift Enterprise repository mirror to the master host.
+
----
[root@master00-GUID ~]# cat << EOF >> /etc/yum.repos.d/open.repo
[rhel-7-server-ose-3.0-rpms]
name=Red Hat Enterprise Linux 7 OSE 3
baseurl=http://www.opentlc.com/repos/rhel-7-server-ose-3.0-rpms
enabled=1
gpgcheck=0

EOF
----

. List the available repositories on the master host.
+
-----
[root@master00-GUID ~]# yum repolist
-----

* You should see the following:
+
----
Loaded plugins: product-id
...[output omitted]...
repo id                           repo name                               status
rhel-7-server-ose-3.0-rpms        Red Hat Enterprise Linux 7 OSE 3           25
rhel-x86_64-server-7              Red Hat Enterprise Linux 7              4,387
rhel-x86_64-server-extras-7       Red Hat Enterprise Linux 7 Extras          19
rhel-x86_64-server-optional-7     Red Hat Enterprise Linux 7 Optional     4,087
rhel-x86_64-server-rh-common-7    Red Hat Enterprise Linux 7 Common          19
...[output omitted]...
----

. Configure the nodes by copying the `repo` file to all the nodes directly from the master.
+
-----
[root@master00-GUID ~]# for node in infranode00-$GUID.oslab.opentlc.com node00-$GUID.oslab.opentlc.com node01-$GUID.oslab.opentlc.com; do scp /etc/yum.repos.d/open.repo ${node}:/etc/yum.repos.d/open.repo ; done
-----


=== Verify Network Configuration

In this lab you verify that the master host is configured correctly for internal and external DNS name resolution.

. Verify the `hostname` for the master host.
+
----
[root@master00-GUID ~]# hostname -f
----

* You should see the following:
+
----
master00-GUID.oslab.opentlc.com
----

. Take note of the master host's internal IP address.
+
----
[root@master00-GUID ~]# ip address show dev eth0|grep "inet "|awk '{print $2}'|cut -f1 -d/
----

. Make sure the master host's internal DNS entry matches the internal IP address.
+
----
[root@master00-GUID ~]# host `hostname -f`
----

. Take note of the master host's external IP address.
+
----
[root@master00-GUID ~]# curl http://www.opentlc.com/getip
----

. Make sure the master host's external DNS entry matches the external IP address.
+
----
[root@master00-GUID ~]# host `hostname -f` 8.8.8.8
----
+
NOTE: If this does not work on the first try, wait a short while and try again. It may take some time for the global DNS servers to update.

. Remove `NetworkManager`.
+
----
[root@master00-GUID ~]# yum -y remove NetworkManager*
----
+
NOTE: It is possible to configure `NetworkManager`  so it doesn't need to be removed.

. Remove `NetworkManager` from the rest of the nodes.
+
----
[root@master00-GUID ~]# for node in infranode00-$GUID.oslab.opentlc.com node00-$GUID.oslab.opentlc.com node01-$GUID.oslab.opentlc.com; do ssh $node "yum -y  remove NetworkManager*"  ; done
----

. Install the following tools and utilities on the master:
+
----
[root@master00-GUID ~]# yum -y install wget git net-tools bind-utils iptables-services bridge-utils python-virtualenv gcc bash-completion bash-completion
----


=== Install Docker

OpenShift Enterprise uses Docker to store and manage container images.  In this lab, you install Docker.

. Install the `docker` package on the master host.
+
----
[root@master00-GUID ~]# yum -y install docker
----
+
NOTE: In this lab, you need to run these commands on the nodes. In a later lab you learn a command to install them all at once.
+
CAUTION: Make sure you run all the commands on the master host.

. Install the `docker` package on the rest of the nodes.
+
----
[root@master00-GUID ~]# for node in infranode00-$GUID.oslab.opentlc.com node00-$GUID.oslab.opentlc.com node01-$GUID.oslab.opentlc.com; do ssh $node "yum -y install docker"  ; done
----

. Configure the Docker registry on the master.
+
----
[root@master00-GUID ~]# sed -i "s/OPTIONS.*/OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0\/0'/" /etc/sysconfig/docker
----
+
[NOTE]
You are using this value because the local registry will be deployed under this subnet.

. Configure the Docker registry on the rest of the nodes.
+
----
[root@master00-GUID ~]# for node in infranode00-$GUID.oslab.opentlc.com node00-$GUID.oslab.opentlc.com node01-$GUID.oslab.opentlc.com; do scp  /etc/sysconfig/docker $node:/etc/sysconfig/docker ; done
----

=== Configure Docker Storage

In this lab you configure the Docker storage pool.

NOTE: The default Docker storage configuration uses loopback devices and is not appropriate for production. Red Hat considers the `dm.thinpooldev` storage option to be the only appropriate configuration for production use.

. Stop the Docker daemon and remove the out-of-the-box loopback docker storage from the host.
+
----
[root@master00-GUID ~]# rm -rf /var/lib/docker/*
----

. Do the same for the rest of the nodes.
+
----
[root@master00-GUID ~]# for node in infranode00-$GUID.oslab.opentlc.com node00-$GUID.oslab.opentlc.com node01-$GUID.oslab.opentlc.com; do ssh $node "rm -rf /var/lib/docker/*"  ; done
----

. Run `docker-storage-setup` on the infranode host to create logical volumes for Docker.
+
NOTE: To use `dm.thinpooldev` you must have space available for an LVM thinpool. The `docker-storage-setup` package helps you configure the LVM.
+
----
[root@infranode00-GUID ~]# pvcreate /dev/vdb
[root@infranode00-GUID ~]# vgextend `vgs | grep rhel | awk '{print $1}'` /dev/vdb
[root@infranode00-GUID ~]# docker-storage-setup
----

* You should see the following:
+
----
  Rounding up size to full physical extent 32.00 MiB
  Logical volume "docker-poolmeta" created.
  Logical volume "docker-pool" created.
  WARNING: Converting logical volume rhel_host2cc260760b15/docker-pool and rhel_host2cc260760b15/docker-poolmeta to pool's data and metadata volumes.
  THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
  Converted rhel_host2cc260760b15/docker-pool to thin pool.
  Logical volume "docker-pool" changed.
----
+
[NOTE]
Be careful with `docker-storage-setup` as it will, by default, find any unused extents in the volume group that contains your root filesystem to create the pool.  You can also specify a specific volume group or block device.  This can be a destructive process to the specified VG or block device!  Consult the OpenShift Enterprise documentation for more information.

. Use the SSH command from the master host to do this quickly for all hosts.
+
----
[root@master00-GUID ~]# for node in infranode00-$GUID.oslab.opentlc.com node00-$GUID.oslab.opentlc.com node01-$GUID.oslab.opentlc.com
do
  ssh $node "pvcreate /dev/vdb ; vgextend `vgs | grep rhel | awk '{print $1}'` /dev/vdb; docker-storage-setup ; "
  ssh $node "systemctl enable docker; reboot "
done
----

. On the master host examine the newly created `docker-pool` logical volume.
+
----
[root@master00-GUID ~]# lvs /dev/rhel_host2cc260760b15/docker-pool
----

* You should see the following:
+
----
  LV          VG                    Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  docker-pool rhel_host2cc260760b15 twi-a-t--- 5.98g             0.00   0.11
----

. On the master host, examine the docker storage configuration.
+
----
[root@master00-GUID ~]# cat /etc/sysconfig/docker-storage
----

* You should see the following:
+
----
DOCKER_STORAGE_OPTIONS=-s devicemapper --storage-opt dm.fs=xfs --storage-opt dm.thinpooldev=/dev/mapper/rhel_host2cc260760b15-docker--pool
----

. Enable, start, and get status for the Docker service on the master host.
+
----
[root@master00-GUID ~]# systemctl enable docker
----

. Reboot the master host.
+
-----
[root@master00-GUID ~]# reboot
-----

=== Populate the Local Docker Registry

. Log back in to the master host after the reboot from the previous lab is complete.

. Log in to each node and check that the Docker service is started.
+
----
[root@master00-GUID ~]# GUID=`hostname|cut -f2 -d-|cut -f1 -d.`
[root@master00-GUID ~]# ssh infranode00-$GUID.oslab.opentlc.com "systemctl status docker"
[root@master00-GUID ~]# ssh node00-$GUID.oslab.opentlc.com "systemctl status docker"
[root@master00-GUID ~]# ssh node01-$GUID.oslab.opentlc.com "systemctl status docker"
----

* You should see the following:
+
----
docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled)
   Active: active (running) since Wed 2015-06-10 15:31:11 EDT; 1s ago
...OUTPUT OMITTED...
----
+
[NOTE]
Make sure the status is `enabled` and `active (running)`.

. To save time later, pre-fetch the Docker images to all the nodes in the primary region (`node00` and `node01`).
+
----
[root@node0X-GUID ~]# REGISTRY="registry.access.redhat.com";PTH="openshift3"
[root@node0X-GUID ~]# docker pull $REGISTRY/$PTH/ose-haproxy-router:v3.0.0.1 ; \
docker pull $REGISTRY/$PTH/ose-deployer:v3.0.0.1 ; \
docker pull $REGISTRY/$PTH/ose-sti-builder:v3.0.0.1 ; \
docker pull $REGISTRY/$PTH/ose-sti-image-builder:v3.0.0.1 ; \
docker pull $REGISTRY/$PTH/ose-docker-builder:v3.0.0.1 ; \
docker pull $REGISTRY/$PTH/ose-pod:v3.0.0.1 ; \
docker pull $REGISTRY/$PTH/ose-keepalived-ipfailover:v3.0.0.1 ; \
docker pull $REGISTRY/$PTH/ruby-20-rhel7 ; \
docker pull $REGISTRY/$PTH/mysql-55-rhel7 ; \
docker pull openshift/hello-openshift:v0.4.3
----
+
[NOTE]
This takes about 10 minutes on each node to complete. You do not have to wait, just connect to each node, run the pull and continue with other tasks.

. On `infranode00`, pull the *Registry* and *Router* images.
+
----
[root@infranode00-GUID ~]# REGISTRY="registry.access.redhat.com";PTH="openshift3"
[root@infranode00-GUID ~]# docker pull $REGISTRY/$PTH/ose-haproxy-router:v3.0.0.1 ; \
docker pull $REGISTRY/$PTH/ose-deployer:v3.0.0.1 ; \
docker pull $REGISTRY/$PTH/ose-docker-registry:v3.0.0.1 ;
----

. Examine Docker pool info on the `node0X` (i.e., `node00`, `node01`, etc.) host.
+
----
[root@node0X-GUID ~]# docker info
----

* You should see something similar to the following:
+
----
Containers: 0
Images: 70
Storage Driver: devicemapper
 Pool Name: rhel_host2cc260760b15-docker--pool
 Pool Blocksize: 524.3 kB
 Backing Filesystem: xfs
 Data file:
 Metadata file:
 Data Space Used: 3.5 GB
 Data Space Total: 6.417 GB
 Data Space Available: 2.918 GB
 Metadata Space Used: 1.081 MB
 Metadata Space Total: 33.55 MB
 Metadata Space Available: 32.47 MB
 Udev Sync Supported: true
 Library Version: 1.02.93-RHEL7 (2015-01-28)
Execution Driver: native-0.2
Kernel Version: 3.10.0-229.el7.x86_64
Operating System: Red Hat Enterprise Linux Server 7.1 (Maipo)
CPUs: 2
Total Memory: 1.797 GiB
Name: infranode00-GUID.oslab.opentlc.com
...
----

. On the `node0X` host, examine the `docker-pool` logical volume again.
+
----
[root@node0X-GUID ~]# lvs /dev/rhel_host2cc260760b15/docker-pool
----

* You should see something similar to the following:
+
----
  LV          VG                    Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  docker-pool rhel_host2cc260760b15 twi-aot--- 5.98g             54.53  3.22
----

== Install OpenShift Enterprise

=== Download the Installer

. On the `master00` host, download and unpack the installation utility on a host that has SSH access to your intended master and node hosts.
+
----

[root@master00-GUID ~]# curl -o oo-install-ose.tgz http://www.opentlc.com/download/ose_implementation/oo-install-ose.tgz

[root@master00-GUID ~]# tar -zxf oo-install-ose.tgz

----

. (Optional) Copy the master and node names to your paste buffer.
+
----
[root@master00-GUID ~]# for node in master00-$GUID.oslab.opentlc.com infranode00-$GUID.oslab.opentlc.com node00-$GUID.oslab.opentlc.com node01-$GUID.oslab.opentlc.com; do echo $node ; done
master00-GUID.oslab.opentlc.com
infranode00-GUID.oslab.opentlc.com
node00-GUID.oslab.opentlc.com
node01-GUID.oslab.opentlc.com

----

=== Run the Installer

. Run the installation utility to interactively configure one or more hosts.
+
----
[root@master00-GUID ~]# ./oo-install-ose
----


. Follow the instructions in the installer.
+
[NOTE]
These instructions will be changing soon. Red Hat is working to add features to the text installer.
+
----
Welcome to the OpenShift Enterprise 3 installation.

Please confirm that following prerequisites have been met:

* All systems where OpenShift will be installed are running Red Hat Enterprise Linux 7.
* All systems are properly subscribed to the required OpenShift Enterprise 3 repositories.
* All systems have run docker-storage-setup (part of the Red Hat docker RPM).
* All systems have working DNS that resolves not only from the perspective of the installer but also from within the cluster.

When the process completes you will have a default configuration for Masters and Nodes.  For ongoing environment maintenance it's recommended that the official Ansible playbooks be used.

For more information on installation prerequisites please see: https://docs.openshift.com/enterprise/latest/admin_guide/install/prerequisites.html

Are you ready to continue?  y/Y to confirm, or n/N to abort [n]:
----

. Enter *y*. 

* You should see the following:
+
----

This installation process will involve connecting to remote hosts via ssh.  Any account may be used however if a non-root account is used it must have passwordless sudo access.

User for ssh access [root]: root

----

. Answer `root`. 

* You should see the following:
+
----

***Master Configuration***

The OpenShift Master serves the API and web console.  It also coordinates the jobs that have to run across the environment.  It can even run the datastore. For wizard based installations the database will be embedded.  It's possible to change this later using etcd from Red Hat Enterprise Linux 7.

Any Masters configured as part of this installation process will also be configured as Nodes.  This is so that the Master will be able to proxy to Pods from the API.  By default this Node will be unscheduleable but this can be changed after installation with 'oadm manage-node'.

http://docs.openshift.com/enterprise/latest/architecture/infrastructure_components/kubernetes_infrastructure.html#master


Next we will launch an editor for entering masters.  The default editor in your environment can be overridden exporting the VISUAL environment variable.

Press any key to continue ...


----


. Press any key, and then press *i* to enter insert mode.
. Enter the following host:
+
----

master00-GUID.oslab.opentlc.com

----

. Press *Esc*, and then enter *:wq* to exit `vi`. 

* You should see the following:
+
----
1) master00-GUID.oslab.opentlc.com
Please confirm the following masters.  y/Y to confirm, or n/N to edit [n]:
----

. Enter *y* to confirm the master hosts.
+
----

***Node Configuration***

The OpenShift Node provides the runtime environments for containers.  It will host the required services to be managed by the Master.

By default all Masters will be configured as Nodes.

http://docs.openshift.org/latest/architecture/infrastructure_components/kubernetes_infrastructure.html#node


Next we will launch an editor for entering nodes.  The default editor in your environment can be overridden exporting the VISUAL environment variable.

Press any key to continue ...

----

. Press any key.

. Press *o* to add a line after `master00-GUID`.
. Add the infranode and the two nodes.
+
[NOTE]
Be sure to leave the master host in the list, as it is also a node.
+
----

master00-GUID.oslab.opentlc.com
infranode00-GUID.oslab.opentlc.com
node00-GUID.oslab.opentlc.com
node01-GUID.oslab.opentlc.com

----

. Press *Esc*, and then enter *:wq* to exit `vi`. 

* You should see the following:
+
----
1) master00-GUID.oslab.opentlc.com
2) infranode00-GUID.oslab.opentlc.com
3) node00-GUID.oslab.opentlc.com
4) node01-GUID.oslab.opentlc.com


Please confirm the following masters.  y/Y to confirm, or n/N to edit [n]:
----

. Enter *y* to confirm the node hosts.
+
----
Gathering information from hosts...
You'll now be asked to edit a file that will be used to validate settings gathered from the Masters and Nodes.  Since it's often the case that the hostname for a system inside the cluster is different from the hostname that is resolveable from commandline or web clients these settings cannot be validated automatically.

For some cloud providers the installer is able to gather metadata exposed in the instance so reasonable defaults will be provided.

Press any key to continue ...


----

. Press any key and verify that the information gathered is correct. The order may vary.
+
[NOTE] 
You might be concerned that the "public" IPs shown here are actually our internal IPs. If you were planning to expose your nodes to the world, you would set the real public IPs here.
+
----
infranode00-GUID.oslab.opentlc.com,192.168.0.101,192.168.0.101,infranode00-GUID.oslab.opentlc.com,infranode00-GUID.oslab.opentlc.com
master00-GUID.oslab.opentlc.com,192.168.0.100,192.168.0.100,master00-GUID.oslab.opentlc.com,master00-GUID.oslab.opentlc.com
node00-GUID.oslab.opentlc.com,192.168.0.200,192.168.0.200,node00-GUID.oslab.opentlc.com,node00-GUID.oslab.opentlc.com
node01-GUID.oslab.opentlc.com,192.168.0.201,192.168.0.201,node01-GUID.oslab.opentlc.com,node01-GUID.oslab.opentlc.com
----

. Enter *:wq* to exit `vi`. 

* You should see the following:
+
----
If changes are needed to the values recorded by the installer please update /root/.config/openshift/installer.cfg.yml.

Proceed? y/Y to confirm, or n/N to exit [y]:
----

. Enter *y* to start the install.
+
----
PLAY [Populate oo_masters_to_config host group] *******************************
PLAY [Populate oo_masters_to_config host group] *******************************

TASK: [add_host ] *************************************************************
ok: [localhost] => (item=192.168.0.100)

PLAY [Configure master instances] *********************************************

GATHERING FACTS ***************************************************************
ok: [192.168.0.100]

TASK: [os_firewall | Install firewalld packages] ******************************
skipping: [192.168.0.100]

TASK: [os_firewall | Check if iptables-services is installed] *****************

....
....
....

PLAY RECAP ********************************************************************
infranode00-GUID.oslab.opentlc.com : ok=40   changed=0    unreachable=0    failed=0
localhost                  : ok=5    changed=0    unreachable=0    failed=0
master00-GUID.oslab.opentlc.com : ok=94   changed=0    unreachable=0    failed=0
node00-GUID.oslab.opentlc.com : ok=40   changed=0    unreachable=0    failed=0
node01-GUID.oslab.opentlc.com : ok=40   changed=0    unreachable=0    failed=0

If this is your first time installing please take a look at the Administrator Guide for advanced options related to routing, storage, authentication and much more:

http://docs.openshift.com/enterprise/latest/admin_guide/overview.html

Press any key to continue ...
Removing temporary assets.
Please see /tmp/oo-install-ose-20150630-2050.log for full output.

The installation was successful!

----

. Add the default route to the OpenShift Enterprise master configuration file.
+
----
echo "configuration:
  subdomain: cloudapps-$GUID.oslab.opentlc.com" >> /etc/origin/master/master-config.yaml
----

. Verify that your `master-config.yaml` file was updated correctly.
+
NOTE: Remember that `yaml` files are space-sensitive.

. After the installer has completed, press any key and reboot the master host.
+
----
root@master00-GUID ~]# reboot
----

=== Verify Your Environment

. Log back in to the master.
. Run `oc get nodes` to check the status of your hosts.
+
----

root@master00-GUID ~]# oc get nodes
NAME                                 LABELS                                                                        STATUS
infranode00-GUID.oslab.opentlc.com   kubernetes.io/hostname=infranode00-GUID.oslab.opentlc.com                     Ready
master00-GUID.oslab.opentlc.com      kubernetes.io/hostname=master00-GUID.oslab.opentlc.com							    Ready,SchedulingDisabled
node00-GUID.oslab.opentlc.com        kubernetes.io/hostname=node00-GUID.oslab.opentlc.com                          Ready
node01-GUID.oslab.opentlc.com        kubernetes.io/hostname=node01-GUID.oslab.opentlc.com                          Ready



----


== Configure and Set Up OpenShift Enterprise

=== Set Regions and Zones

Labels on the nodes handle the assignments of _regions_ and _zones_ at the node level.

. Label the nodes.
+
----
root@master00-GUID ~]# oc label node infranode00-$GUID.oslab.opentlc.com region="infra" zone="infranodes"
root@master00-GUID ~]# oc label node node00-$GUID.oslab.opentlc.com region="primary" zone="east"
root@master00-GUID ~]# oc label node node01-$GUID.oslab.opentlc.com region="primary" zone="west"
----


. On the master host, run `oc get nodes` to see how the labels were implemented.
+
----

[root@master00-GUID ~]# oc get nodes

----

* You should see the following:
+
----

NAME                                 LABELS                                                                                   STATUS
infranode00-GUID.oslab.opentlc.com   kubernetes.io/hostname=infranode00-GUID.oslab.opentlc.com,region=infra,zone=infranodes   Ready
master00-GUID.oslab.opentlc.com      kubernetes.io/hostname=master00-GUID.oslab.opentlc.com                                   Ready,SchedulingDisabled
node00-GUID.oslab.opentlc.com        kubernetes.io/hostname=node00-GUID.oslab.opentlc.com,region=primary,zone=east            Ready
node01-GUID.oslab.opentlc.com        kubernetes.io/hostname=node01-GUID.oslab.opentlc.com,region=primary,zone=west            Ready

----

At this point, you have a running OpenShift Enterprise environment across three hosts, with one master and three nodes, divided into two regions: _infra_ and _primary_.

From here you start to deploy applications and other resources into OpenShift Enterprise.

=== Configure OpenShift Enterprise: Tips

.Setting the Default Route

To set a _default route_, run the following:

[source,bash]
----
[root@master00-GUID ~]# sed -i "s/router.default.local/cloudapps-${GUID}.oslab.opentlc.com/g" /etc/origin/master/master-config.yaml
[root@master00-GUID ~]# systemctl restart openshift-master

----

.Logs and `journalctl`  (Reference Only)
Red Hat Enterprise Linux 7 uses `systemd` and `journal`. Because of this, you no longer use `/var/log/messages` to look at logs. You now use `journalctl`.

Because Red Hat Enterprise Linux 7 runs all components in higher log levels, your instructor recommends that you use your terminal emulator to set up windows for each process.

On the master host, run each of the following in its own window:

----

[root@master00-GUID ~]# journalctl -f -u openshift-master
[root@master00-GUID ~]# journalctl -f -u openshift-node

----

[NOTE]
You might want to run this on the other nodes, but you do not need the `openshift-master` service. You might also want to watch the Docker logs.

.Setting the Default `NodeSelector` (Reference Only)

To set a default `NodeSelector`, run the following:

CAUTION: Do not set a default `NodeSelector` in your lab environment. Skip this step.

[source,bash]
----
[root@master00-GUID ~]# sed -i 's/defaultNodeSelector: ""/defaultNodeSelector: "region=primary"' /etc/origin/master/master-config.yaml
[root@master00-GUID ~]# systemctl restart openshift-master
----



=== The Registy and Router

In the scenario you are simulating in the lab, you are using `infranode00` as the target for both the _registry_ and the _default router_.

. To make a node unschedulable, run the following:
+
[NOTE]
You do not need this in your lab environment, because the installer already makes the master unschedulable.
+
----
[root@master00-GUID ~]# oadm manage-node master00-$GUID.oslab.opentlc.com  --schedulable=false
master00-GUID.oslab.opentlc.com   kubernetes.io/hostname=master00-GUID.oslab.opentlc.com,region=infra,zone=na   Ready,SchedulingDisabled
----

. Deploy the `registry`.
+
----

[root@master00-GUID ~]# oadm registry  --credentials=/etc/openshift/master/openshift-registry.kubeconfig  --images='registry.access.redhat.com/openshift3/ose-docker-registry:v3.0.0.1' --selector='region=infra'

----

. To see the status of your pod, run the following.
+
[NOTE]
This can take a few minutes the first time you run it as the images are being pulled from the registry.
+
----

NAME                       READY     REASON    RESTARTS   AGE
docker-registry-1-deploy   0/1       Running   0          6s

... Wait a few seconds ...

NAME                      READY     REASON    RESTARTS   AGE
docker-registry-1-j6hdu   1/1       Running   0          59s

----

. Deploy the default router.
+
----
[root@master00-GUID ~]# oadm router trainingrouter --stats-password='r3dh@t1!' --replicas=1 \
--config=/etc/openshift/master/admin.kubeconfig  \
--credentials='/etc/openshift/master/openshift-router.kubeconfig' \
--images='registry.access.redhat.com/openshift3/ose-haproxy-router:v3.0.0.1' \
--selector='region=infra'
----

* You should see the following:
+
----
deploymentconfigs/trainingrouter
services/trainingrouter
----

. In a separate terminal, check the status of your pods.
+
----
[root@master00-GUID ~]# watch oc get pods
NAME                      READY     REASON    RESTARTS   AGE
...
trainingrouter-1-deploy   0/1       Pending   0          4s

.. Wait a few seconds ..

NAME                      READY     REASON    RESTARTS   AGE
...
trainingrouter-1-22mr1    0/1       Pending   0          2s
trainingrouter-1-deploy   1/1       Running   0          8s

.. Wait a few seconds ..

NAME                      READY     REASON    RESTARTS   AGE
...
trainingrouter-1-22mr1    0/1       Running   0          8s
trainingrouter-1-deploy   1/1       Running   0          14s

----
+
Your output probably also includes the Docker registry pods.

. To exit the watch on `oc get pods`, press *Ctrl + C*.


=== Populate OpenShift Enterprise (Reference Only)

OpenShift Enterprise ships with _image streams_ and _templates_. They reside in `/usr/share/openshift/examples/`.  The installer imports all the image streams and templates for you from this directory.

* Take a look at the JSON files in `/usr/share/openshift/examples`.

[IMPORTANT]
The remaining steps in this lab are for reference only. You would run the commands shown only if you needed to perform the described task for some reason.

* To re-create the core set of image streams that use images based on Red Hat Enterprise Linux 7:
+
----

 oc create -f /usr/share/openshift/examples/image-streams/image-streams-rhel7.json -n openshift
----

* To create the core set of database templates:
+
----
 oc create -f /usr/share/openshift/examples/db-templates -n openshift
----

* To create the core QuickStart templates:
+
----

 oc create -f /usr/share/openshift/examples/quickstart-templates -n openshift

----

== Configure Authentication

. Create a copy of your master's configuration file.
+
----
[root@master00-GUID ~]# cp /etc/origin/master/master-config.yaml /etc/origin/master/master-config.yaml.original
----
. Edit `/etc/origin/master/master-config.yaml` so that the `oauthConfig` section looks like the following:
+
----
oauthConfig:
  assetPublicURL: https://master00-GUID.oslab.opentlc.com:8443/console/
  grantConfig:
    method: auto
  identityProviders:
  - name: htpasswd_auth
    challenge: true
    login: true
    provider:
      apiVersion: v1
      kind: HTPasswdPasswordIdentityProvider
      file: /etc/openshift/openshift-passwd
  masterPublicURL: https://master00-GUID.oslab.opentlc.com:8443
  masterURL: https://master00-GUID.oslab.opentlc.com:8443
  sessionConfig:
    sessionMaxAgeSeconds: 3600
    sessionName: ssn
    sessionSecretsFile:
  tokenConfig:
    accessTokenMaxAgeSeconds: 86400
    authorizeTokenMaxAgeSeconds: 500

----

=== Add Development Users

In the real world, your developers are likely to use the OpenShift Enterprise tools (`oc` and the web console) on their own machines . For this course, you create user accounts for two nonprivileged OpenShift Enterprise users, `andrew` and `marina`, on the master. You do this both for convenience and because you are using `htpasswd` for authentication.

. On the master host, add two Linux accounts.
+
----

[root@master00-GUID ~]# useradd andrew
[root@master00-GUID ~]# useradd marina

----

=== Configure `htpasswd` Authentication

OpenShift Enterprise 3 supports a number of authentication mechanisms. The simplest use case for testing purposes is authentication based on `htpasswd`.

To start, you need the `htpasswd` binary available in the `httpd-tools` package.

. Install `httpd-tools` on the master host.
+
----

[root@master00-GUID ~]# yum -y install httpd-tools

----

. Create a password for users `andrew` and `marina` on the master host.
+
----

[root@master00-GUID ~]# touch /etc/openshift/openshift-passwd
[root@master00-GUID ~]# htpasswd -b /etc/openshift/openshift-passwd andrew r3dh4t1!
[root@master00-GUID ~]# htpasswd -b /etc/openshift/openshift-passwd marina r3dh4t1!

----

. Restart `openshift-master` for the changes to take effect.
+
----
[root@master00-GUID ~]# systemctl restart openshift-master
----



== Set Up Persistent Storage

Having a database for development is nice, but what if you actually want the data you store to persist after you redeploy the database pod? Pods are ephemeral, and, by default, so is their storage. For shared or persistent storage, you need a way to specify that pods should use external volumes.

For this training, you use the `oselab` host to export an NFS volume for use as storage by the database.

=== Prepare for NFS Persistent Storage

. As `root` on the master host, ensure that `nfs-utils` is installed on _all_ nodes.
+
----
[root@master00-GUID ~]# ]# for node in infranode00-$GUID.oslab.opentlc.com node00-$GUID.oslab.opentlc.com node01-$GUID.oslab.opentlc.com; do ssh $node "yum -y install nfs-utils" ; done
----

=== Export an NFS Volume for Persistent Storage

On the `oselab` admin host, create a directory for each volume that you wish to export via NFS.

. Create 100 directory exports to use as persistent volumes.
+
----
[root@oselab-GUID ~]# mkdir -p /var/export/pvs/pv{1..100}
[root@oselab-GUID ~]# chown -R nfsnobody:nfsnobody /var/export/pvs/
[root@oselab-GUID ~]# chmod -R 700 /var/export/pvs/

----

. Add a line for each export directory to `/etc/exports`:
+
----

[root@oselab-GUID ~]# for volume in pv{1..100} ; do
echo Creating export for volume $volume
echo "/var/export/pvs/${volume} 192.168.0.0/24(rw,sync,all_squash)" >> /etc/exports;
done;

----

. Enable and start NFS services.
+
----

[root@oselab-GUID ~]# systemctl enable rpcbind nfs-server
[root@oselab-GUID ~]# systemctl start rpcbind nfs-server nfs-lock nfs-idmap
[root@oselab-GUID ~]# systemctl stop firewalld
[root@oselab-GUID ~]# systemctl disable firewalld

----
+
Note that the volume is owned by `nfsnobody`, and access by all remote users is "squashed" (using the `all_squash` command) to be access by this user. This essentially disables user permissions for clients mounting the volume. While another configuration might be preferable, one problem you may run into is that the container cannot modify the permissions of the actual volume directory when mounted. In the case of MySQL below, MySQL wants the volume to belong to the `mysql` user and assumes that it is, which causes problems later. Arguably, the container should operate differently. In the long run, Red Hat may work to come up with best practices for use of NFS from containers.


=== Allow NFS Access in SELinux Policy

By policy default, containers are not allowed to write to NFS mounted directories. You want to allow this for some of your pods.

. To allow containers to write to NFS mounted directories on all nodes where the pod could land (i.e., all of them):
+
----

[root@master00-GUID ~]# for node in infranode00-$GUID.oslab.opentlc.com node00-$GUID.oslab.opentlc.com node01-$GUID.oslab.opentlc.com; do setsebool -P virt_use_nfs=true ; done

----
+
[NOTE]
Once the Ansible-based installer performs this task automatically, this step will be removed from the lab.

=== Create Definition Files for Your Volumes

. Create a directory to store definition files for persistent volumes (`pvs`) in your environment.
+
----
[root@master00-GUID ~]# mkdir /root/pvs
----
. Create 25 `PersistentVolumes` (`pv1` to `pv25`) with the size of 5 gigabytes.
+
----

[root@master00-GUID ~]# export volsize="5Gi"
[root@master00-GUID ~]# for volume in pv{1..25} ; do
cat << EOF > /root/pvs/${volume}
{
  "apiVersion": "v1",
  "kind": "PersistentVolume",
  "metadata": {
    "name": "${volume}"
  },
  "spec": {
    "capacity": {
        "storage": "${volsize}"
    },
    "accessModes": [ "ReadWriteOnce" ],
    "nfs": {
        "path": "/var/export/pvs/${volume}",
        "server": "192.168.0.254"
    },
    "persistentVolumeReclaimPolicy": "Recycle"
  }
}
EOF
echo "Created def file for ${volume}";
done;
----

. Create 25 additional `PersistentVolumes` (`pv26` to `pv50`) with the size of 10 gigabytes.
+
----

[root@master00-GUID ~]# export volsize="10Gi"
[root@master00-GUID ~]# for volume in pv{26..50} ; do
cat << EOF > /root/pvs/${volume}
{
  "apiVersion": "v1",
  "kind": "PersistentVolume",
  "metadata": {
    "name": "${volume}"
  },
  "spec": {
    "capacity": {
        "storage": "${volsize}"
    },
    "accessModes": [ "ReadWriteOnce" ],
    "nfs": {
        "path": "/var/export/pvs/${volume}",
        "server": "192.168.0.254"
    },
    "persistentVolumeReclaimPolicy": "Recycle"
  }
}
EOF
echo "Created def file for ${volume}";
done;
----

. Create 50 `PersistentVolumes` (`pv51` to `pv100`) with the size of 1 gigabyte.
+
----

[root@master00-GUID ~]# export volsize="1Gi"
[root@master00-GUID ~]# for volume in pv{51..100} ; do
cat << EOF > /root/pvs/${volume}
{
  "apiVersion": "v1",
  "kind": "PersistentVolume",
  "metadata": {
    "name": "${volume}"
  },
  "spec": {
    "capacity": {
        "storage": "${volsize}"
    },
    "accessModes": [ "ReadWriteOnce" ],
    "nfs": {
        "path": "/var/export/pvs/${volume}",
        "server": "192.168.0.254"
    },
    "persistentVolumeReclaimPolicy": "Recycle"
  }
}
EOF
echo "Created def file for ${volume}";
done;
----

. Allocate three volumes, 5 gigabytes each, to the `default` project.
+
----
[root@master00-GUID ~]# cd /root/pvs
[root@master00-GUID ~]# cat pv21 pv22 pv23 | oc create -f - -n default
----

. To see that your `pvs` were added and are available, run `oc get pvs`.
+
----
[root@master00-GUID pvs]# oc get pv
NAME               LABELS    CAPACITY      ACCESSMODES   STATUS      CLAIM                    REASON
pv21               <none>    5368709120    RWO           Available
pv22               <none>    5368709120    RWO           Available
pv23               <none>    5368709120    RWO           Available
----

[NOTE] 
Although this process is fairly manual now, one could easily automate this process to create a volume on request.

=== Verify NFS Access

. Make sure you can connect to each of your nodes, including `infranode`.
+
----
[root@node0X-GUID ~]# mkdir /tmp/test
[root@node0X-GUID ~]# mount -v 192.168.0.254:/var/export/pvs/pv98 /tmp/test
----

[NOTE]
At this point, you have created the infrastructure for using persistent volumes but have not used it. You will use these exports (volumes) in upcoming labs.


=== Open NFS Firewall Ports (Reference Only)

[IMPORTANT]
In your lab environment, the firewall is disabled on the `oselab` host, so you do not need to do these steps. They are shown here for information only.

In an actual development environment, you need to open ports on the firewall on the master to enable NFS to communicate from the nodes. 

. Add rules for NFS to the running state of the firewall:
+
----
    iptables -I OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 111 -j ACCEPT
    iptables -I OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2049 -j ACCEPT
    iptables -I OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 20048 -j ACCEPT
    iptables -I OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 50825 -j ACCEPT
    iptables -I OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 53248 -j ACCEPT
----

. Add the rules to `/etc/sysconfig/iptables`. 
+
[NOTE]
Put them at the top of the `OS_FIREWALL_ALLOW` set:
+
----
    -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 53248 -j ACCEPT
    -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 50825 -j ACCEPT
    -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 20048 -j ACCEPT
    -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2049 -j ACCEPT
    -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 111 -j ACCEPT
----

. Edit the NFS configuration to use these ports.
.. In `/etc/sysconfig/nfs`, change the `RPC` option to the following:
+
----
    RPCMOUNTDOPTS="-p 20048"
----

.. Next, change the `STATD` option to the following:
+
----
    STATDARG="-p 50825"
----

.. Edit `/etc/sysctl.conf` as follows: 
+
----
    fs.nfs.nlm_tcpport=53248
    fs.nfs.nlm_udpport=53248
----

.. Persist the `sysctl` changes:
+
----
    sysctl -p
----

.. Restart NFS:
+
----
    systemctl restart nfs
----
