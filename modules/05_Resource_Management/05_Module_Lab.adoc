:toc2:
:icons: images/icons

== Resource Management Lab

=== Overview

In this lab you learn how to manage OpenShift Container Platform resources. The lab contains the following four sections.

.Manage Users, Projects, and Quotas

In this section you create projects and test the use of quotas and limits.

.Create Services and Routes

In this section you manually create services and routes for pods and review the changes to a service when an application is scaled.

.Explore Containers

In this section you run commands within active pods and explore the `docker-registry` and `Default Router` containers.

.Create Persistent Volume for Registry

In this section you create a persistent volume for your registry, attach it to `deploymentConfiguration`, and redeploy the registry.


:numbered:

== Manage Users, Projects, and Quotas

=== Create Project

. Connect to the `master1` host:
+
----
[root@bastion ~]# ssh master1.example.com
----

. Run `oadm` to create a project named `resourcemanagement` and assign the administrative user `andrew` to it:
+
----
[root@master1 ~]# oadm new-project resourcemanagement --display-name="Resources Management" \
    --description="This is the project we use to learn about resource management" \
    --admin=andrew  --node-selector='region=primary'
----


* `andrew` can create his own project with the `oc new-project` command--an option you experiment with later in this course.
* Defining `--node-selector` is optional. You already defined the default node selector in the previous lab.


=== View Resources in Web Console

The web console has been completely redesigned for version 3.

. Open your web browser and go to `https://master1-GUID.oslab.opentlc.com:8443`.
+
[NOTE]
====
The web console can take up to 90 seconds to become available after a restart of the master.

The first time you access the URL, you may need to accept the self-signed SSL certificate.
====

. When prompted, type the following username and password:
* *Username*: `andrew`
* *Password*: `r3dh4t1!`

. Click the *Resources Management* project.

* The project is empty because it has no data. 
* You enter data in this lab.

=== Apply Quota to Project

. On the master host, create a quota definition file:
+
----

[root@master1 ~]#  cat << EOF > quota.json
{
  "apiVersion": "v1",
  "kind": "ResourceQuota",
  "metadata": {
    "name": "test-quota"
  },
  "spec": {
    "hard": {
      "memory": "512Mi",
      "cpu": "20",
      "pods": "3",
      "services": "5",
      "replicationcontrollers":"5",
      "resourcequotas":"1"
    }
  }
}
EOF
----

. Run `oc create` to apply the file you just created:
+
----
[root@master1 ~]# oc create -f quota.json --namespace=resourcemanagement
----

. Verify that the quota exists:
+
----
[root@master1 ~]# oc get -n resourcemanagement quota
----
* Expect to see output that looks like this:
+
----
NAME         AGE
test-quota   8s
----

. Verify the limits and examine the usage:
+
----
[root@master1 ~]# oc describe -n resourcemanagement quota test-quota
----
* The output looks like this:
+
----
Name:			test-quota
Namespace:		resourcemanagement
Resource		Used	Hard
--------		----	----
cpu			0	20
memory			0	512Mi
pods			0	3
replicationcontrollers	0	5
resourcequotas		1	1
services		0	5
----


. On the web console, click the *Resource Management* project.

. Click the *Resources*/*Quota* tab for information on the quota.

=== Apply Limit Ranges to Project

For quotas to be effective, you must create _limit ranges_. Limit ranges allocate the maximum, minimum, and default memory and CPU at both the pod level and the container level. Without defaults for containers, projects with quotas fail because the deployer and other infrastructure pods are unbounded and therefore forbidden.

. On the master host, create the `limits.json` file:
+
----
[root@master1 ~]# cat << EOF > limits.json
{
    "kind": "LimitRange",
    "apiVersion": "v1",
    "metadata": {
        "name": "limits",
        "creationTimestamp": null
    },
    "spec": {
        "limits": [
            {
                "type": "Pod",
                "max": {
                    "cpu": "500m",
                    "memory": "750Mi"
                },
                "min": {
                    "cpu": "10m",
                    "memory": "5Mi"
                }
            },
            {
                "type": "Container",
                "max": {
                    "cpu": "500m",
                    "memory": "750Mi"
                },
                "min": {
                    "cpu": "10m",
                    "memory": "5Mi"
                },
                "default": {
                    "cpu": "100m",
                    "memory": "100Mi"
                }
            }
        ]
    }
}
EOF
----

. Run `oc create` against the `limits.json` file and the `resourcemanagement` project:
+
----
[root@master1 ~]# oc create -f limits.json --namespace=resourcemanagement
----

. Review the limit ranges:
+
----
[root@master1 ~]# oc describe limitranges limits -n resourcemanagement
----
* Expect to see output similar to this:
+
----
Name:		limits
Namespace:	resourcemanagement
Type		Resource	Min	Max	Request	Limit	Limit/Request
----		--------	---	---	-------	-----	-------------
Pod		memory		5Mi	750Mi	-	-	-
Pod		cpu		10m	500m	-	-	-
Container	memory		5Mi	750Mi	100Mi	100Mi	-
Container	cpu		10m	500m	100m	100m	-
----

=== Test Quotas

This exercise shows you the manual, step-by-step method of creating each object. There are easier ways to create a deployment and its components. One of those ways is with the `oc new-app` command, which is covered later in this lab.

NOTE: You are running commands as the Linux users `andrew` and `root` in a lab environment. In a real-world scenario, users would issue `oc` commands from their workstations and not from the OpenShift master.

. Authenticate to OpenShift Container Platform and choose your project:

.. Connect to the master using the procedure you followed previously.

.. When prompted, enter `andrew` as the username and `r3dh4t1!` as the password:
+
----
[root@master1 ~]# su - andrew
[andrew@master1 ~]$ oc login -u andrew --insecure-skip-tls-verify --server=https://master1.example.com:8443
----

* Expect output similar to this:
+
----
Login successful.

Using project "resourcemanagement".
Welcome! See 'oc help' to get started.
----

. Create the `hello-pod.json` pod definition file:
+
----

[andrew@master1 ~]$ cat <<EOF > hello-pod.json
{
  "kind": "Pod",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-openshift",
    "creationTimestamp": null,
    "labels": {
      "name": "hello-openshift"
    }
  },
  "spec": {
    "containers": [
      {
        "name": "hello-openshift",
        "image": "openshift/hello-openshift:v1.2.1",
        "ports": [
          {
            "containerPort": 8080,
            "protocol": "TCP"
          }
        ],
        "resources": {
        },
        "terminationMessagePath": "/dev/termination-log",
        "imagePullPolicy": "IfNotPresent",
        "capabilities": {},
        "securityContext": {
          "capabilities": {},
          "privileged": false
        }
      }
    ],
    "restartPolicy": "Always",
    "dnsPolicy": "ClusterFirst",
    "serviceAccount": ""
  },
  "status": {}
}

EOF

----

=== Run Pod

In this exercise you create a simple pod without a route or service.

. Create the `hello-openshift` pod:
+
----
[andrew@master1 ~]$ oc create -f hello-pod.json
----
* The output looks like this:
+
----
pod "hello-openshift" created
----

. Verify that the pod exists:
+
----
[andrew@master1 ~]$ oc get pods
----
* Expect the output to look like this:
+
----
NAME              READY     STATUS              RESTARTS   AGE
hello-openshift   0/1       ContainerCreating   0          6s
----
* and then, when container creation is finished:
+
----
NAME              READY     STATUS    RESTARTS   AGE
hello-openshift   1/1       Running   0          8s
----

. Run `oc describe` to get pod details:
+
----
[andrew@master1 ~]$ oc describe pod hello-openshift
Name:                   hello-openshift
Namespace:              resourcemanagement
Security Policy:        restricted
Node:                   node2.example.com/192.168.0.202
Start Time:             Tue, 30 May 2017 10:20:24 -0400
Labels:                 name=hello-openshift
Status:                 Running
IP:                     10.128.0.3
Controllers:            <none>
Containers:
  hello-openshift:
    Container ID:       docker://2381b0e13a04eb7859d3d286b9515374b09fd98ec99b9531f568952cf690937b
    Image:              openshift/hello-openshift:v1.2.1
    Image ID:           docker-pullable://docker.io/openshift/hello-openshift@sha256:e44d78ac0b70255d8a84a7707c2001d115d4db9354591b4098a718c8ccb693f7
    Port:               8080/TCP
    Limits:
      cpu:      100m
      memory:   100Mi
    Requests:
      cpu:              100m
      memory:           100Mi
    State:              Running
      Started:          Tue, 30 May 2017 10:20:31 -0400
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-j3bbr (ro)
    Environment Variables:      <none>
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-j3bbr:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-j3bbr
QoS Class:      Guaranteed
Tolerations:    <none>
Events:
  FirstSeen     LastSeen        Count   From                            SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                            -------------                           --------        ------          -------
  9m            9m              1       {default-scheduler }                                                    Normal          Scheduled       Successfully assigned hello-openshift to node2.example.com
  9m            9m              1       {kubelet node2.example.com}     spec.containers{hello-openshift}        Normal          Pulled          Container image "openshift/hello-openshift:v1.2.1" already present on machine
  9m            9m              1       {kubelet node2.example.com}     spec.containers{hello-openshift}        Normal          Created         Created container with docker id 2381b0e13a04; Security:[seccomp=unconfined]
  9m            9m              1       {kubelet node2.example.com}     spec.containers{hello-openshift}        Normal          Started         Started container with docker id 2381b0e13a04
----
+
. Test that your pod responds with the following command:
+
----
[andrew@master1 ~]$ ip=`oc describe pod hello-openshift|grep IP:|awk '{print $2}'`
[andrew@master1 ~]$ curl http://${ip}:8080
----

* This output indicates a correct response:
+
----
Hello OpenShift!
----

. Delete all the objects in the `hello-pod.json` definition file, which, at this point, is only the pod:
+
----
[andrew@master1 ~]$ oc delete -f hello-pod.json
----
+
TIP: You can also delete a pod using the format +oc delete pod _podname_+.

. Create a new definition file that launches four `hello-openshift` pods:
+
----
[andrew@master1 ~]$  cat << EOF > hello-many-pods.json
{
  "metadata":{
    "name":"quota-pod-deployment-test"
  },
  "kind":"List",
  "apiVersion":"v1",
  "items":[
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-1",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.2.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-2",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.2.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-3",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.2.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-4",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.2.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    }
  ]
}
EOF

----

. Create the items in the `hello-many-pods.json` file:
+
----
[andrew@master1 ~]$ oc create -f hello-many-pods.json
----
* The output looks like this:
+
----
pod "hello-openshift-1" created
pod "hello-openshift-2" created
pod "hello-openshift-3" created
Error from server (Forbidden): pods "hello-openshift-4" is forbidden: exceeded quota: test-quota, requested: pods=1, used: pods=3, limited: pods=3
----
* Because you defined the quota earlier with a pod value of `3`, the `oc create` command created only three pods instead of four.

. Delete the objects in the `hello-many-pods.json` definition file (the three pods):
+
----
[andrew@master1 ~]$ oc delete  -f hello-many-pods.json
----
* The output looks like this:
+
----
pod "hello-openshift-1" deleted
pod "hello-openshift-2" deleted
pod "hello-openshift-3" deleted
Error from server (NotFound): pods "hello-openshift-4" not found
----
* Because `hello-openshift-4` was not created, the last deletion returns an error and can be ignored.

. (Optional) Create a project, set the quota with a pod value of `10`, and run `hello-many-pods.json`.

== Create Services and Routes

. As `andrew`, create a project called `scvslab`:
+
----

[andrew@master1 ~]$ oc new-project svcslab --display-name="Services Lab" \
    --description="This is the project we use to learn about services"
----

* The output looks like this:
+
----
Now using project "svcslab" on server "https://master1.example.com:8443".
----
+
TIP: To switch between projects, run `oc project <projectname>`.

. Create the `hello-service.json` file:
+
----

[andrew@master1 ~]$  cat <<EOF > hello-service.json
{
  "kind": "Service",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-service",
    "labels": {
      "name": "hello-openshift"
    }
  },
  "spec": {
    "selector": {
      "name":"hello-openshift"
    },
    "ports": [
      {
        "protocol": "TCP",
        "port": 8888,
        "targetPort": 8080
      }
    ]
  }
}
EOF

----

. Create the `hello-service` service:
+
----
[andrew@master1 ~]$ oc create -f hello-service.json
----
* The output looks like this:
+
----
service "hello-service" created
----
+
. Display the services that are running in the current project:
+
----
[andrew@master1 ~]$ oc get services
----
* Expect the output to look like this:
+
----
NAME            CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
hello-service   172.30.xxx.yyy   <none>        8888/TCP   11s
----
+
. Examine the details of the service:
+
----

[andrew@master1 ~]$ oc describe service hello-service

----
* The output looks similar like this:
+
----
Name:			hello-service
Namespace:		svcslab
Labels:			name=hello-openshift
Selector:		name=hello-openshift
Type:			ClusterIP
IP:			172.30.231.196
Port:			<unnamed>	8888/TCP
Endpoints:		<none>
Session Affinity:	None
No events.
----
* Note the following:
** `Selector` describes which pods the service selects or lists.
** `oc describe service` or `oc get services -o wide` will show the `SELECTOR` column
** `Endpoints` displays all of the pods that are currently listed (none in the current project).

. Create pods according to the `hello-many-pods.json` definition file:
+
----
[andrew@master1 ~]$ oc create -f hello-many-pods.json
----
* The output looks like this:
+
----
pod "hello-openshift-1" created
pod "hello-openshift-2" created
pod "hello-openshift-3" created
pod "hello-openshift-4" created
----

. Wait a few seconds, and check the service again to verify that the pods that have the label `hello-openshift` are listed:
+
----
[andrew@master1 ~]$ oc describe service hello-service
----
* Expect output similar to this:
+
----
Name:			hello-service
Namespace:		svcslab
Labels:			name=hello-openshift
Selector:		name=hello-openshift
Type:			ClusterIP
IP:			172.30.231.196
Port:			<unnamed>	8888/TCP
Endpoints:		10.1.1.2:8080,10.1.1.3:8080,10.1.2.5:8080 + 1 more...
Session Affinity:	None
No events.
----

. Test that your service is working:
+
----
[andrew@master1 ~]$ ip=$(oc get service hello-service --template "{{ .spec.clusterIP }}")
[andrew@master1 ~]$ curl http://${ip}:8888
----
* Output like this indicates success:
+
----
Hello OpenShift!
----

. Create a new route to expose your application:
+
----
[andrew@master1 ~]$ cat <<EOF > hello-route.yml
---
apiVersion: v1
kind: Route
metadata:
  name: hello-service
spec:
  host: hello2-openshift.cloudapps-${guid}.oslab.opentlc.com
  to:
    kind: Service
    name: hello-service
EOF
[andrew@master1 ~]$ oc create -f hello-route.yml
----

. View the route:
+
----
[andrew@master1 ~]$ oc get routes
----
* Expect output similar to this:
+
----
NAME            HOST/PORT                                           PATH      SERVICES        PORT      TERMINATION   WILDCARD
hello-service   hello2-openshift.cloudapps-9e91.oslab.opentlc.com             hello-service   8888                    None
----

. Test the route:
+
----
[andrew@master1 ~]$ curl http://hello2-openshift.cloudapps-${guid}.oslab.opentlc.com
----
* Output like this indicates success:
+
----
Hello OpenShift!
----

== Explore Containers

Next, take a look at the route and registry containers.

=== Explore Route Container

==== Create Applications as Examples

. As `andrew`, create a project called `explore-example`:
+
----
[andrew@master1 ~]$ oc new-project explore-example --display-name="Explore Example" \
    --description="This is the project we use to learn about connecting to pods"
----

. Applying the same image as before, run `oc new-app` to deploy `hello-openshift`:
+
----
[andrew@master1 ~]$ oc new-app --docker-image=openshift/hello-openshift:v1.2.1 -l "todelete=yes"
----
* The output looks like this:
+
----
--> Found Docker image 7ce9d7b (10 weeks old) from Docker Hub for "openshift/hello-openshift:v1.2.1"
    * An image stream will be created as "hello-openshift:v1.2.1" that will track this image
    * This image will be deployed in deployment config "hello-openshift"
    * Ports 8080/tcp, 8888/tcp will be load balanced by service "hello-openshift"
--> Creating resources with label todelete=yes ...
    ImageStream "hello-openshift" created
    DeploymentConfig "hello-openshift" created
    Service "hello-openshift" created
--> Success
    Run 'oc status' to view your app.
----

. Verify that `oc new-app` created the service and a pod:
.. Verify the service:
+
----
[andrew@master1 ~]$ oc get service
----
+
----
NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
hello-openshift 172.30.211.235 8080/TCP,8888/TCP 6m
----

.. Verify the pod:
+
----
[andrew@master1 ~]$ oc get pods
----
+
----
NAME                      READY     STATUS    RESTARTS   AGE
hello-openshift-1-g3xow   1/1       Running   0          2m
----

. Expose the service and create a route for the application:
+
----
[andrew@master1 ~]$ oc expose service hello-openshift --hostname=explore.cloudapps-${guid}.oslab.opentlc.com
----

. In a later section, you explore the `docker-registry` container. To save time, start an S2I build now to push an image into the registry:
+
----
[andrew@master1 ~]$ oc new-app https://github.com/openshift/sinatra-example -l "todelete=yes"
----

==== Connect to Default Router Container

As `root`, you can use the `oc rsh` command along with the default router's pod name to execute the `bash` shell inside the router. There are two ways to do this.

.Option 1

. Use two separate steps:
.. Use `oc get pods` to find the default router's pod name:
+
----
[root@master1 ~]# oc get pods

NAME                      READY     REASON    RESTARTS   AGE
docker-registry-2-snarn   1/1       Running   0          17h
trainingrouter-1-jm5zk    1/1       Running   0          18h
----
.. Then use `oc rsh` with the specific pod's name to execute the `bash` shell:
+
----
[root@master1 ~]# oc rsh trainingrouter-1-jm5zk bash
/usr/bin/id: cannot find name for user ID 1000000000
[I have no name!@infranode1 conf]$ 
----

.Option 2
 
. Combine the `oc get pods` and `oc rsh` in a single command:
+
----
[root@master1 ~]#  oc rsh $(oc get pods --selector="router=trainingrouter" --template='{{range .items}}{{.metadata.name}}{{end}}') bash
/usr/bin/id: cannot find name for user ID 1000000000
[I have no name!@infranode1 conf]$ 
----


* The same prompt is displayed with each option: `[I have no name!@infranode1 conf]$`.

* You are now running `bash` inside the container. 

* The prompt shows that you are on the `infranode1` host because the router container resolved the host name through the host's IP address.


. Run `id`.
. Run `pwd` and `ls`, and note the directory you are in.
. Run `cat haproxy.config` to verify that your configuration file is not empty, and then view the process status.
+
----
[I have no name!@infranode1 conf]$ id
uid=1000000000 gid=0(root) groups=0(root),1000000000
----
+
----
[I have no name!@infranode1 conf]$ pwd
/var/lib/haproxy/conf
----
+
----
[I have no name!@infranode1 conf]$ ls 
cert_config.map          os_edge_http_be.map         os_sni_passthrough.map
default_pub_keys.pem     os_http_be.map              os_tcp_be.map
error-page-503.http      os_reencrypt.map            os_wildcard_domain.map
haproxy.config           os_route_http_expose.map
haproxy-config.template  os_route_http_redirect.map
----
+
----
[I have no name!@infranode1 conf]$ ps -ef
UID         PID   PPID  C STIME TTY          TIME CMD
root          1      0  0 02:07 ?        00:00:14 /usr/bin/openshift-router
root        243      0  0 22:08 ?        00:00:00 /bin/bash
root        319      1  0 22:11 ?        00:00:00 /usr/sbin/haproxy -f /var/lib/
root        342    243  0 22:16 ?        00:00:00 ps -ef
----
+
----
[I have no name!@infranode1 conf]$ cat haproxy.config
----

. Note the following in the output below:

* The route is the one you created in the previous lab.
* The route points to the endpoints directly.
+
----
backend be_http_explore-example_hello-openshift

  mode http
  option redispatch
  option forwardfor
  balance leastconn
  timeout check 5000ms
  http-request set-header X-Forwarded-Host %[req.hdr(host)]
  http-request set-header X-Forwarded-Port %[dst_port]
  http-request set-header X-Forwarded-Proto https if { ssl_fc }

    cookie OPENSHIFT_explore-example_hello-openshift_SERVERID insert indirect nocache httponly
    http-request set-header X-Forwarded-Proto http

  http-request set-header Forwarded for=%[src],host=%[req.hdr(host)],proto=%[req.hdr(X-Forwarded-Proto)]

  server 10.1.1.7:8080 10.1.1.7:8080 check inter 5000ms cookie 10.1.1.7:8080

...
...
----

. As `andrew`, scale `hello-openshift` to have 5 replicas of its pod:
+
----
[andrew@master1 ~]$ oc get deploymentconfig # or oc get dc
NAME              TRIGGERS                    LATEST
hello-openshift   ConfigChange, ImageChange   1
----
+
----
[andrew@master1 ~]$ oc scale dc hello-openshift --replicas=5
deploymentconfig "hello-openshift" scaled
----

. Switch back to root, go back to the router container and view the `haproxy.config` file again:
+
----
[andrew@master1 ~]$ exit
[root@master1 ~]#  oc rsh $(oc get pods --selector="router=trainingrouter" --template='{{range .items}}{{.metadata.name}}{{end}}') bash
/usr/bin/id: cannot find name for user ID 1000000000
[I have no name!@infranode1 conf]$ sed '/^ *$/d' haproxy.config | grep -A 20 backend.*explore-example_hello-openshift
----
* All of your pods within the `haproxy` configuration are listed:
+
----
backend be_http_explore-example_hello-openshift
  mode http
  option redispatch
  option forwardfor
  balance leastconn
  timeout check 5000ms
  http-request set-header X-Forwarded-Host %[req.hdr(host)]
  http-request set-header X-Forwarded-Port %[dst_port]
  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }
  http-request set-header X-Forwarded-Proto https if { ssl_fc }
  cookie 7cf54b74789cba0ee0faded0db7f5e0f insert indirect nocache httponly
  http-request set-header Forwarded for=%[src];host=%[req.hdr(host)];proto=%[req.hdr(X-Forwarded-Proto)]
  server 25b5ea63c3e6a44f7ef8b3ecf0fbc106 10.129.0.12:8080 check inter 5000ms cookie 25b5ea63c3e6a44f7ef8b3ecf0fbc106 weight 100
  server a96e38a5ff729ac00c03f54bb130bf7b 10.129.0.9:8080 check inter 5000ms cookie a96e38a5ff729ac00c03f54bb130bf7b weight 100
  server 1a0c4763292ccf975ee887c01ae07ece 10.130.0.15:8080 check inter 5000ms cookie 1a0c4763292ccf975ee887c01ae07ece weight 100
  server ae3943fea0f5bf7953d2309c09f760fa 10.130.0.16:8080 check inter 5000ms cookie ae3943fea0f5bf7953d2309c09f760fa weight 100
  server a5f87fb137db69b26a7471f6896c7fa4 10.130.0.17:8080 check inter 5000ms cookie a5f87fb137db69b26a7471f6896c7fa4 weight 100
----


* The router routes proxy connections to the pods directly and not through the service. 
* The router uses the service only to obtain a list of the pod endpoints (IP addresses).

=== Explore Registry Container

Verify that your build from earlier is complete.

. As user `andrew`, run the following to see the build:
+
----
[andrew@master1 ~]$ oc logs builds/sinatra-example-1
----
* Look for output similar to the following:
+
----
...
...
...
I1120 02:16:05.875303       1 sti.go:298] Successfully built 172.30.41.32:5000/svcslab/sinatra-example:latest
I1120 02:16:06.512944       1 cleanup.go:23] Removing temporary directory /tmp/s2i-build079968192
I1120 02:16:06.513477       1 fs.go:99] Removing directory '/tmp/s2i-build079968192'
I1120 02:16:06.546932       1 sti.go:213] Using provided push secret for pushing 172.30.41.32:5000/svcslab/sinatra-example:latest image
I1120 02:16:06.547064       1 sti.go:217] Pushing 172.30.41.32:5000/svcslab/sinatra-example:latest image ...
I1120 02:19:58.237018       1 sti.go:233] Successfully pushed 172.30.41.32:5000/svcslab/sinatra-example:latest
----
+
[NOTE]
This step takes a while on the lab environment's hardware. If the build is not yet complete, feel free to take a quick break here.

. As `root`, execute the `bash` shell inside the registry container by running `oc rsh` with the `docker-registry` pod's name:
+
----
[root@master1 ~]#  oc rsh $(oc get pods --selector="deploymentconfig=docker-registry" --template='{{range .items}}{{.metadata.name}}{{end}}') bash
----


. Run `id`.
. Run `pwd` and `ls` and note the directory you are in.
. Run `cat config.yml` to verify that your configuration file is empty.
+
----
bash-4.2$ id
uid=1000000000 gid=0(root) groups=0(root)
----
+
----
bash-4.2$ pwd
/
----
+
----
bash-4.2$ ls
bin   config.yml  etc	lib    media  opt   registry  run   srv  tmp  var
boot  dev	  home	lib64  mnt    proc  root      sbin  sys  usr
----
+
----
bash-4.2$ cat config.yml
version: 0.1
log:
  level: debug
http:
  addr: :5000
storage:
  cache:
    layerinfo: inmemory
  filesystem:
    rootdirectory: /registry
auth:
  openshift:
    realm: openshift
middleware:
  repository:
    - name: openshift
bash-4.2$
----

. View the repositories and images that are available:
+
----
bash-4.2$  cd /registry/docker/registry/v2/repositories
bash-4.2$ ls
explore-example
----
+
----
bash-4.2$ ls explore-example/sinatra-example/_layers/
sha256
----
+
----
bash-4.2$ ls explore-example/sinatra-example/_layers/sha256/
50c4d0284685934ca2920fd6e056318cac1187773e8a239dd02d8f248a59d382
50de3644a809b46b344074ca0a691524eb06af3af6a07d25e90c25b50a00980f
9320560b540438b82b1bb1a51d035490812ad9298b945c041da3d0a4b646abf6
e1e04a46f510bf9b3fb68e6cf3fc027100cec875a7ff02e6d0da5206fa7f6b8c
----

. As user `andrew`, look at one of the pods you started earlier:
+
----
[andrew@master1 ~]$ oc get pods
----
* Expect output similar to this:
+
----
NAME                      READY     STATUS      RESTARTS   AGE
hello-openshift-1-1ecah   1/1       Running     0          27m
hello-openshift-1-b8o3d   1/1       Running     0          27m
hello-openshift-1-g3xow   1/1       Running     0          45m
hello-openshift-1-rbfri   1/1       Running     0          27m
hello-openshift-1-yxidw   1/1       Running     0          27m
sinatra-example-1-build   0/1       Completed   0          11m
sinatra-example-1-yxyod   1/1       Running     0          8m
----

. Connect to the pod:
+
----
[andrew@master1 ~]$ oc rsh sinatra-example-1-yxyod bash
bash-4.2$
----

. Explore the pod:
.. Run `id`.
.. Run `pwd` and `ls` and note the directory you are in.
.. Run `ps -ef` to see what processes are running.
+
----
bash-4.2$ id
uid=1000050000 gid=0(root) groups=0(root)
----
+
----
bash-4.2$ pwd
/opt/app-root/src
----
+
----
bash-4.2$ ls
Gemfile       README.md  config.ru	  example-mustache	 public
Gemfile.lock  app.rb	 example-model	  example-views		 tmp
README	      bundle	 example-modular  example-views-modular
----
+
----
bash-4.2$ ps -ef
UID         PID   PPID  C STIME TTY          TIME CMD
1000050+      1      0  0 22:41 ?        00:00:01 ruby /opt/app-root/src/bundle/
1000050+     33      0  0 22:51 ?        00:00:00 /bin/bash
1000050+     62     33  0 22:51 ?        00:00:00 ps -ef
----
* Expect different pod names and output from the example shown here.


== Recreate Persistent Volume for Registry

In this exercise you create an NFS export for the registry and attach the persistent volume to the registry.

The registry is currently running with a NFS persistent volume. This volume was set by the following Ansible configuration:
----
openshift_hosted_registry_storage_kind=nfs
openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
openshift_hosted_registry_storage_host=bastion.example.com
openshift_hosted_registry_storage_nfs_directory=/exports
openshift_hosted_registry_storage_volume_name=registry
openshift_hosted_registry_storage_volume_size=5Gi
----

WARNING: Here, the NFS Persistent Volume (PV) is defined with a size of 5GiB in OpenShift. This is only used to map PV and PV Claims (PVC) and it is arbitrary here. It is not the actual size of the export. If you want to enforce disk quotas with NFS you need to use partitions with limited size and create in OpenShift one PV per partition. See https://docs.openshift.com/container-platform/3.5/install_config/persistent_storage/persistent_storage_nfs.html#nfs-enforcing-disk-quotas.

Exports are automatically created by the ansible playbook in `/etc/exports.d/openshift-ansible.exports` :
----
[root@bastion ~]# cat /etc/exports.d/openshift-ansible.exports 
/exports/registry *(rw,root_squash)
/exports/metrics *(rw,root_squash)
/exports/logging-es *(rw,root_squash)
/exports/logging-es-ops *(rw,root_squash)
----


=== Create NFS Export for Registry
On the bastion there should already be a configured Volume Group and Logical Volume which we can use to create the NFS export: `nfs-lv` mounted to `/srv`.

. List devices:
+
----
[root@bastion ~]# lsblk
NAME                              MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
fd0                                 2:0    1    4K  0 disk 
sr0                                11:0    1 1024M  0 rom  
vda                               252:0    0   10G  0 disk 
└─vda1                            252:1    0   10G  0 part /
vdb                               252:16   0   20G  0 disk 
└─vdb1                            252:17   0   20G  0 part 
  ├─docker--vg-docker--pool_tmeta 253:1    0   24M  0 lvm  
  │ └─docker--vg-docker--pool     253:3    0 11.5G  0 lvm  
  └─docker--vg-docker--pool_tdata 253:2    0 11.5G  0 lvm  
    └─docker--vg-docker--pool     253:3    0 11.5G  0 lvm  
vdc                               252:32   0   25G  0 disk 
└─nfs--vg-nfs--lv                 253:0    0   25G  0 lvm  /srv
vdd                               252:48   0  370K  1 disk 
----

. [[NFSPermission]] Create a new directory for your new NFS export:
+
----
[root@bastion ~]# export volname=registry-storage
[root@bastion ~]# mkdir -p /srv/export/pvs/${volname}
[root@bastion ~]# chown nfsnobody:nfsnobody /srv/export/pvs/${volname}
[root@bastion ~]# chmod 770 /srv/export/pvs/${volname}
----

. Add this line to `/etc/exports`:
+
----
[root@bastion ~]# echo "/srv/export/pvs/${volname} *(rw,root_squash)" >> /etc/exports
----

. Do not forget to remove the previous export from NFS:
+
----
[root@bastion ~]# sed -i /registry/d /etc/exports.d/openshift-ansible.exports
----

. Make NFS aware of the changes, synchronize `/etc/exports` and `/etc/exports.d/*` with `/var/lib/nfs/etab`:
+
----
[root@bastion ~]# exportfs -r
----

=== Delete Existing Persistent Volume

. It is good practice to keep the ansible inventory file in sync whith the current state of its OpenShift cluster. Update the ansible inventory file to reflect the change. Here we are doing a manual step, so just comment all lines about registry-storage:
+
----
[root@bastion ~]# sed -e '/openshift_hosted_registry_storage_/ s/^#*/#/' -i /etc/ansible/hosts
----

. As `root` on `master1`, ensure you are in the `default` project:
+
----
[root@bastion ~]# ssh master1.example.com
[root@master1 ~]# oc project default
----


. delete the existing persistent volume and persistent volume claim for the registry:
+
----
[root@master1 ~]# oc delete pvc registry-claim ; oc delete pv registry-volume
----

* This output indicates success:
+
----
persistentvolumeclaim "registry-claim" deleted
persistentvolume "registry-volume" deleted
----

. Create a persistent volume definition file named `registry-volume.json`:
+
----
[root@master1 ~]# cat << EOF > registry-volume.json
    {
      "apiVersion": "v1",
      "kind": "PersistentVolume",
      "metadata": {
        "name": "registry-volume"
      },
      "spec": {
        "capacity": {
            "storage": "15Gi"
            },
        "accessModes": [ "ReadWriteMany" ],
        "nfs": {
            "path": "/srv/export/pvs/registry-storage",
            "server": "bastion.example.com"
        }
      }
    }
EOF
----

* You create the persistent volume in the `default` project because that is the project in which the registry runs.

. Create the `registry-volume` persistent volume from the definition file:
+
----
[root@master1 ~]# oc create -f registry-volume.json
----

* This output indicates success:
+
----
persistentvolume "registry-volume" created
----

. View the persistent volume you just created:
+
----
[root@master1 ~]# oc get pv
NAME               CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS      CLAIM     REASON    AGE
registry-volume   15Gi       RWX           Retain          Available                       20s
----

. Create a `registry-volume-claim.json` claim definition file to claim your volume:
+
----
[root@master1 ~]# cat << EOF > registry-volume-claim.json
    {
      "apiVersion": "v1",
      "kind": "PersistentVolumeClaim",
      "metadata": {
        "name": "registry-claim-new"
      },
      "spec": {
        "accessModes": [ "ReadWriteMany" ],
        "resources": {
          "requests": {
            "storage": "15Gi"
          }
        }
      }
    }
EOF
----

. Create the `registry-claim-new` claim from the definition file:
+
----
[root@master1 ~]# oc create -f registry-volume-claim.json
----

* This output indicates success:
+
----
persistentvolumeclaim "registry-claim-new" created
----

. View the persistent volume you created:
+
----
[root@master1 ~]# oc get pv
----
* Expect the output to look like this:
+
----
NAME               CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                    REASON    AGE
registry-volume   15Gi       RWX           Retain          Bound     default/registry-claim-new             1m
----
** Note that the status is `Bound`.

. View the persistent volume claim you created (whose status is also `Bound`):
+
----
[root@master1 ~]# oc get pvc
----
* The output looks similar to this:
+
----
NAME                 STATUS    VOLUME             CAPACITY   ACCESSMODES   AGE
registry-claim-new   Bound     registry-volume    15Gi       RWX           55s
----

=== Attach Persistent Volume to Registry

. Assuming that your registry is already running, obtain the names of the deployment configurations (`dc`):
+
----
[root@master1 ~]# oc get dc
----
* Expect output like this:
+
----
NAME               REVISION   DESIRED   CURRENT   TRIGGERED BY
docker-registry    1          1         1         config
registry-console   1          1         1         config
trainingrouter     1          1         1         config
----

. Run `oc volume` to modify the deployment configurations. We will use the claim we just created. Add the `registry-claim` PVC to the registry's deployment configuration, which redeploys the registry:
+
----
[root@master1 ~]# oc volume dc/docker-registry --add --overwrite -t persistentVolumeClaim \
--claim-name=registry-claim-new --name=registry-storage --mount-path=/registry
----

* This output indicates success:
+
----
deploymentconfig "docker-registry" updated
----

. Run `oc get pods` and look for results like this:
+
----
NAME                       READY     STATUS    RESTARTS   AGE
docker-registry-2-4gd7x    1/1       Running   0          27s
registry-console-1-vlnh3   1/1       Running   1          10h
trainingrouter-1-fnn00     1/1       Running   1          10h
----

. Check if new volume is correctly mounted inside registry pod:
+
----
[root@master1 ~]# oc rsh $(oc get pod|grep docker-registry|grep Running|awk '{print $1}') \
                  df -h|grep '/registry'
----

* Look for output like this:
+
----
bastion.example.com:/srv/export/pvs/registry-storage       25G  7.9G   16G  34% /registry
----

. Check that the volume is writable from the pod:
+
----
[root@master1 ~]# oc rsh $(oc get pod|grep docker-registry|grep Running|awk '{print $1}') \
                  touch /registry/OK
[root@master1 ~]# oc rsh $(oc get pod|grep docker-registry|grep Running|awk '{print $1}') \
                  ls /registry/
----
* Successful output:
+
----
OK
----
+
[NOTE]
====
During the setup of the NFS export in <<NFSPermissions,previous step>>, we set the permissions for the NFS export on `bastion` to mode `770` and user:group `nfsnobody:nfsnobody` (65534:65534). Because registry has group `0`, and because of `root_squash` option in NFS the registry can write to this NFS share. In other situations, you might need to add `supplementalGroups` to the deployment config. See https://docs.openshift.com/container-platform/3.5/install_config/persistent_storage/persistent_storage_nfs.html#nfs-volume-security. Here we could patch the current `dc/docker-registry` to add into supplemental groups:
----
# OPTIONAL
[root@master1 ~]# oc patch dc/docker-registry \
--type=json -p='[{ "op": "add",
"path": "/spec/template/spec/securityContext",
"value": { "supplementalGroups": [65534] } }]'

# OUTPUT
"docker-registry" patched
----
====

* When the first `docker-registry` container was deleted, all of the images stored in it (so in the previous PV) are not available anymore.
* Now that your registry contains a persistent volume, images are saved even if you delete or replace the `docker-registry` pod.

. As `andrew` on the `master1` host, start an application based on the `https://github.com/openshift/sti-php` repository that requires an S2I build:
+
----
[root@master1 ~]# su - andrew
[andrew@master1 ~]$ oc new-app openshift/php~https://github.com/openshift/sti-php -l "todelete=yes"
----
+
* Expect output similar to this:
+
----
--> Found image 355eabc (2 weeks old) in image stream "php in project openshift" under tag :latest for "openshift/php"
    * A source build using source code from https://github.com/openshift/sti-php will be created
      * The resulting image will be pushed to image stream "sti-php:latest"
    * This image will be deployed in deployment config "sti-php"
    * Port 8080/tcp will be load balanced by service "sti-php"
--> Creating resources with label todelete=yes ...
    ImageStream "sti-php" created
    BuildConfig "sti-php" created
    DeploymentConfig "sti-php" created
    Service "sti-php" created
--> Success
    Build scheduled for "sti-php" - use the logs command to track its progress.
    Run 'oc status' to view your app.
----

. Check the build logs to ensure that the build is complete and was pushed into
 the registry:
+
----
[andrew@master1 ~]$ oc logs -f builds/sti-php-1
----
* Expect log entries similar to these:
+
----
I1126 23:24:28.604316       1 sti.go:298] Successfully built 172.30.42.118:5000/default/sti-php:latest
I1126 23:24:28.716843       1 cleanup.go:23] Removing temporary directory /tmp/s2i-build491090638
I1126 23:24:28.717016       1 fs.go:99] Removing directory '/tmp/s2i-build491090638'
I1126 23:24:28.740315       1 sti.go:213] Using provided push secret for pushing 172.30.42.118:5000/default/sti-php:latest image
I1126 23:24:28.740431       1 sti.go:217] Pushing 172.30.42.118:5000/default/sti-php:latest image ...
I1126 23:25:51.808905       1 sti.go:233] Successfully pushed 172.30.42.118:5000/default/sti-php:latest
----
+
TIP: The `-f` flag sets `oc logs` to follow the log, similar to `tail -f`.

. Go back on the NFS server, `bastion`, verify that the registry is using the `registry-storage` NFS share:
+
----
[andrew@master1 ~]$ exit
[root@master1 ~]# exit
[root@bastion ~]# find /srv/export/pvs/registry-storage | grep sti-php
----
* Look for output like this:
+
----
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_layers
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_layers/sha256
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_layers/sha256/4652ee8dcd2bfc7fc7de7aa1cb94373a8c6a55507b1bc35bcefc6475f321c3c2
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_layers/sha256/4652ee8dcd2bfc7fc7de7aa1cb94373a8c6a55507b1bc35bcefc6475f321c3c2/link
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_layers/sha256/ef6f4053c8d4d892acbed1e4e03c8380a02b657d5ae67f55628c320978ec6d57
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_layers/sha256/ef6f4053c8d4d892acbed1e4e03c8380a02b657d5ae67f55628c320978ec6d57/link
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_layers/sha256/c07f4fd29626d000f5815a35835c449ad82a671655d153ee54ddb6a4e3f85122
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_layers/sha256/c07f4fd29626d000f5815a35835c449ad82a671655d153ee54ddb6a4e3f85122/link
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_layers/sha256/1e5540a6e8376a5ea87396460b2e0759616af93d193a9635c7ebfa131c0f26b6
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_layers/sha256/1e5540a6e8376a5ea87396460b2e0759616af93d193a9635c7ebfa131c0f26b6/link
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_layers/sha256/895500133e139e37057ca24f49e1f6c4c66255a78c0d947d5f1c4165db6d64ca
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_layers/sha256/895500133e139e37057ca24f49e1f6c4c66255a78c0d947d5f1c4165db6d64ca/link
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_layers/sha256/51baebb1fefa46091f94bde51129fdb997a4246e1109d59e31cd2103235a35cc
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_layers/sha256/51baebb1fefa46091f94bde51129fdb997a4246e1109d59e31cd2103235a35cc/link
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_manifests
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_manifests/revisions
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_manifests/revisions/sha256
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_manifests/revisions/sha256/615fc8a471b6c9ee91900a3db14add13787b86a1e90c13bec23477b62293242f
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_manifests/revisions/sha256/615fc8a471b6c9ee91900a3db14add13787b86a1e90c13bec23477b62293242f/link
/srv/export/pvs/registry-storage/docker/registry/v2/repositories/ossss/sti-php/_uploads
----
* Previously created images are not in the registry because they were created before the registry was restarted and given a new persistent volume.
